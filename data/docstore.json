[[["21c67406-2127-4691-b918-67a7a5a2d965",{"pageContent":"# langchain-examples\n\nThis folder contains examples of how to use LangChain.\n\n## Run an example\n\nWhat you'll usually want to do.\n\nFirst, build langchain. From the repository root, run:\n\n```sh\nyarn\nyarn build\n```\n\nMost examples require API keys. Run `cp .env.example .env`, then edit `.env` with your API keys.\n\nThen from the `examples/` directory, run:\n\n`yarn run start <path to example>`\n\neg.\n\n`yarn run start ./src/prompts/few_shot.ts`\n\n## Run an example with the transpiled JS\n\nYou shouldn't need to do this, but if you want to run an example with the transpiled JS, you can do so with:\n\n`yarn run start:dist <path to example>`\n\neg.\n\n`yarn run start:dist ./dist/prompts/few_shot.js`","metadata":{"source":"examples/src/README.md","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":34}}}}],["ce3a846e-5592-4747-9e3b-1c4aede74555",{"pageContent":"import { awaitAllCallbacks } from \"langchain/callbacks\";\nimport path from \"path\";\nimport url from \"url\";\n\nconst [exampleName, ...args] = process.argv.slice(2);\n\nif (!exampleName) {\n  console.error(\"Please provide path to example to run\");\n  process.exit(1);\n}\n\n// Allow people to pass all possible variations of a path to an example\n// ./src/foo.ts, ./dist/foo.js, src/foo.ts, dist/foo.js, foo.ts\nlet exampleRelativePath = exampleName;\n\nif (exampleRelativePath.startsWith(\"./examples/\")) {\n  exampleRelativePath = exampleName.slice(11);\n} else if (exampleRelativePath.startsWith(\"examples/\")) {\n  exampleRelativePath = exampleName.slice(9);\n}\n\nif (exampleRelativePath.startsWith(\"./src/\")) {\n  exampleRelativePath = exampleRelativePath.slice(6);\n} else if (exampleRelativePath.startsWith(\"./dist/\")) {\n  exampleRelativePath = exampleRelativePath.slice(7);\n} else if (exampleRelativePath.startsWith(\"src/\")) {\n  exampleRelativePath = exampleRelativePath.slice(4);\n} else if (exampleRelativePath.startsWith(\"dist/\")) {\n  exampleRelativePath = exampleRelativePath.slice(5);\n}\n\nlet runExample;\ntry {\n  ({ run: runExample } = await import(\n    path.join(\n      path.dirname(url.fileURLToPath(import.meta.url)),\n      exampleRelativePath\n    )\n  ));\n} catch (e) {\n  throw new Error(`Could not load example ${exampleName}: ${e}`);\n}\n\nif (runExample) {\n  const maybePromise = runExample(args);\n\n  if (maybePromise instanceof Promise) {\n    maybePromise\n      .catch((e) => {\n        console.error(`Example failed with:`);\n        console.error(e);\n      })\n      .finally(() => awaitAllCallbacks());\n  }\n}","metadata":{"source":"examples/src/index.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":55}}}}],["bac06d20-d2c8-45aa-85bb-478448e9a3eb",{"pageContent":"import { initializeAgentExecutorWithOptions } from \"langchain/agents\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { SerpAPI } from \"langchain/tools\";\nimport { Calculator } from \"langchain/tools/calculator\";\n\nconst model = new OpenAI({ temperature: 0 });\nconst tools = [\n  new SerpAPI(process.env.SERPAPI_API_KEY, {\n    location: \"Austin,Texas,United States\",\n    hl: \"en\",\n    gl: \"us\",\n  }),\n  new Calculator(),\n];\nconst executor = await initializeAgentExecutorWithOptions(tools, model, {\n  agentType: \"zero-shot-react-description\",\n});\n\nconst input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;\nconst result = await executor.run(input, [\n  {\n    handleAgentAction(action, runId) {\n      console.log(\"\\nhandleAgentAction\", action, runId);\n    },\n    handleAgentEnd(action, runId) {\n      console.log(\"\\nhandleAgentEnd\", action, runId);\n    },\n    handleToolEnd(output, runId) {\n      console.log(\"\\nhandleToolEnd\", output, runId);\n    },\n  },\n]);\n/*\nhandleAgentAction {\n  tool: 'search',\n  toolInput: 'Olivia Wilde boyfriend',\n  log: \" I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.\\n\" +\n    'Action: search\\n' +\n    'Action Input: \"Olivia Wilde boyfriend\"'\n} 9b978461-1f6f-4d5f-80cf-5b229ce181b6\n\nhandleToolEnd In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling. Their relationship ended in November 2022. 062fef47-8ad1-4729-9949-a57be252e002\n\nhandleAgentAction {\n  tool: 'search',\n  toolInput: 'Harry Styles age',\n  log: \" I need to find out Harry Styles' age.\\n\" +\n    'Action: search\\n' +\n    'Action Input: \"Harry Styles age\"'\n} 9b978461-1f6f-4d5f-80cf-5b229ce181b6\n\nhandleToolEnd 29 years 9ec91e41-2fbf-4de0-85b6-12b3e6b3784e 61d77e10-c119-435d-a985-1f9d45f0ef08\n\nhandleAgentAction {\n  tool: 'calculator',\n  toolInput: '29^0.23',\n  log: ' I need to calculate 29 raised to the 0.23 power.\\n' +\n    'Action: calculator\\n' +\n    'Action Input: 29^0.23'\n} 9b978461-1f6f-4d5f-80cf-5b229ce181b6\n\nhandleToolEnd 2.169459462491557 07aec96a-ce19-4425-b863-2eae39db8199\n\nhandleAgentEnd {\n  returnValues: {\n    output: \"Harry Styles is Olivia Wilde's boyfriend and his current age raised to the 0.23 power is 2.169459462491557.\"\n  },\n  log: ' I now know the final answer.\\n' +\n    \"Final Answer: Harry Styles is Olivia Wilde's boyfriend and his current age raised to the 0.23 power is 2.169459462491557.\"\n} 9b978461-1f6f-4d5f-80cf-5b229ce181b6\n*/\n\nconsole.log({ result });\n// { result: \"Harry Styles is Olivia Wilde's boyfriend and his current age raised to the 0.23 power is 2.169459462491557.\" }","metadata":{"source":"examples/src/agents/agent_callbacks.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":74}}}}],["9c62401c-a27a-4d9e-b7d0-f07887bed597",{"pageContent":"import { initializeAgentExecutorWithOptions } from \"langchain/agents\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { SerpAPI } from \"langchain/tools\";\nimport { Calculator } from \"langchain/tools/calculator\";\n\nconst model = new OpenAI({ temperature: 0 });\nconst tools = [\n  new SerpAPI(process.env.SERPAPI_API_KEY, {\n    location: \"Austin,Texas,United States\",\n    hl: \"en\",\n    gl: \"us\",\n  }),\n  new Calculator(),\n];\nconst executor = await initializeAgentExecutorWithOptions(tools, model, {\n  agentType: \"zero-shot-react-description\",\n});\nconst controller = new AbortController();\n\n// Call `controller.abort()` somewhere to cancel the request.\nsetTimeout(() => {\n  controller.abort();\n}, 2000);\n\ntry {\n  const input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;\n  const result = await executor.call({ input, signal: controller.signal });\n} catch (e) {\n  console.log(e);\n  /*\n  Error: Cancel: canceled\n      at file:///Users/nuno/dev/langchainjs/langchain/dist/util/async_caller.js:60:23\n      at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\n      at RetryOperation._fn (/Users/nuno/dev/langchainjs/node_modules/p-retry/index.js:50:12) {\n    attemptNumber: 1,\n    retriesLeft: 6\n  }\n  */\n}","metadata":{"source":"examples/src/agents/agent_cancellation.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":39}}}}],["b3e631f0-69d1-4403-9389-6d74767c1707",{"pageContent":"import { initializeAgentExecutorWithOptions } from \"langchain/agents\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { SerpAPI } from \"langchain/tools\";\nimport { Calculator } from \"langchain/tools/calculator\";\n\nconst model = new OpenAI({ temperature: 0 });\nconst tools = [\n  new SerpAPI(process.env.SERPAPI_API_KEY, {\n    location: \"Austin,Texas,United States\",\n    hl: \"en\",\n    gl: \"us\",\n  }),\n  new Calculator(),\n];\nconst executor = await initializeAgentExecutorWithOptions(tools, model, {\n  agentType: \"zero-shot-react-description\",\n});\n\ntry {\n  const input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;\n  const result = await executor.call({ input, timeout: 2000 }); // 2 seconds\n} catch (e) {\n  console.log(e);\n  /*\n  Error: Cancel: canceled\n      at file:///Users/nuno/dev/langchainjs/langchain/dist/util/async_caller.js:60:23\n      at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\n      at RetryOperation._fn (/Users/nuno/dev/langchainjs/node_modules/p-retry/index.js:50:12) {\n    attemptNumber: 1,\n    retriesLeft: 6\n  }\n  */\n}","metadata":{"source":"examples/src/agents/agent_timeout.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":33}}}}],["8eb3551f-8daa-4d72-a07f-619b26fee1ea",{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { initializeAgentExecutorWithOptions } from \"langchain/agents\";\nimport {\n  RequestsGetTool,\n  RequestsPostTool,\n  AIPluginTool,\n} from \"langchain/tools\";\n\nexport const run = async () => {\n  const tools = [\n    new RequestsGetTool(),\n    new RequestsPostTool(),\n    await AIPluginTool.fromPluginUrl(\n      \"https://www.klarna.com/.well-known/ai-plugin.json\"\n    ),\n  ];\n  const agent = await initializeAgentExecutorWithOptions(\n    tools,\n    new ChatOpenAI({ temperature: 0 }),\n    { agentType: \"chat-zero-shot-react-description\", verbose: true }\n  );\n\n  const result = await agent.call({\n    input: \"what t shirts are available in klarna?\",\n  });\n\n  console.log({ result });\n};","metadata":{"source":"examples/src/agents/aiplugin-tool.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":28}}}}],["90a61672-fa16-48d7-bd7c-0c50861c223a",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport {\n  createAWSSfnAgent,\n  AWSSfnToolkit,\n} from \"langchain/agents/toolkits/aws_sfn\";\n\nconst _EXAMPLE_STATE_MACHINE_ASL = `\n{\n  \"Comment\": \"A simple example of the Amazon States Language to define a state machine for new client onboarding.\",\n  \"StartAt\": \"OnboardNewClient\",\n  \"States\": {\n    \"OnboardNewClient\": {\n      \"Type\": \"Pass\",\n      \"Result\": \"Client onboarded!\",\n      \"End\": true\n    }\n  }\n}`;\n\n/**\n * This example uses a deployed AWS Step Function state machine with the above Amazon State Language (ASL) definition.\n * You can test by provisioning a state machine using the above ASL within your AWS environment, or you can use a tool like LocalStack\n * to mock AWS services locally. See https://localstack.cloud/ for more information.\n */\nexport const run = async () => {\n  const model = new OpenAI({ temperature: 0 });\n  const toolkit = new AWSSfnToolkit({\n    name: \"onboard-new-client-workflow\",\n    description:\n      \"Onboard new client workflow. Can also be used to get status of any excuting workflow or state machine.\",\n    stateMachineArn:\n      \"arn:aws:states:us-east-1:1234567890:stateMachine:my-state-machine\", // Update with your state machine ARN accordingly\n    region: \"<your Sfn's region>\",\n    accessKeyId: \"<your access key id>\",\n    secretAccessKey: \"<your secret access key>\",\n  });\n  const executor = createAWSSfnAgent(model, toolkit);\n\n  const input = `Onboard john doe (john@example.com) as a new client.`;\n\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await executor.call({ input });\n\n  console.log(`Got output ${result.output}`);\n\n  console.log(\n    `Got intermediate steps ${JSON.stringify(\n      result.intermediateSteps,\n      null,\n      2\n    )}`\n  );\n};","metadata":{"source":"examples/src/agents/aws_sfn.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":54}}}}],["bad4076c-d870-4799-aa9c-c24aed1a2bbd",{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { initializeAgentExecutorWithOptions } from \"langchain/agents\";\nimport { SerpAPI } from \"langchain/tools\";\nimport { Calculator } from \"langchain/tools/calculator\";\n\nexport const run = async () => {\n  process.env.LANGCHAIN_HANDLER = \"langchain\";\n  const model = new ChatOpenAI({ temperature: 0 });\n  const tools = [\n    new SerpAPI(process.env.SERPAPI_API_KEY, {\n      location: \"Austin,Texas,United States\",\n      hl: \"en\",\n      gl: \"us\",\n    }),\n    new Calculator(),\n  ];\n\n  // Passing \"chat-conversational-react-description\" as the agent type\n  // automatically creates and uses BufferMemory with the executor.\n  // If you would like to override this, you can pass in a custom\n  // memory option, but the memoryKey set on it must be \"chat_history\".\n  const executor = await initializeAgentExecutorWithOptions(tools, model, {\n    agentType: \"chat-conversational-react-description\",\n    verbose: true,\n  });\n  console.log(\"Loaded agent.\");\n\n  const input0 = \"hi, i am bob\";\n\n  const result0 = await executor.call({ input: input0 });\n\n  console.log(`Got output ${result0.output}`);\n\n  const input1 = \"whats my name?\";\n\n  const result1 = await executor.call({ input: input1 });\n\n  console.log(`Got output ${result1.output}`);\n\n  const input2 = \"whats the weather in pomfret?\";\n\n  const result2 = await executor.call({ input: input2 });\n\n  console.log(`Got output ${result2.output}`);\n};","metadata":{"source":"examples/src/agents/chat_convo_with_tracing.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":45}}}}],["d06c1d6b-0cbd-4f18-a902-a4f39f1c00ac",{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { AgentExecutor } from \"langchain/agents\";\nimport { SerpAPI } from \"langchain/tools\";\nimport { Calculator } from \"langchain/tools/calculator\";\nimport { pull } from \"langchain/hub\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport { RunnableSequence } from \"langchain/schema/runnable\";\nimport { AgentStep, BaseMessage } from \"langchain/schema\";\nimport { BufferMemory } from \"langchain/memory\";\nimport { formatLogToString } from \"langchain/agents/format_scratchpad/log\";\nimport { renderTextDescription } from \"langchain/tools/render\";\nimport { ReActSingleInputOutputParser } from \"langchain/agents/react/output_parser\";\n\n/** Define your chat model */\nconst model = new ChatOpenAI({ modelName: \"gpt-4\" });\n/** Bind a stop token to the model */\nconst modelWithStop = model.bind({\n  stop: [\"\\nObservation\"],\n});\n/** Define your list of tools */\nconst tools = [\n  new SerpAPI(process.env.SERPAPI_API_KEY, {\n    location: \"Austin,Texas,United States\",\n    hl: \"en\",\n    gl: \"us\",\n  }),\n  new Calculator(),\n];\n/**\n * Pull a prompt from LangChain Hub\n * @link https://smith.langchain.com/hub/hwchase17/react-chat\n */\nconst prompt = await pull<PromptTemplate>(\"hwchase17/react-chat\");\n/** Add input variables to prompt */\nconst toolNames = tools.map((tool) => tool.name);\nconst promptWithInputs = await prompt.partial({\n  tools: renderTextDescription(tools),\n  tool_names: toolNames.join(\",\"),\n});\n\nconst runnableAgent = RunnableSequence.from([\n  {\n    input: (i: {\n      input: string;\n      steps: AgentStep[];\n      chat_history: BaseMessage[];\n    }) => i.input,\n    agent_scratchpad: (i: {\n      input: string;\n      steps: AgentStep[];\n      chat_history: BaseMessage[];\n    }) => formatLogToString(i.steps),\n    chat_history: (i: {\n      input: string;\n      steps: AgentStep[];\n      chat_history: BaseMessage[];\n    }) => i.chat_history,\n  },\n  promptWithInputs,\n  modelWithStop,\n  new ReActSingleInputOutputParser({ toolNames }),\n]);\n/**\n * Define your memory store\n * @important The memoryKey must be \"chat_history\" for the chat agent to work\n * because this is the key this particular prompt expects.\n */\nconst memory = new BufferMemory({ memoryKey: \"chat_history\" });\n/** Define your executor and pass in the agent, tools and memory */\nconst executor = AgentExecutor.fromAgentAndTools({\n  agent: runnableAgent,\n  tools,\n  memory,\n});\n\nconsole.log(\"Loaded agent.\");\n\nconst input0 = \"hi, i am bob\";\nconst result0 = await executor.call({ input: input0 });\nconsole.log(`Got output ${result0.output}`);\n\nconst input1 = \"whats my name?\";\nconst result1 = await executor.call({ input: input1 });\nconsole.log(`Got output ${result1.output}`);\n\nconst input2 = \"whats the weather in pomfret?\";\nconst result2 = await executor.call({ input: input2 });\nconsole.log(`Got output ${result2.output}`);\n/**\n * Loaded agent.\n * Got output Hello Bob, how can I assist you today?\n * Got output Your name is Bob.\n * Got output The current weather in Pomfret, CT is partly cloudy with a temperature of 59 degrees Fahrenheit. The humidity is at 52% and there is a wind speed of 8 mph. There is a 0% chance of precipitation.\n */","metadata":{"source":"examples/src/agents/chat_convo_with_tracing_runnable.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":94}}}}],["dcd5bea2-6370-41bf-9e73-26906724fb95",{"pageContent":"import { initializeAgentExecutorWithOptions } from \"langchain/agents\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { SerpAPI } from \"langchain/tools\";\nimport { Calculator } from \"langchain/tools/calculator\";\n\nexport const run = async () => {\n  const model = new ChatOpenAI({ temperature: 0 });\n  const tools = [\n    new SerpAPI(process.env.SERPAPI_API_KEY, {\n      location: \"Austin,Texas,United States\",\n      hl: \"en\",\n      gl: \"us\",\n    }),\n    new Calculator(),\n  ];\n\n  const executor = await initializeAgentExecutorWithOptions(tools, model, {\n    agentType: \"chat-zero-shot-react-description\",\n    returnIntermediateSteps: true,\n  });\n  console.log(\"Loaded agent.\");\n\n  const input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;\n\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await executor.call({ input });\n\n  console.log(`Got output ${result.output}`);\n\n  console.log(\n    `Got intermediate steps ${JSON.stringify(\n      result.intermediateSteps,\n      null,\n      2\n    )}`\n  );\n};","metadata":{"source":"examples/src/agents/chat_mrkl.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":38}}}}],["d832a754-71b3-4a53-b4e3-4e30d2056ad3",{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { initializeAgentExecutorWithOptions } from \"langchain/agents\";\nimport { SerpAPI } from \"langchain/tools\";\nimport { Calculator } from \"langchain/tools/calculator\";\n\nexport const run = async () => {\n  process.env.LANGCHAIN_TRACING = \"true\";\n  const model = new ChatOpenAI({ temperature: 0 });\n  const tools = [\n    new SerpAPI(process.env.SERPAPI_API_KEY, {\n      location: \"Austin,Texas,United States\",\n      hl: \"en\",\n      gl: \"us\",\n    }),\n    new Calculator(),\n  ];\n\n  const executor = await initializeAgentExecutorWithOptions(tools, model, {\n    agentType: \"chat-zero-shot-react-description\",\n    returnIntermediateSteps: true,\n    verbose: true,\n  });\n  console.log(\"Loaded agent.\");\n\n  const input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;\n\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await executor.call({ input });\n\n  console.log(`Got output ${result.output}`);\n\n  console.log(\n    `Got intermediate steps ${JSON.stringify(\n      result.intermediateSteps,\n      null,\n      2\n    )}`\n  );\n};","metadata":{"source":"examples/src/agents/chat_mrkl_with_tracing.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":40}}}}],["19830401-ec33-4a1b-bc0f-24ad923e70b8",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { initializeAgentExecutorWithOptions } from \"langchain/agents\";\nimport { SerpAPI } from \"langchain/tools\";\nimport { Calculator } from \"langchain/tools/calculator\";\nimport process from \"process\";\n\nexport const run = async () => {\n  process.env.LANGCHAIN_TRACING = \"true\";\n  const model = new OpenAI({ temperature: 0 });\n  const tools = [\n    new SerpAPI(process.env.SERPAPI_API_KEY, {\n      location: \"Austin,Texas,United States\",\n      hl: \"en\",\n      gl: \"us\",\n    }),\n    new Calculator(),\n  ];\n\n  const executor = await initializeAgentExecutorWithOptions(tools, model, {\n    agentType: \"zero-shot-react-description\",\n    verbose: true,\n  });\n\n  console.log(\"Loaded agent.\");\n\n  const input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;\n\n  console.log(`Executing with input \"${input}\"...`);\n\n  // This will result in a lot of errors, because the shared Tracer is not concurrency-safe.\n  const [resultA, resultB, resultC] = await Promise.all([\n    executor.call({ input }),\n    executor.call({ input }),\n    executor.call({ input }),\n  ]);\n\n  console.log(`Got output ${resultA.output} ${resultA.__run.runId}`);\n  console.log(`Got output ${resultB.output} ${resultB.__run.runId}`);\n  console.log(`Got output ${resultC.output} ${resultC.__run.runId}`);\n};","metadata":{"source":"examples/src/agents/concurrent_mrkl.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":40}}}}],["90e0df64-7489-442d-8d73-c2a388311a2f",{"pageContent":"import { AgentExecutor, ZeroShotAgent } from \"langchain/agents\";\nimport { LLMChain } from \"langchain/chains\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { SerpAPI } from \"langchain/tools\";\nimport { Calculator } from \"langchain/tools/calculator\";\n\nexport const run = async () => {\n  const model = new OpenAI({ temperature: 0 });\n  const tools = [\n    new SerpAPI(process.env.SERPAPI_API_KEY, {\n      location: \"Austin,Texas,United States\",\n      hl: \"en\",\n      gl: \"us\",\n    }),\n    new Calculator(),\n  ];\n\n  const prefix = `Answer the following questions as best you can, but speaking as a pirate might speak. You have access to the following tools:`;\n  const suffix = `Begin! Remember to speak as a pirate when giving your final answer. Use lots of \"Args\"\n\nQuestion: {input}\n{agent_scratchpad}`;\n\n  const createPromptArgs = {\n    suffix,\n    prefix,\n    inputVariables: [\"input\", \"agent_scratchpad\"],\n  };\n\n  const prompt = ZeroShotAgent.createPrompt(tools, createPromptArgs);\n\n  const llmChain = new LLMChain({ llm: model, prompt });\n  const agent = new ZeroShotAgent({\n    llmChain,\n    allowedTools: [\"search\", \"calculator\"],\n  });\n  const agentExecutor = AgentExecutor.fromAgentAndTools({ agent, tools });\n  console.log(\"Loaded agent.\");\n\n  const input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;\n\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await agentExecutor.call({ input });\n\n  console.log(`Got output ${result.output}`);\n};","metadata":{"source":"examples/src/agents/custom_agent.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":47}}}}],["f8d1a06f-0843-4105-aa5f-f9d1bfbab815",{"pageContent":"import {\n  LLMSingleActionAgent,\n  AgentActionOutputParser,\n  AgentExecutor,\n} from \"langchain/agents\";\nimport { LLMChain } from \"langchain/chains\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport {\n  BaseStringPromptTemplate,\n  SerializedBasePromptTemplate,\n  renderTemplate,\n} from \"langchain/prompts\";\nimport {\n  InputValues,\n  PartialValues,\n  AgentStep,\n  AgentAction,\n  AgentFinish,\n} from \"langchain/schema\";\nimport { SerpAPI, Tool } from \"langchain/tools\";\nimport { Calculator } from \"langchain/tools/calculator\";\n\nconst PREFIX = `Answer the following questions as best you can. You have access to the following tools:`;\nconst formatInstructions = (\n  toolNames: string\n) => `Use the following format in your response:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [${toolNames}]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question`;","metadata":{"source":"examples/src/agents/custom_llm_agent.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":35}}}}],["f8aab76a-aa4f-4bce-adb7-373c2034f779",{"pageContent":"const SUFFIX = `Begin!\n\nQuestion: {input}\nThought:{agent_scratchpad}`;\n\nclass CustomPromptTemplate extends BaseStringPromptTemplate {\n  tools: Tool[];\n\n  constructor(args: { tools: Tool[]; inputVariables: string[] }) {\n    super({ inputVariables: args.inputVariables });\n    this.tools = args.tools;\n  }\n\n  _getPromptType(): string {\n    throw new Error(\"Not implemented\");\n  }\n\n  format(input: InputValues): Promise<string> {\n    /** Construct the final template */\n    const toolStrings = this.tools\n      .map((tool) => `${tool.name}: ${tool.description}`)\n      .join(\"\\n\");\n    const toolNames = this.tools.map((tool) => tool.name).join(\"\\n\");\n    const instructions = formatInstructions(toolNames);\n    const template = [PREFIX, toolStrings, instructions, SUFFIX].join(\"\\n\\n\");\n    /** Construct the agent_scratchpad */\n    const intermediateSteps = input.intermediate_steps as AgentStep[];\n    const agentScratchpad = intermediateSteps.reduce(\n      (thoughts, { action, observation }) =>\n        thoughts +\n        [action.log, `\\nObservation: ${observation}`, \"Thought:\"].join(\"\\n\"),\n      \"\"\n    );\n    const newInput = { agent_scratchpad: agentScratchpad, ...input };\n    /** Format the template. */\n    return Promise.resolve(renderTemplate(template, \"f-string\", newInput));\n  }\n\n  partial(_values: PartialValues): Promise<BaseStringPromptTemplate> {\n    throw new Error(\"Not implemented\");\n  }\n\n  serialize(): SerializedBasePromptTemplate {\n    throw new Error(\"Not implemented\");\n  }\n}\n\nclass CustomOutputParser extends AgentActionOutputParser {\n  lc_namespace = [\"langchain\", \"agents\", \"custom_llm_agent\"];\n\n  async parse(text: string): Promise<AgentAction | AgentFinish> {\n    if (text.includes(\"Final Answer:\")) {\n      const parts = text.split(\"Final Answer:\");\n      const input = parts[parts.length - 1].trim();\n      const finalAnswers = { output: input };\n      return { log: text, returnValues: finalAnswers };\n    }\n\n    const match = /Action: (.*)\\nAction Input: (.*)/s.exec(text);\n    if (!match) {\n      throw new Error(`Could not parse LLM output: ${text}`);\n    }\n\n    return {\n      tool: match[1].trim(),\n      toolInput: match[2].trim().replace(/^\"+|\"+$/g, \"\"),\n      log: text,\n    };\n  }\n\n  getFormatInstructions(): string {\n    throw new Error(\"Not implemented\");\n  }\n}\n\nexport const run = async () => {\n  const model = new OpenAI({ temperature: 0 });\n  const tools = [\n    new SerpAPI(process.env.SERPAPI_API_KEY, {\n      location: \"Austin,Texas,United States\",\n      hl: \"en\",\n      gl: \"us\",\n    }),\n    new Calculator(),\n  ];\n\n  const llmChain = new LLMChain({\n    prompt: new CustomPromptTemplate({\n      tools,\n      inputVariables: [\"input\", \"agent_scratchpad\"],\n    }),\n    llm: model,\n  });\n\n  const agent = new LLMSingleActionAgent({\n    llmChain,\n    outputParser: new CustomOutputParser(),\n    stop: [\"\\nObservation\"],\n  });\n  const executor = new AgentExecutor({\n    agent,\n    tools,\n  });\n  console.log(\"Loaded agent.\");\n\n  const input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;\n\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await executor.call({ input });\n\n  console.log(`Got output ${result.output}`);\n};","metadata":{"source":"examples/src/agents/custom_llm_agent.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":36,"to":148}}}}],["6dadcbbb-a747-4640-978f-48e8c5eae9ac",{"pageContent":"import {\n  AgentActionOutputParser,\n  AgentExecutor,\n  LLMSingleActionAgent,\n} from \"langchain/agents\";\nimport { LLMChain } from \"langchain/chains\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport {\n  BaseChatPromptTemplate,\n  SerializedBasePromptTemplate,\n  renderTemplate,\n} from \"langchain/prompts\";\nimport {\n  AgentAction,\n  AgentFinish,\n  AgentStep,\n  BaseMessage,\n  HumanMessage,\n  InputValues,\n  PartialValues,\n} from \"langchain/schema\";\nimport { SerpAPI, Tool } from \"langchain/tools\";\nimport { Calculator } from \"langchain/tools/calculator\";\n\nconst PREFIX = `Answer the following questions as best you can. You have access to the following tools:`;\nconst formatInstructions = (\n  toolNames: string\n) => `Use the following format in your response:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [${toolNames}]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question`;","metadata":{"source":"examples/src/agents/custom_llm_agent_chat.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":37}}}}],["d181a110-ef02-4ade-9e68-a46d4a6ee1a2",{"pageContent":"const SUFFIX = `Begin!\n\nQuestion: {input}\nThought:{agent_scratchpad}`;\n\nclass CustomPromptTemplate extends BaseChatPromptTemplate {\n  tools: Tool[];\n\n  constructor(args: { tools: Tool[]; inputVariables: string[] }) {\n    super({ inputVariables: args.inputVariables });\n    this.tools = args.tools;\n  }\n\n  _getPromptType(): string {\n    return \"chat\";\n  }\n\n  async formatMessages(values: InputValues): Promise<BaseMessage[]> {\n    /** Construct the final template */\n    const toolStrings = this.tools\n      .map((tool) => `${tool.name}: ${tool.description}`)\n      .join(\"\\n\");\n    const toolNames = this.tools.map((tool) => tool.name).join(\"\\n\");\n    const instructions = formatInstructions(toolNames);\n    const template = [PREFIX, toolStrings, instructions, SUFFIX].join(\"\\n\\n\");\n    /** Construct the agent_scratchpad */\n    const intermediateSteps = values.intermediate_steps as AgentStep[];\n    const agentScratchpad = intermediateSteps.reduce(\n      (thoughts, { action, observation }) =>\n        thoughts +\n        [action.log, `\\nObservation: ${observation}`, \"Thought:\"].join(\"\\n\"),\n      \"\"\n    );\n    const newInput = { agent_scratchpad: agentScratchpad, ...values };\n    /** Format the template. */\n    const formatted = renderTemplate(template, \"f-string\", newInput);\n    return [new HumanMessage(formatted)];\n  }\n\n  partial(_values: PartialValues): Promise<BaseChatPromptTemplate> {\n    throw new Error(\"Not implemented\");\n  }\n\n  serialize(): SerializedBasePromptTemplate {\n    throw new Error(\"Not implemented\");\n  }\n}\n\nclass CustomOutputParser extends AgentActionOutputParser {\n  lc_namespace = [\"langchain\", \"agents\", \"custom_llm_agent_chat\"];\n\n  async parse(text: string): Promise<AgentAction | AgentFinish> {\n    if (text.includes(\"Final Answer:\")) {\n      const parts = text.split(\"Final Answer:\");\n      const input = parts[parts.length - 1].trim();\n      const finalAnswers = { output: input };\n      return { log: text, returnValues: finalAnswers };\n    }\n\n    const match = /Action: (.*)\\nAction Input: (.*)/s.exec(text);\n    if (!match) {\n      throw new Error(`Could not parse LLM output: ${text}`);\n    }\n\n    return {\n      tool: match[1].trim(),\n      toolInput: match[2].trim().replace(/^\"+|\"+$/g, \"\"),\n      log: text,\n    };\n  }\n\n  getFormatInstructions(): string {\n    throw new Error(\"Not implemented\");\n  }\n}\n\nexport const run = async () => {\n  const model = new ChatOpenAI({ temperature: 0 });\n  const tools = [\n    new SerpAPI(process.env.SERPAPI_API_KEY, {\n      location: \"Austin,Texas,United States\",\n      hl: \"en\",\n      gl: \"us\",\n    }),\n    new Calculator(),\n  ];\n\n  const llmChain = new LLMChain({\n    prompt: new CustomPromptTemplate({\n      tools,\n      inputVariables: [\"input\", \"agent_scratchpad\"],\n    }),\n    llm: model,\n  });\n\n  const agent = new LLMSingleActionAgent({\n    llmChain,\n    outputParser: new CustomOutputParser(),\n    stop: [\"\\nObservation\"],\n  });\n  const executor = new AgentExecutor({\n    agent,\n    tools,\n  });\n  console.log(\"Loaded agent.\");\n\n  const input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;\n\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await executor.call({ input });\n\n  console.log(`Got output ${result.output}`);\n};\nrun();","metadata":{"source":"examples/src/agents/custom_llm_agent_chat.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":38,"to":152}}}}],["d14862be-45bb-4bd7-8839-39f69835bcfc",{"pageContent":"import { AgentExecutor } from \"langchain/agents\";\nimport { formatLogToString } from \"langchain/agents/format_scratchpad/log\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport {\n  AgentAction,\n  AgentFinish,\n  AgentStep,\n  BaseMessage,\n  HumanMessage,\n  InputValues,\n} from \"langchain/schema\";\nimport { RunnableSequence } from \"langchain/schema/runnable\";\nimport { SerpAPI } from \"langchain/tools\";\nimport { Calculator } from \"langchain/tools/calculator\";\n\n/**\n * Instantiate the chat model and bind the stop token\n * @important The stop token must be set, if not the LLM will happily continue generating text forever.\n */\nconst model = new ChatOpenAI({ temperature: 0 }).bind({\n  stop: [\"\\nObservation\"],\n});\n/** Define the tools */\nconst tools = [\n  new SerpAPI(process.env.SERPAPI_API_KEY, {\n    location: \"Austin,Texas,United States\",\n    hl: \"en\",\n    gl: \"us\",\n  }),\n  new Calculator(),\n];\n/** Create the prefix prompt */\nconst PREFIX = `Answer the following questions as best you can. You have access to the following tools:\n{tools}`;\n/** Create the tool instructions prompt */\nconst TOOL_INSTRUCTIONS_TEMPLATE = `Use the following format in your response:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [{tool_names}]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question`;\n/** Create the suffix prompt */\nconst SUFFIX = `Begin!\n\nQuestion: {input}\nThought:`;\n\nasync function formatMessages(\n  values: InputValues\n): Promise<Array<BaseMessage>> {\n  /** Check input and intermediate steps are both inside values */\n  if (!(\"input\" in values) || !(\"intermediate_steps\" in values)) {\n    throw new Error(\"Missing input or agent_scratchpad from values.\");\n  }\n  /** Extract and case the intermediateSteps from values as Array<AgentStep> or an empty array if none are passed */\n  const intermediateSteps = values.intermediate_steps\n    ? (values.intermediate_steps as Array<AgentStep>)\n    : [];\n  /** Call the helper `formatLogToString` which returns the steps as a string  */\n  const agentScratchpad = formatLogToString(intermediateSteps);\n  /** Construct the tool strings */\n  const toolStrings = tools\n    .map((tool) => `${tool.name}: ${tool.description}`)\n    .join(\"\\n\");\n  const toolNames = tools.map((tool) => tool.name).join(\",\\n\");\n  /** Create templates and format the instructions and suffix prompts */\n  const prefixTemplate = new PromptTemplate({\n    template: PREFIX,\n    inputVariables: [\"tools\"],\n  });\n  const instructionsTemplate = new PromptTemplate({\n    template: TOOL_INSTRUCTIONS_TEMPLATE,\n    inputVariables: [\"tool_names\"],\n  });\n  const suffixTemplate = new PromptTemplate({\n    template: SUFFIX,\n    inputVariables: [\"input\"],\n  });\n  /** Format both templates by passing in the input variables */\n  const formattedPrefix = await prefixTemplate.format({\n    tools: toolStrings,\n  });\n  const formattedInstructions = await instructionsTemplate.format({\n    tool_names: toolNames,\n  });\n  const formattedSuffix = await suffixTemplate.format({\n    input: values.input,\n  });\n  /** Construct the final prompt string */\n  const formatted = [\n    formattedPrefix,\n    formattedInstructions,\n    formattedSuffix,\n    agentScratchpad,\n  ].join(\"\\n\");\n  /** Return the message as a HumanMessage. */\n  return [new HumanMessage(formatted)];\n}\n\n/** Define the custom output parser */","metadata":{"source":"examples/src/agents/custom_llm_agent_chat_runnable.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":105}}}}],["a391475f-803d-4772-92c4-09069ee11615",{"pageContent":"function customOutputParser(message: BaseMessage): AgentAction | AgentFinish {\n  const text = message.content;\n  /** If the input includes \"Final Answer\" return as an instance of `AgentFinish` */\n  if (text.includes(\"Final Answer:\")) {\n    const parts = text.split(\"Final Answer:\");\n    const input = parts[parts.length - 1].trim();\n    const finalAnswers = { output: input };\n    return { log: text, returnValues: finalAnswers };\n  }\n  /** Use RegEx to extract any actions and their values */\n  const match = /Action: (.*)\\nAction Input: (.*)/s.exec(text);\n  if (!match) {\n    throw new Error(`Could not parse LLM output: ${text}`);\n  }\n  /** Return as an instance of `AgentAction` */\n  return {\n    tool: match[1].trim(),\n    toolInput: match[2].trim().replace(/^\"+|\"+$/g, \"\"),\n    log: text,\n  };\n}\n\n/** Define the Runnable with LCEL */\nconst runnable = RunnableSequence.from([\n  {\n    input: (values: InputValues) => values.input,\n    intermediate_steps: (values: InputValues) => values.steps,\n  },\n  formatMessages,\n  model,\n  customOutputParser,\n]);\n/** Pass the runnable to the `AgentExecutor` class as the agent */\nconst executor = new AgentExecutor({\n  agent: runnable,\n  tools,\n});\nconsole.log(\"Loaded agent.\");\n\nconst input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;\n\nconsole.log(`Executing with input \"${input}\"...`);\n\nconst result = await executor.call({ input });\n\nconsole.log(`Got output ${result.output}`);\n/**\n * Got output Harry Styles' current age raised to the 0.23 power is approximately 2.1156502324195268.\n */","metadata":{"source":"examples/src/agents/custom_llm_agent_chat_runnable.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":106,"to":154}}}}],["dfe05912-8cfa-4fc9-8917-152b368e151e",{"pageContent":"import { AgentExecutor } from \"langchain/agents\";\nimport { formatLogToString } from \"langchain/agents/format_scratchpad/log\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport {\n  AgentAction,\n  AgentFinish,\n  AgentStep,\n  BaseMessage,\n  HumanMessage,\n  InputValues,\n} from \"langchain/schema\";\nimport { RunnableSequence } from \"langchain/schema/runnable\";\nimport { SerpAPI } from \"langchain/tools\";\nimport { Calculator } from \"langchain/tools/calculator\";\n\n/**\n * Instantiate the LLM and bind the stop token\n * @important The stop token must be set, if not the LLM will happily continue generating text forever.\n */\nconst model = new OpenAI({ temperature: 0 }).bind({\n  stop: [\"\\nObservation\"],\n});\n/** Define the tools */\nconst tools = [\n  new SerpAPI(process.env.SERPAPI_API_KEY, {\n    location: \"Austin,Texas,United States\",\n    hl: \"en\",\n    gl: \"us\",\n  }),\n  new Calculator(),\n];\n/** Create the prefix prompt */\nconst PREFIX = `Answer the following questions as best you can. You have access to the following tools:\n{tools}`;\n/** Create the tool instructions prompt */\nconst TOOL_INSTRUCTIONS_TEMPLATE = `Use the following format in your response:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [{tool_names}]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question`;\n/** Create the suffix prompt */\nconst SUFFIX = `Begin!\n\nQuestion: {input}\nThought:`;\n\nasync function formatMessages(\n  values: InputValues\n): Promise<Array<BaseMessage>> {\n  /** Check input and intermediate steps are both inside values */\n  if (!(\"input\" in values) || !(\"intermediate_steps\" in values)) {\n    throw new Error(\"Missing input or agent_scratchpad from values.\");\n  }\n  /** Extract and case the intermediateSteps from values as Array<AgentStep> or an empty array if none are passed */\n  const intermediateSteps = values.intermediate_steps\n    ? (values.intermediate_steps as Array<AgentStep>)\n    : [];\n  /** Call the helper `formatLogToString` which returns the steps as a string  */\n  const agentScratchpad = formatLogToString(intermediateSteps);\n  /** Construct the tool strings */\n  const toolStrings = tools\n    .map((tool) => `${tool.name}: ${tool.description}`)\n    .join(\"\\n\");\n  const toolNames = tools.map((tool) => tool.name).join(\",\\n\");\n  /** Create templates and format the instructions and suffix prompts */\n  const prefixTemplate = new PromptTemplate({\n    template: PREFIX,\n    inputVariables: [\"tools\"],\n  });\n  const instructionsTemplate = new PromptTemplate({\n    template: TOOL_INSTRUCTIONS_TEMPLATE,\n    inputVariables: [\"tool_names\"],\n  });\n  const suffixTemplate = new PromptTemplate({\n    template: SUFFIX,\n    inputVariables: [\"input\"],\n  });\n  /** Format both templates by passing in the input variables */\n  const formattedPrefix = await prefixTemplate.format({\n    tools: toolStrings,\n  });\n  const formattedInstructions = await instructionsTemplate.format({\n    tool_names: toolNames,\n  });\n  const formattedSuffix = await suffixTemplate.format({\n    input: values.input,\n  });\n  /** Construct the final prompt string */\n  const formatted = [\n    formattedPrefix,\n    formattedInstructions,\n    formattedSuffix,\n    agentScratchpad,\n  ].join(\"\\n\");\n  /** Return the message as a HumanMessage. */\n  return [new HumanMessage(formatted)];\n}\n\n/** Define the custom output parser */","metadata":{"source":"examples/src/agents/custom_llm_agent_runnable.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":105}}}}],["e1e9af40-5883-468d-8044-11a2da4fdbdc",{"pageContent":"function customOutputParser(text: string): AgentAction | AgentFinish {\n  /** If the input includes \"Final Answer\" return as an instance of `AgentFinish` */\n  if (text.includes(\"Final Answer:\")) {\n    const parts = text.split(\"Final Answer:\");\n    const input = parts[parts.length - 1].trim();\n    const finalAnswers = { output: input };\n    return { log: text, returnValues: finalAnswers };\n  }\n  /** Use RegEx to extract any actions and their values */\n  const match = /Action: (.*)\\nAction Input: (.*)/s.exec(text);\n  if (!match) {\n    throw new Error(`Could not parse LLM output: ${text}`);\n  }\n  /** Return as an instance of `AgentAction` */\n  return {\n    tool: match[1].trim(),\n    toolInput: match[2].trim().replace(/^\"+|\"+$/g, \"\"),\n    log: text,\n  };\n}\n\n/** Define the Runnable with LCEL */\nconst runnable = RunnableSequence.from([\n  {\n    input: (values: InputValues) => values.input,\n    intermediate_steps: (values: InputValues) => values.steps,\n  },\n  formatMessages,\n  model,\n  customOutputParser,\n]);\n/** Pass the runnable to the `AgentExecutor` class as the agent */\nconst executor = new AgentExecutor({\n  agent: runnable,\n  tools,\n});\nconsole.log(\"Loaded agent.\");\n\nconst input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;\n\nconsole.log(`Executing with input \"${input}\"...`);\n\nconst result = await executor.call({ input });\n\nconsole.log(`Got output ${result.output}`);\n/**\n * Got output Harry Styles, Olivia Wilde's boyfriend, is 29 years old and his age raised to the 0.23 power is 2.169459462491557.\n */","metadata":{"source":"examples/src/agents/custom_llm_agent_runnable.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":106,"to":153}}}}],["f72e46e1-8368-4451-8380-a8b491f5b105",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { initializeAgentExecutorWithOptions } from \"langchain/agents\";\nimport { DynamicTool } from \"langchain/tools\";\n\nexport const run = async () => {\n  const model = new OpenAI({ temperature: 0 });\n  const tools = [\n    new DynamicTool({\n      name: \"FOO\",\n      description:\n        \"call this to get the value of foo. input should be an empty string.\",\n      func: () =>\n        new Promise((resolve) => {\n          resolve(\"foo\");\n        }),\n    }),\n    new DynamicTool({\n      name: \"BAR\",\n      description:\n        \"call this to get the value of bar. input should be an empty string.\",\n      func: () =>\n        new Promise((resolve) => {\n          resolve(\"baz1\");\n        }),\n    }),\n  ];\n\n  const executor = await initializeAgentExecutorWithOptions(tools, model, {\n    agentType: \"zero-shot-react-description\",\n  });\n\n  console.log(\"Loaded agent.\");\n\n  const input = `What is the value of foo?`;\n\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await executor.call({ input });\n\n  console.log(`Got output ${result.output}`);\n};","metadata":{"source":"examples/src/agents/custom_tool.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":41}}}}],["6d6161c4-80ac-4b94-8699-39175a9f0760",{"pageContent":"import { z } from \"zod\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { initializeAgentExecutorWithOptions } from \"langchain/agents\";\nimport { DynamicStructuredTool } from \"langchain/tools\";\n\nconst model = new ChatOpenAI({ temperature: 0.1 });\nconst tools = [\n  new DynamicStructuredTool({\n    name: \"task-scheduler\",\n    description: \"Schedules tasks\",\n    schema: z\n      .object({\n        tasks: z\n          .array(\n            z.object({\n              title: z\n                .string()\n                .describe(\"The title of the tasks, reminders and alerts\"),\n              due_date: z\n                .string()\n                .describe(\"Due date. Must be a valid JavaScript date string\"),\n              task_type: z\n                .enum([\n                  \"Call\",\n                  \"Message\",\n                  \"Todo\",\n                  \"In-Person Meeting\",\n                  \"Email\",\n                  \"Mail\",\n                  \"Text\",\n                  \"Open House\",\n                ])\n                .describe(\"The type of task\"),\n            })\n          )\n          .describe(\"The JSON for task, reminder or alert to create\"),\n      })\n      .describe(\"JSON definition for creating tasks, reminders and alerts\"),\n    func: async (input: { tasks: object }) => JSON.stringify(input),\n  }),\n];\n\nconst executor = await initializeAgentExecutorWithOptions(tools, model, {\n  agentType: \"openai-functions\",\n  verbose: true,\n  handleParsingErrors:\n    \"Please try again, paying close attention to the allowed enum values\",\n});\nconsole.log(\"Loaded agent.\");\n\nconst input = `Set a reminder to renew our online property ads next week.`;\n\nconsole.log(`Executing with input \"${input}\"...`);\n\nconst result = await executor.invoke({ input });\n\nconsole.log({ result });\n\n/*\n  {\n    result: {\n      output: 'I have set a reminder for you to renew your online property ads on October 10th, 2022.'\n    }\n  }\n*/","metadata":{"source":"examples/src/agents/handle_parsing_error.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":65}}}}],["8e760733-6417-4f31-8986-683a0bd49b01",{"pageContent":"import * as fs from \"fs\";\nimport * as yaml from \"js-yaml\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { JsonSpec, JsonObject } from \"langchain/tools\";\nimport { JsonToolkit, createJsonAgent } from \"langchain/agents\";\n\nexport const run = async () => {\n  let data: JsonObject;\n  try {\n    const yamlFile = fs.readFileSync(\"openai_openapi.yaml\", \"utf8\");\n    data = yaml.load(yamlFile) as JsonObject;\n    if (!data) {\n      throw new Error(\"Failed to load OpenAPI spec\");\n    }\n  } catch (e) {\n    console.error(e);\n    return;\n  }\n\n  const toolkit = new JsonToolkit(new JsonSpec(data));\n  const model = new OpenAI({ temperature: 0 });\n  const executor = createJsonAgent(model, toolkit);\n\n  const input = `What are the required parameters in the request body to the /completions endpoint?`;\n\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await executor.call({ input });\n\n  console.log(`Got output ${result.output}`);\n\n  console.log(\n    `Got intermediate steps ${JSON.stringify(\n      result.intermediateSteps,\n      null,\n      2\n    )}`\n  );\n};","metadata":{"source":"examples/src/agents/json.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":39}}}}],["e36c562b-c69e-4a34-9692-e85170295763",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { AgentExecutor } from \"langchain/agents\";\nimport { loadAgent } from \"langchain/agents/load\";\nimport { SerpAPI } from \"langchain/tools\";\nimport { Calculator } from \"langchain/tools/calculator\";\n\nexport const run = async () => {\n  const model = new OpenAI({ temperature: 0 });\n  const tools = [\n    new SerpAPI(process.env.SERPAPI_API_KEY, {\n      location: \"Austin,Texas,United States\",\n      hl: \"en\",\n      gl: \"us\",\n    }),\n    new Calculator(),\n  ];\n\n  const agent = await loadAgent(\n    \"lc://agents/zero-shot-react-description/agent.json\",\n    { llm: model, tools }\n  );\n  console.log(\"Loaded agent from Langchain hub\");\n\n  const executor = AgentExecutor.fromAgentAndTools({\n    agent,\n    tools,\n    returnIntermediateSteps: true,\n  });\n\n  const input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await executor.call({ input });\n\n  console.log(`Got output ${result.output}`);\n};","metadata":{"source":"examples/src/agents/load_from_hub.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":36}}}}],["65ed9bf2-1b6e-43e1-a04a-ba15ba13b561",{"pageContent":"import { initializeAgentExecutorWithOptions } from \"langchain/agents\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { SerpAPI } from \"langchain/tools\";\nimport { Calculator } from \"langchain/tools/calculator\";\n\nconst model = new OpenAI({ temperature: 0 });\nconst tools = [\n  new SerpAPI(process.env.SERPAPI_API_KEY, {\n    location: \"Austin,Texas,United States\",\n    hl: \"en\",\n    gl: \"us\",\n  }),\n  new Calculator(),\n];\n\nconst executor = await initializeAgentExecutorWithOptions(tools, model, {\n  agentType: \"zero-shot-react-description\",\n  verbose: true,\n});\n\nconst input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;\n\nconst result = await executor.call({ input });","metadata":{"source":"examples/src/agents/mrkl.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":23}}}}],["ea69f6ad-b6a1-42cd-83d8-eaa7303da3c9",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { initializeAgentExecutorWithOptions } from \"langchain/agents\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { SerpAPI } from \"langchain/tools\";\nimport { Calculator } from \"langchain/tools/calculator\";\nimport { WebBrowser } from \"langchain/tools/webbrowser\";\n\nexport const run = async () => {\n  const model = new OpenAI({ temperature: 0 });\n  const embeddings = new OpenAIEmbeddings();\n  const tools = [\n    new SerpAPI(process.env.SERPAPI_API_KEY, {\n      location: \"Austin,Texas,United States\",\n      hl: \"en\",\n      gl: \"us\",\n    }),\n    new Calculator(),\n    new WebBrowser({ model, embeddings }),\n  ];\n\n  const executor = await initializeAgentExecutorWithOptions(tools, model, {\n    agentType: \"zero-shot-react-description\",\n    verbose: true,\n  });\n  console.log(\"Loaded agent.\");\n\n  const input = `What is the word of the day on merriam webster. What is the top result on google for that word`;\n\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await executor.call({ input });\n  /*\n  Entering new agent_executor chain...\n  I need to find the word of the day on Merriam Webster and then search for it on Google\n  Action: web-browser\n  Action Input: \"https://www.merriam-webster.com/word-of-the-day\", \"\"\n\n\n  Summary: Merriam-Webster is a website that provides users with a variety of resources, including a dictionary, thesaurus, word finder, word of the day, games and quizzes, and more. The website also allows users to log in and save words, view recents, and access their account settings. The Word of the Day for April 14, 2023 is \"lackadaisical\", which means lacking in life, spirit, or zest. The website also provides quizzes and games to help users build their vocabulary.\n\n  Relevant Links: \n  - [Test Your Vocabulary](https://www.merriam-webster.com/games)\n  - [Thesaurus](https://www.merriam-webster.com/thesaurus)\n  - [Word Finder](https://www.merriam-webster.com/wordfinder)\n  - [Word of the Day](https://www.merriam-webster.com/word-of-the-day)\n  - [Shop](https://shop.merriam-webster.com/?utm_source=mwsite&utm_medium=nav&utm_content=\n  I now need to search for the word of the day on Google\n  Action: search\n  Action Input: \"lackadaisical\"\n  lackadaisical implies a carefree indifference marked by half-hearted efforts. lackadaisical college seniors pretending to study. listless suggests a lack of ...\n  Finished chain.\n  */","metadata":{"source":"examples/src/agents/mrkl_browser.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":52}}}}],["c7633acc-362f-4c0d-8a89-e2a9a62b9bcf",{"pageContent":"console.log(`Got output ${JSON.stringify(result, null, 2)}`);\n  /*\n  Got output {\n    \"output\": \"The word of the day on Merriam Webster is \\\"lackadaisical\\\", which implies a carefree indifference marked by half-hearted efforts.\",\n    \"intermediateSteps\": [\n      {\n        \"action\": {\n          \"tool\": \"web-browser\",\n          \"toolInput\": \"https://www.merriam-webster.com/word-of-the-day\\\", \",\n          \"log\": \" I need to find the word of the day on Merriam Webster and then search for it on Google\\nAction: web-browser\\nAction Input: \\\"https://www.merriam-webster.com/word-of-the-day\\\", \\\"\\\"\"\n        },\n        \"observation\": \"\\n\\nSummary: Merriam-Webster is a website that provides users with a variety of resources, including a dictionary, thesaurus, word finder, word of the day, games and quizzes, and more. The website also allows users to log in and save words, view recents, and access their account settings. The Word of the Day for April 14, 2023 is \\\"lackadaisical\\\", which means lacking in life, spirit, or zest. The website also provides quizzes and games to help users build their vocabulary.\\n\\nRelevant Links: \\n- [Test Your Vocabulary](https://www.merriam-webster.com/games)\\n- [Thesaurus](https://www.merriam-webster.com/thesaurus)\\n- [Word Finder](https://www.merriam-webster.com/wordfinder)\\n- [Word of the Day](https://www.merriam-webster.com/word-of-the-day)\\n- [Shop](https://shop.merriam-webster.com/?utm_source=mwsite&utm_medium=nav&utm_content=\"\n      },\n      {\n        \"action\": {\n          \"tool\": \"search\",\n          \"toolInput\": \"lackadaisical\",\n          \"log\": \" I now need to search for the word of the day on Google\\nAction: search\\nAction Input: \\\"lackadaisical\\\"\"\n        },\n        \"observation\": \"lackadaisical implies a carefree indifference marked by half-hearted efforts. lackadaisical college seniors pretending to study. listless suggests a lack of ...\"\n      }\n    ]\n  }\n  */\n};","metadata":{"source":"examples/src/agents/mrkl_browser.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":54,"to":78}}}}],["d6ec6b76-bec9-45d7-9acd-45036c4b9389",{"pageContent":"import { AgentExecutor, ChatAgentOutputParser } from \"langchain/agents\";\nimport { formatLogToString } from \"langchain/agents/format_scratchpad/log\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { ChatPromptTemplate, PromptTemplate } from \"langchain/prompts\";\nimport { AgentStep } from \"langchain/schema\";\nimport { RunnableSequence } from \"langchain/schema/runnable\";\nimport { SerpAPI } from \"langchain/tools\";\nimport { Calculator } from \"langchain/tools/calculator\";\nimport { renderTextDescription } from \"langchain/tools/render\";\n\n/** Define the model to be used */\nconst model = new OpenAI({ temperature: 0 });\n\n/** Create a list of the tools we're providing to the agent */\nconst tools = [\n  new SerpAPI(process.env.SERPAPI_API_KEY, {\n    location: \"Austin,Texas,United States\",\n    hl: \"en\",\n    gl: \"us\",\n  }),\n  new Calculator(),\n];\n\n/**\n * Define our output parser.\n * In this case we'll use the default output parser\n * for chat agents. `ChatAgentOutputParser`\n */\nconst outputParser = new ChatAgentOutputParser();\n\n/**\n * Define our prompts.\n * For this example we'll use the same default prompts\n * that the `ChatAgent` class uses.\n */\nconst PREFIX = `Answer the following questions as best you can. You have access to the following tools:\n{tools}`;\nconst FORMAT_INSTRUCTIONS = `The way you use the tools is by specifying a json blob, denoted below by $JSON_BLOB\nSpecifically, this $JSON_BLOB should have a \"action\" key (with the name of the tool to use) and a \"action_input\" key (with the input to the tool going here). \nThe $JSON_BLOB should only contain a SINGLE action, do NOT return a list of multiple actions. Here is an example of a valid $JSON_BLOB:\n\n\\`\\`\\`\n{{\n  \"action\": \"calculator\",\n  \"action_input\": \"1 + 2\"\n}}\n\\`\\`\\`\n\nALWAYS use the following format:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: \n\\`\\`\\`\n$JSON_BLOB\n\\`\\`\\`\nObservation: the result of the action\n... (this Thought/Action/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question`;\nconst SUFFIX = `Begin! Reminder to always use the exact characters \\`Final Answer\\` when responding.\nThoughts: {agent_scratchpad}`;\nconst DEFAULT_HUMAN_MESSAGE_TEMPLATE = \"Question: {input}\";\n/**\n * Now we can combine all our prompts together, passing\n * in the required input variables.\n */\n// The `renderTextDescription` util function combines\n// all tool names and descriptions into a single string.\nconst toolStrings = renderTextDescription(tools);\nconst prefixTemplate = PromptTemplate.fromTemplate(PREFIX);\nconst formattedPrefix = await prefixTemplate.format({ tools: toolStrings });\nconst template = [formattedPrefix, FORMAT_INSTRUCTIONS, SUFFIX].join(\"\\n\\n\");\n// Add the template, and human message template to an array of messages.\nconst prompt = ChatPromptTemplate.fromMessages([\n  [\"ai\", template],\n  [\"human\", DEFAULT_HUMAN_MESSAGE_TEMPLATE],\n]);\n\n/**\n * Combine all our previous steps into a runnable agent.\n * We'll use a `RunnableSequence` which takes in an input,\n * and the previous steps. We then format the steps into a\n * string so it can be passed to the prompt.\n */\nconst runnableAgent = RunnableSequence.from([\n  {\n    input: (i: { input: string; steps: AgentStep[] }) => i.input,\n    agent_scratchpad: (i: { input: string; steps: AgentStep[] }) =>\n      formatLogToString(i.steps),\n  },\n  prompt,\n  model,\n  outputParser,\n]);\n\n/**\n * The last step is to pass our agent into the\n * AgentExecutor along with the tools.\n * The AgentExecutor is responsible for actually\n * running the iterations.\n */\nconst executor = AgentExecutor.fromAgentAndTools({\n  agent: runnableAgent,\n  tools,\n});\n\nconsole.log(\"Loaded agent executor\");\n\nconst input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;\nconsole.log(`Calling agent with prompt: ${input}`);","metadata":{"source":"examples/src/agents/mrkl_runnable.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":111}}}}],["f9f4ce2e-30cf-4231-9578-84fd3e6b2735",{"pageContent":"const result = await executor.call({ input });\nconsole.log(result);\n/**\nLoaded agent executor\nCalling agent with prompt: Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\n{\n  output: \"Jason Sudeikis is Olivia Wilde's boyfriend and his current age raised to the 0.23 power is approximately 1.7.\"\n}\n */","metadata":{"source":"examples/src/agents/mrkl_runnable.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":112,"to":120}}}}],["0f106b30-dbf1-469e-a673-195fbe4d8871",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { initializeAgentExecutorWithOptions } from \"langchain/agents\";\nimport { SerpAPI } from \"langchain/tools\";\nimport { Calculator } from \"langchain/tools/calculator\";\nimport process from \"process\";\n\nexport const run = async () => {\n  process.env.LANGCHAIN_TRACING_V2 = \"true\";\n  const model = new OpenAI({ temperature: 0 });\n  const tools = [\n    new SerpAPI(process.env.SERPAPI_API_KEY, {\n      location: \"Austin,Texas,United States\",\n      hl: \"en\",\n      gl: \"us\",\n    }),\n    new Calculator(),\n  ];\n\n  const executor = await initializeAgentExecutorWithOptions(tools, model, {\n    agentType: \"zero-shot-react-description\",\n    verbose: true,\n  });\n  console.log(\"Loaded agent.\");\n\n  const input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;\n\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await executor.call({ input });\n\n  console.log(`Got output ${result.output}`);\n};","metadata":{"source":"examples/src/agents/mrkl_with_tracing.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":32}}}}],["d8a8acaf-8e72-4b67-bef6-91a64e80f375",{"pageContent":"import { initializeAgentExecutorWithOptions } from \"langchain/agents\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { SerpAPI } from \"langchain/tools\";\nimport { Calculator } from \"langchain/tools/calculator\";\n\nconst tools = [new Calculator(), new SerpAPI()];\nconst chat = new ChatOpenAI({ modelName: \"gpt-4\", temperature: 0 });\n\nconst executor = await initializeAgentExecutorWithOptions(tools, chat, {\n  agentType: \"openai-functions\",\n  verbose: true,\n});\n\nconst result = await executor.run(\"What is the weather in New York?\");\nconsole.log(result);\n\n/*\n  The current weather in New York is 72°F with a wind speed of 1 mph coming from the SSW. The humidity is at 89% and the UV index is 0 out of 11. The cloud cover is 79% and there has been no rain.\n*/","metadata":{"source":"examples/src/agents/openai.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":19}}}}],["f2d74435-0a5e-4606-a205-9c70bc28a21b",{"pageContent":"import { initializeAgentExecutorWithOptions } from \"langchain/agents\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { SerpAPI } from \"langchain/tools\";\nimport { Calculator } from \"langchain/tools/calculator\";\n\nconst tools = [new Calculator(), new SerpAPI()];\nconst chat = new ChatOpenAI({ modelName: \"gpt-4\", temperature: 0 });\nconst prefix =\n  \"You are a helpful AI assistant. However, all final response to the user must be in pirate dialect.\";\n\nconst executor = await initializeAgentExecutorWithOptions(tools, chat, {\n  agentType: \"openai-functions\",\n  verbose: true,\n  agentArgs: {\n    prefix,\n  },\n});\n\nconst result = await executor.run(\"What is the weather in New York?\");\nconsole.log(result);\n\n// Arr matey, in New York, it be feelin' like 75 degrees, with a gentle breeze blowin' from the northwest at 3 knots. The air be 77% full o' water, and the clouds be coverin' 35% of the sky. There be no rain in sight, yarr!","metadata":{"source":"examples/src/agents/openai_custom_prompt.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":22}}}}],["271bc995-1a58-4a49-8059-fc05bbad8799",{"pageContent":"import { AgentExecutor } from \"langchain/agents\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { ChatPromptTemplate, MessagesPlaceholder } from \"langchain/prompts\";\nimport {\n  AIMessage,\n  AgentStep,\n  BaseMessage,\n  FunctionMessage,\n} from \"langchain/schema\";\nimport { RunnableSequence } from \"langchain/schema/runnable\";\nimport { SerpAPI, formatToOpenAIFunction } from \"langchain/tools\";\nimport { Calculator } from \"langchain/tools/calculator\";\nimport { OpenAIFunctionsAgentOutputParser } from \"langchain/agents/openai/output_parser\";\n\n/** Define your list of tools. */\nconst tools = [new Calculator(), new SerpAPI()];\n/**\n * Define your chat model to use.\n * In this example we'll use gpt-4 as it is much better\n * at following directions in an agent than other models.\n */\nconst model = new ChatOpenAI({ modelName: \"gpt-4\", temperature: 0 });\n/**\n * Define your prompt for the agent to follow\n * Here we're using `MessagesPlaceholder` to contain our agent scratchpad\n * This is important as later we'll use a util function which formats the agent\n * steps into a list of `BaseMessages` which can be passed into `MessagesPlaceholder`\n */\nconst prompt = ChatPromptTemplate.fromMessages([\n  [\"ai\", \"You are a helpful assistant\"],\n  [\"human\", \"{input}\"],\n  new MessagesPlaceholder(\"agent_scratchpad\"),\n]);\n/**\n * Bind the tools to the LLM.\n * Here we're using the `formatToOpenAIFunction` util function\n * to format our tools into the proper schema for OpenAI functions.\n */\nconst modelWithTools = model.bind({\n  functions: [...tools.map((tool) => formatToOpenAIFunction(tool))],\n});\n/**\n * Define a new agent steps parser.\n */\nconst formatAgentSteps = (steps: AgentStep[]): BaseMessage[] =>\n  steps.flatMap(({ action, observation }) => {\n    if (\"messageLog\" in action && action.messageLog !== undefined) {\n      const log = action.messageLog as BaseMessage[];\n      return log.concat(new FunctionMessage(observation, action.tool));\n    } else {\n      return [new AIMessage(action.log)];\n    }\n  });\n/**\n * Construct the runnable agent.\n *\n * We're using a `RunnableSequence` which takes two inputs:\n * - input --> the users input\n * - agent_scratchpad --> the previous agent steps\n *\n * We're using the `formatForOpenAIFunctions` util function to format the agent\n * steps into a list of `BaseMessages` which can be passed into `MessagesPlaceholder`\n */\nconst runnableAgent = RunnableSequence.from([\n  {\n    input: (i: { input: string; steps: AgentStep[] }) => i.input,\n    agent_scratchpad: (i: { input: string; steps: AgentStep[] }) =>\n      formatAgentSteps(i.steps),\n  },\n  prompt,\n  modelWithTools,\n  new OpenAIFunctionsAgentOutputParser(),\n]);\n/** Pass the runnable along with the tools to create the Agent Executor */\nconst executor = AgentExecutor.fromAgentAndTools({\n  agent: runnableAgent,\n  tools,\n});\n\nconsole.log(\"Loaded agent executor\");\n\nconst query = \"What is the weather in New York?\";\nconsole.log(`Calling agent executor with query: ${query}`);\nconst result = await executor.call({\n  input: query,\n});\nconsole.log(result);\n/*\nLoaded agent executor\nCalling agent executor with query: What is the weather in New York?\n{\n  output: 'The current weather in New York is sunny with a temperature of 66 degrees Fahrenheit. The humidity is at 54% and the wind is blowing at 6 mph. There is 0% chance of precipitation.'\n}\n*/","metadata":{"source":"examples/src/agents/openai_runnable.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":94}}}}],["22fc7253-538b-4665-8520-cc40e8ac4e48",{"pageContent":"import * as fs from \"fs\";\nimport * as yaml from \"js-yaml\";\nimport { JsonSpec, JsonObject } from \"langchain/tools\";\nimport { createOpenApiAgent, OpenApiToolkit } from \"langchain/agents\";\nimport { OpenAI } from \"langchain/llms/openai\";\n\nexport const run = async () => {\n  let data: JsonObject;\n  try {\n    const yamlFile = fs.readFileSync(\"openai_openapi.yaml\", \"utf8\");\n    data = yaml.load(yamlFile) as JsonObject;\n    if (!data) {\n      throw new Error(\"Failed to load OpenAPI spec\");\n    }\n  } catch (e) {\n    console.error(e);\n    return;\n  }\n\n  const headers = {\n    \"Content-Type\": \"application/json\",\n    Authorization: `Bearer ${process.env.OPENAI_API_KEY}`,\n  };\n  const model = new OpenAI({ temperature: 0 });\n  const toolkit = new OpenApiToolkit(new JsonSpec(data), model, headers);\n  const executor = createOpenApiAgent(model, toolkit);\n\n  const input = `Make a POST request to openai /completions. The prompt should be 'tell me a joke.'`;\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await executor.call({ input });\n  console.log(`Got output ${result.output}`);\n\n  console.log(\n    `Got intermediate steps ${JSON.stringify(\n      result.intermediateSteps,\n      null,\n      2\n    )}`\n  );\n};","metadata":{"source":"examples/src/agents/openapi.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":41}}}}],["49065315-bf79-4904-af2d-f422d34e2c47",{"pageContent":"import { Calculator } from \"langchain/tools/calculator\";\nimport { SerpAPI } from \"langchain/tools\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { PlanAndExecuteAgentExecutor } from \"langchain/experimental/plan_and_execute\";\n\nconst tools = [new Calculator(), new SerpAPI()];\nconst model = new ChatOpenAI({\n  temperature: 0,\n  modelName: \"gpt-3.5-turbo\",\n  verbose: true,\n});\nconst executor = await PlanAndExecuteAgentExecutor.fromLLMAndTools({\n  llm: model,\n  tools,\n});\n\nconst result = await executor.call({\n  input: `Who is the current president of the United States? What is their current age raised to the second power?`,\n});\n\nconsole.log({ result });","metadata":{"source":"examples/src/agents/plan_and_execute.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":21}}}}],["af15dc56-d8ef-4310-8366-dce269f464ea",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { SqlDatabase } from \"langchain/sql_db\";\nimport { createSqlAgent, SqlToolkit } from \"langchain/agents/toolkits/sql\";\nimport { DataSource } from \"typeorm\";\n\n/** This example uses Chinook database, which is a sample database available for SQL Server, Oracle, MySQL, etc.\n * To set it up follow the instructions on https://database.guide/2-sample-databases-sqlite/, placing the .db file\n * in the examples folder.\n */\nexport const run = async () => {\n  const datasource = new DataSource({\n    type: \"sqlite\",\n    database: \"Chinook.db\",\n  });\n  const db = await SqlDatabase.fromDataSourceParams({\n    appDataSource: datasource,\n  });\n  const model = new OpenAI({ temperature: 0 });\n  const toolkit = new SqlToolkit(db, model);\n  const executor = createSqlAgent(model, toolkit);\n\n  const input = `List the total sales per country. Which country's customers spent the most?`;\n\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await executor.call({ input });\n\n  console.log(`Got output ${result.output}`);\n\n  console.log(\n    `Got intermediate steps ${JSON.stringify(\n      result.intermediateSteps,\n      null,\n      2\n    )}`\n  );\n\n  await datasource.destroy();\n};","metadata":{"source":"examples/src/agents/sql.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":39}}}}],["5e895393-9189-4482-8d62-3b482969a262",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { SqlDatabase } from \"langchain/sql_db\";\nimport { createSqlAgent, SqlToolkit } from \"langchain/agents/toolkits/sql\";\nimport { DataSource } from \"typeorm\";\n\n/**\n * This example uses a SAP HANA Cloud database. You can create a free trial database via https://developers.sap.com/tutorials/hana-cloud-deploying.html\n *\n * You will need to add the following packages to your package.json as they are required when using typeorm with SAP HANA:\n *\n *    \"hdb-pool\": \"^0.1.6\",             (or latest version)\n *    \"@sap/hana-client\": \"^2.17.22\"    (or latest version)\n *\n */\nexport const run = async () => {\n  const datasource = new DataSource({\n    type: \"sap\",\n    host: \"<ADD_YOURS_HERE>.hanacloud.ondemand.com\",\n    port: 443,\n    username: \"<ADD_YOURS_HERE>\",\n    password: \"<ADD_YOURS_HERE>\",\n    schema: \"<ADD_YOURS_HERE>\",\n    encrypt: true,\n    extra: {\n      sslValidateCertificate: false,\n    },\n  });\n\n  // A custom SQL_PREFIX is required because we want to be explicit that a schema name is needed when querying HANA fool-proof (line 33)\n  const custom_SQL_PREFIX = `You are an agent designed to interact with a SQL database.\n        Given an input question, create a syntactically correct SAP HANA query to run, then look at the results of the query and return the answer.\n        Unless the user specifies a specific number of examples they wish to obtain, always limit your query to at most {top_k} results using the LIMIT clause.\n        You can order the results by a relevant column to return the most interesting examples in the database.\n        Never query for all the columns from a specific table, only ask for a the few relevant columns given the question.\n        You have access to tools for interacting with the database.\n        Only use the below tools. Only use the information returned by the below tools to construct your final answer.\n        You MUST double check your query before executing it. If you get an error while executing a query, rewrite the query and try again.\n        Always use a schema name when running a query.\n        \n        DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database.\n        \n        If the question does not seem related to the database, just return \"I don't know\" as the answer.`;\n\n  const db = await SqlDatabase.fromDataSourceParams({\n    appDataSource: datasource,\n  });\n\n  const model = new OpenAI({ temperature: 0 });\n  const toolkit = new SqlToolkit(db, model);\n  const executor = createSqlAgent(model, toolkit, {\n    prefix: custom_SQL_PREFIX,\n  });\n\n  const input = `List the total sales per country. Which country's customers spent the most?`;\n\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await executor.call({ input });\n\n  console.log(`Got output ${result.output}`);\n\n  console.log(\n    `Got intermediate steps ${JSON.stringify(\n      result.intermediateSteps,\n      null,\n      2\n    )}`\n  );\n\n  await datasource.destroy();\n};","metadata":{"source":"examples/src/agents/sql_sap_hana.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":71}}}}],["52409e6c-e915-4a50-84b2-0842ffa4d423",{"pageContent":"import { LLMChain } from \"langchain/chains\";\nimport { AgentExecutor, ZeroShotAgent } from \"langchain/agents\";\nimport { BaseCallbackHandler } from \"langchain/callbacks\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { Calculator } from \"langchain/tools/calculator\";\nimport { AgentAction } from \"langchain/schema\";\nimport { Serialized } from \"langchain/load/serializable\";\n\nexport const run = async () => {\n  // You can implement your own callback handler by extending BaseCallbackHandler\n  class CustomHandler extends BaseCallbackHandler {\n    name = \"custom_handler\";\n\n    handleLLMNewToken(token: string) {\n      console.log(\"token\", { token });\n    }\n\n    handleLLMStart(llm: Serialized, _prompts: string[]) {\n      console.log(\"handleLLMStart\", { llm });\n    }\n\n    handleChainStart(chain: Serialized) {\n      console.log(\"handleChainStart\", { chain });\n    }\n\n    handleAgentAction(action: AgentAction) {\n      console.log(\"handleAgentAction\", action);\n    }\n\n    handleToolStart(tool: Serialized) {\n      console.log(\"handleToolStart\", { tool });\n    }\n  }\n\n  const handler1 = new CustomHandler();\n\n  // Additionally, you can use the `fromMethods` method to create a callback handler\n  const handler2 = BaseCallbackHandler.fromMethods({\n    handleLLMStart(llm, _prompts: string[]) {\n      console.log(\"handleLLMStart: I'm the second handler!!\", { llm });\n    },\n    handleChainStart(chain) {\n      console.log(\"handleChainStart: I'm the second handler!!\", { chain });\n    },\n    handleAgentAction(action) {\n      console.log(\"handleAgentAction\", action);\n    },\n    handleToolStart(tool) {\n      console.log(\"handleToolStart\", { tool });\n    },\n  });\n\n  // You can restrict callbacks to a particular object by passing it upon creation\n  const model = new ChatOpenAI({\n    temperature: 0,\n    callbacks: [handler2], // this will issue handler2 callbacks related to this model\n    streaming: true, // needed to enable streaming, which enables handleLLMNewToken\n  });\n\n  const tools = [new Calculator()];\n  const agentPrompt = ZeroShotAgent.createPrompt(tools);\n\n  const llmChain = new LLMChain({\n    llm: model,\n    prompt: agentPrompt,\n    callbacks: [handler2], // this will issue handler2 callbacks related to this chain\n  });\n  const agent = new ZeroShotAgent({\n    llmChain,\n    allowedTools: [\"search\"],\n  });\n\n  const agentExecutor = AgentExecutor.fromAgentAndTools({\n    agent,\n    tools,\n  });","metadata":{"source":"examples/src/agents/streaming.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":76}}}}],["2cd60b99-3b4b-45de-877c-c4d2506fb27b",{"pageContent":"/*\n   * When we pass the callback handler to the agent executor, it will be used for all\n   * callbacks related to the agent and all the objects involved in the agent's\n   * execution, in this case, the Tool, LLMChain, and LLM.\n   *\n   * The `handler2` callback handler will only be used for callbacks related to the\n   * LLMChain and LLM, since we passed it to the LLMChain and LLM objects upon creation.\n   */\n  const result = await agentExecutor.call(\n    {\n      input: \"What is 2 to the power of 8\",\n    },\n    [handler1]\n  ); // this is needed to see handleAgentAction\n  /*\n  handleChainStart { chain: { name: 'agent_executor' } }\n  handleChainStart { chain: { name: 'llm_chain' } }\n  handleChainStart: I'm the second handler!! { chain: { name: 'llm_chain' } }\n  handleLLMStart { llm: { name: 'openai' } }\n  handleLLMStart: I'm the second handler!! { llm: { name: 'openai' } }\n  token { token: '' }\n  token { token: 'I' }\n  token { token: ' can' }\n  token { token: ' use' }\n  token { token: ' the' }\n  token { token: ' calculator' }\n  token { token: ' tool' }\n  token { token: ' to' }\n  token { token: ' solve' }\n  token { token: ' this' }\n  token { token: '.\\n' }\n  token { token: 'Action' }\n  token { token: ':' }\n  token { token: ' calculator' }\n  token { token: '\\n' }\n  token { token: 'Action' }\n  token { token: ' Input' }\n  token { token: ':' }\n  token { token: ' ' }\n  token { token: '2' }\n  token { token: '^' }\n  token { token: '8' }\n  token { token: '' }\n  handleAgentAction {\n    tool: 'calculator',\n    toolInput: '2^8',\n    log: 'I can use the calculator tool to solve this.\\n' +\n      'Action: calculator\\n' +\n      'Action Input: 2^8'\n  }\n  handleToolStart { tool: { name: 'calculator' } }\n  handleChainStart { chain: { name: 'llm_chain' } }\n  handleChainStart: I'm the second handler!! { chain: { name: 'llm_chain' } }\n  handleLLMStart { llm: { name: 'openai' } }\n  handleLLMStart: I'm the second handler!! { llm: { name: 'openai' } }\n  token { token: '' }\n  token { token: 'That' }\n  token { token: ' was' }\n  token { token: ' easy' }\n  token { token: '!\\n' }\n  token { token: 'Final' }\n  token { token: ' Answer' }\n  token { token: ':' }\n  token { token: ' ' }\n  token { token: '256' }\n  token { token: '' }\n  */\n\n  console.log(result);\n  /*\n  {\n    output: '256',\n    __run: { runId: '26d481a6-4410-4f39-b74d-f9a4f572379a' }\n  }\n  */\n};","metadata":{"source":"examples/src/agents/streaming.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":78,"to":153}}}}],["58f51c18-539f-4a88-bf30-38fa253dc6bc",{"pageContent":"import { z } from \"zod\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { initializeAgentExecutorWithOptions } from \"langchain/agents\";\nimport { Calculator } from \"langchain/tools/calculator\";\nimport { DynamicStructuredTool } from \"langchain/tools\";\n\nexport const run = async () => {\n  const model = new ChatOpenAI({ temperature: 0 });\n  const tools = [\n    new Calculator(), // Older existing single input tools will still work\n    new DynamicStructuredTool({\n      name: \"random-number-generator\",\n      description: \"generates a random number between two input numbers\",\n      schema: z.object({\n        low: z.number().describe(\"The lower bound of the generated number\"),\n        high: z.number().describe(\"The upper bound of the generated number\"),\n      }),\n      func: async ({ low, high }) =>\n        (Math.random() * (high - low) + low).toString(), // Outputs still must be strings\n      returnDirect: false, // This is an option that allows the tool to return the output directly\n    }),\n  ];\n\n  const executor = await initializeAgentExecutorWithOptions(tools, model, {\n    agentType: \"structured-chat-zero-shot-react-description\",\n    verbose: true,\n  });\n  console.log(\"Loaded agent.\");\n\n  const input = `What is a random number between 5 and 10 raised to the second power?`;\n\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await executor.call({ input });\n\n  console.log({ result });\n\n  /*\n    {\n      \"output\": \"67.95299776074\"\n    }\n  */\n};","metadata":{"source":"examples/src/agents/structured_chat.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":43}}}}],["edee7678-ca92-4f88-81b9-907255a681ce",{"pageContent":"import { z } from \"zod\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport {\n  AgentExecutor,\n  StructuredChatOutputParserWithRetries,\n} from \"langchain/agents\";\nimport { Calculator } from \"langchain/tools/calculator\";\nimport { DynamicStructuredTool } from \"langchain/tools\";\nimport {\n  ChatPromptTemplate,\n  HumanMessagePromptTemplate,\n  PromptTemplate,\n  SystemMessagePromptTemplate,\n} from \"langchain/prompts\";\nimport { renderTextDescriptionAndArgs } from \"langchain/tools/render\";\nimport { RunnableSequence } from \"langchain/schema/runnable\";\nimport { AgentStep } from \"langchain/schema\";\nimport { formatLogToString } from \"langchain/agents/format_scratchpad/log\";\n\n/**\n * Need:\n * memory\n * multi input tools\n */\n\n/** Define the chat model. */\nconst model = new ChatOpenAI({ temperature: 0 }).bind({\n  stop: [\"\\nObservation:\"],\n});\n/** Define your list of tools, including the `DynamicStructuredTool` */\nconst tools = [\n  new Calculator(), // Older existing single input tools will still work\n  new DynamicStructuredTool({\n    name: \"random-number-generator\",\n    description: \"generates a random number between two input numbers\",\n    schema: z.object({\n      low: z.number().describe(\"The lower bound of the generated number\"),\n      high: z.number().describe(\"The upper bound of the generated number\"),\n    }),\n    func: async ({ low, high }) =>\n      (Math.random() * (high - low) + low).toString(), // Outputs still must be strings\n    returnDirect: false, // This is an option that allows the tool to return the output directly\n  }),\n];\nconst toolNames = tools.map((tool) => tool.name);\n\n/**\n * Create your prompt.\n * Here we'll use three prompt strings: prefix, format instructions and suffix.\n * With these we'll format the prompt with the tool schemas and names.\n */\nconst PREFIX = `Answer the following questions truthfully and as best you can.`;\nconst AGENT_ACTION_FORMAT_INSTRUCTIONS = `Output a JSON markdown code snippet containing a valid JSON blob (denoted below by $JSON_BLOB).\nThis $JSON_BLOB must have a \"action\" key (with the name of the tool to use) and an \"action_input\" key (tool input).\n\nValid \"action\" values: \"Final Answer\" (which you must use when giving your final response to the user), or one of [{tool_names}].\n\nThe $JSON_BLOB must be valid, parseable JSON and only contain a SINGLE action. Here is an example of an acceptable output:\n\n\\`\\`\\`json\n{{\n  \"action\": $TOOL_NAME,\n  \"action_input\": $INPUT\n}}\n\\`\\`\\`\n\nRemember to include the surrounding markdown code snippet delimiters (begin with \"\\`\\`\\`\" json and close with \"\\`\\`\\`\")!\n`;","metadata":{"source":"examples/src/agents/structured_chat_runnable.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":68}}}}],["7acc2880-3f0d-4b7b-a48d-b4cf2e815e2e",{"pageContent":"const FORMAT_INSTRUCTIONS = `You have access to the following tools.\nYou must format your inputs to these tools to match their \"JSON schema\" definitions below.\n\n\"JSON Schema\" is a declarative language that allows you to annotate and validate JSON documents.\n\nFor example, the example \"JSON Schema\" instance {{\"properties\": {{\"foo\": {{\"description\": \"a list of test words\", \"type\": \"array\", \"items\": {{\"type\": \"string\"}}}}}}, \"required\": [\"foo\"]}}}}\nwould match an object with one required property, \"foo\". The \"type\" property specifies \"foo\" must be an \"array\", and the \"description\" property semantically describes it as \"a list of test words\". The items within \"foo\" must be strings.\nThus, the object {{\"foo\": [\"bar\", \"baz\"]}} is a well-formatted instance of this example \"JSON Schema\". The object {{\"properties\": {{\"foo\": [\"bar\", \"baz\"]}}}} is not well-formatted.\n\nHere are the JSON Schema instances for the tools you have access to:\n\n{tool_schemas}\n\nThe way you use the tools is as follows:\n\n------------------------\n\n${AGENT_ACTION_FORMAT_INSTRUCTIONS}\n\nIf you are using a tool, \"action_input\" must adhere to the tool's input schema, given above.\n\n------------------------\n\nALWAYS use the following format:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction:\n\\`\\`\\`json\n$JSON_BLOB\n\\`\\`\\`\nObservation: the result of the action\n... (this Thought/Action/Observation can repeat N times)\nThought: I now know the final answer\nAction:\n\\`\\`\\`json\n{{\n  \"action\": \"Final Answer\",\n  \"action_input\": \"Final response to human\"\n}}\n\\`\\`\\``;\nconst SUFFIX = `Begin! Reminder to ALWAYS use the above format, and to use tools if appropriate.`;\nconst inputVariables = [\"input\", \"agent_scratchpad\"];\nconst template = [\n  PREFIX,\n  FORMAT_INSTRUCTIONS,\n  SUFFIX,\n  `Thoughts: {agent_scratchpad}`,\n].join(\"\\n\\n\");\nconst humanMessageTemplate = \"{input}\";\nconst messages = [\n  new SystemMessagePromptTemplate(\n    new PromptTemplate({\n      template,\n      inputVariables,\n      partialVariables: {\n        tool_schemas: renderTextDescriptionAndArgs(tools),\n        tool_names: toolNames.join(\", \"),\n      },\n    })\n  ),\n  new HumanMessagePromptTemplate(\n    new PromptTemplate({\n      template: humanMessageTemplate,\n      inputVariables,\n    })\n  ),\n];\nconst prompt = ChatPromptTemplate.fromMessages(messages);\n\n/**\n * Now we can create our output parser.\n * For this, we'll use the pre-built `StructuredChatOutputParserWithRetries`\n *\n * @important This step is very important and not to be overlooked for one main reason: retries.\n * If the agent fails to produce a valid output, it will preform retries to try and coerce the agent\n * into producing a valid output.\n *\n * @important You can not pass in the same model we're using in the executor since it has stop tokens\n * bound to it, and the implementation of `StructuredChatOutputParserWithRetries.fromLLM` does not accept\n * LLMs of this type.\n */\nconst outputParser = StructuredChatOutputParserWithRetries.fromLLM(\n  new ChatOpenAI({ temperature: 0 }),\n  {\n    toolNames,\n  }\n);\n\n/**\n * Finally, construct the runnable agent using a\n * `RunnableSequence` and pass it to the agent executor\n */\nconst runnableAgent = RunnableSequence.from([\n  {\n    input: (i: { input: string; steps: AgentStep[] }) => i.input,\n    agent_scratchpad: (i: { input: string; steps: AgentStep[] }) =>\n      formatLogToString(i.steps),\n  },\n  prompt,\n  model,\n  outputParser,\n]);\n\nconst executor = AgentExecutor.fromAgentAndTools({\n  agent: runnableAgent,\n  tools,\n});\n\nconsole.log(\"Loaded agent.\");\n\nconst input = `What is a random number between 5 and 10 raised to the second power?`;\nconsole.log(`Executing with input \"${input}\"...`);\nconst result = await executor.call({ input });\nconsole.log(result);\n\n/*\nLoaded agent.\nExecuting with input \"What is a random number between 5 and 10 raised to the second power?\"...\n{ output: '67.02412461717323' }\n*/","metadata":{"source":"examples/src/agents/structured_chat_runnable.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":69,"to":189}}}}],["f3834537-ecd7-4f37-8517-f7e753b418b6",{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { initializeAgentExecutorWithOptions } from \"langchain/agents\";\nimport { Calculator } from \"langchain/tools/calculator\";\nimport { MessagesPlaceholder } from \"langchain/prompts\";\nimport { BufferMemory } from \"langchain/memory\";\n\nexport const run = async () => {\n  const model = new ChatOpenAI({ temperature: 0 });\n  const tools = [new Calculator()];\n\n  const executor = await initializeAgentExecutorWithOptions(tools, model, {\n    agentType: \"structured-chat-zero-shot-react-description\",\n    verbose: true,\n    memory: new BufferMemory({\n      memoryKey: \"chat_history\",\n      returnMessages: true,\n    }),\n    agentArgs: {\n      inputVariables: [\"input\", \"agent_scratchpad\", \"chat_history\"],\n      memoryPrompts: [new MessagesPlaceholder(\"chat_history\")],\n    },\n  });\n\n  const result = await executor.call({ input: `what is 9 to the 2nd power?` });\n\n  console.log(result);\n\n  /*\n    {\n      \"output\": \"81\"\n    }\n  */\n\n  const result2 = await executor.call({\n    input: `what is that number squared?`,\n  });\n\n  console.log(result2);\n\n  /*\n    {\n      \"output\": \"6561\"\n    }\n  */\n};","metadata":{"source":"examples/src/agents/structured_chat_with_memory.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":45}}}}],["96b59be4-9e44-458d-989e-dc871d78f5b1",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport * as fs from \"fs\";\nimport {\n  VectorStoreToolkit,\n  createVectorStoreAgent,\n  VectorStoreInfo,\n} from \"langchain/agents\";\n\nconst model = new OpenAI({ temperature: 0 });\n/* Load in the file we want to do question answering over */\nconst text = fs.readFileSync(\"state_of_the_union.txt\", \"utf8\");\n/* Split the text into chunks using character, not token, size */\nconst textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });\nconst docs = await textSplitter.createDocuments([text]);\n/* Create the vectorstore */\nconst vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());\n\n/* Create the agent */\nconst vectorStoreInfo: VectorStoreInfo = {\n  name: \"state_of_union_address\",\n  description: \"the most recent state of the Union address\",\n  vectorStore,\n};\n\nconst toolkit = new VectorStoreToolkit(vectorStoreInfo, model);\nconst agent = createVectorStoreAgent(model, toolkit);\n\nconst input =\n  \"What did biden say about Ketanji Brown Jackson is the state of the union address?\";\nconsole.log(`Executing: ${input}`);\n\nconst result = await agent.call({ input });\nconsole.log(`Got output ${result.output}`);\nconsole.log(\n  `Got intermediate steps ${JSON.stringify(result.intermediateSteps, null, 2)}`\n);","metadata":{"source":"examples/src/agents/vectorstore.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":39}}}}],["56a254b6-2775-4264-9b88-2fe783f163c3",{"pageContent":"import { ChatAnthropic } from \"langchain/chat_models/anthropic\";\nimport { initializeAgentExecutorWithOptions } from \"langchain/agents\";\nimport { SerpAPI } from \"langchain/tools\";\n\nconst model = new ChatAnthropic({ modelName: \"claude-2\", temperature: 0.1 });\nconst tools = [new SerpAPI()];\n\nconst executor = await initializeAgentExecutorWithOptions(tools, model, {\n  agentType: \"xml\",\n  verbose: true,\n});\nconsole.log(\"Loaded agent.\");\n\nconst input = `What is the weather in Honolulu?`;\n\nconst result = await executor.call({ input });\n\nconsole.log(result);\n\n/*\n  https://smith.langchain.com/public/d0acd50a-f99d-4af0-ae66-9009de319fb5/r\n  {\n    output: 'The weather in Honolulu is currently 75 degrees Fahrenheit with a small craft advisory in effect. The forecast calls for generally clear skies tonight with a low of 75 degrees.'\n  }\n*/","metadata":{"source":"examples/src/agents/xml.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":25}}}}],["54373c4d-eede-4514-a0fc-9755d65fb8c2",{"pageContent":"import { ChatAnthropic } from \"langchain/chat_models/anthropic\";\nimport { AgentExecutor } from \"langchain/agents\";\nimport { SerpAPI, Tool } from \"langchain/tools\";\nimport {\n  ChatPromptTemplate,\n  HumanMessagePromptTemplate,\n  MessagesPlaceholder,\n} from \"langchain/prompts\";\nimport { RunnableSequence } from \"langchain/schema/runnable\";\nimport { AgentStep } from \"langchain/schema\";\nimport { XMLAgentOutputParser } from \"langchain/agents/xml/output_parser\";\nimport { renderTextDescription } from \"langchain/tools/render\";\nimport { formatLogToMessage } from \"langchain/agents/format_scratchpad/log_to_message\";\n\n/**\n * Define your chat model.\n * In this case we'll use Claude since it preforms well on XML related tasks\n */\nconst model = new ChatAnthropic({ modelName: \"claude-2\", temperature: 0 }).bind(\n  {\n    stop: [\"</tool_input>\", \"</final_answer>\"],\n  }\n);\n/** Define your list of tools. */\nconst tools = [new SerpAPI()];\n\n/**\n * Construct your prompt.\n * For XML not too much work is necessary, we just need to\n * define our prompt, and a messages placeholder for the\n * previous agent steps.\n */\nconst AGENT_INSTRUCTIONS = `You are a helpful assistant. Help the user answer any questions.\n\nYou have access to the following tools:\n\n{tools}\n\nIn order to use a tool, you can use <tool></tool> and <tool_input></tool_input> tags.\nYou will then get back a response in the form <observation></observation>\nFor example, if you have a tool called 'search' that could run a google search, in order to search for the weather in SF you would respond:\n\n<tool>search</tool><tool_input>weather in SF</tool_input>\n<observation>64 degrees</observation>\n\nWhen you are done, respond with a final answer between <final_answer></final_answer>. For example:\n\n<final_answer>The weather in SF is 64 degrees</final_answer>\n\nBegin!\n\nQuestion: {input}`;\nconst prompt = ChatPromptTemplate.fromMessages([\n  HumanMessagePromptTemplate.fromTemplate(AGENT_INSTRUCTIONS),\n  new MessagesPlaceholder(\"agent_scratchpad\"),\n]);\n\n/**\n * Next construct your runnable agent using a `RunnableSequence`\n * which takes in two arguments: input and agent_scratchpad.\n * The agent_scratchpad is then formatted using the `formatLogToMessage`\n * util because we're using a `MessagesPlaceholder` in our prompt.\n *\n * We also need to pass our tools through formatted as a string since\n * our prompt function does not format the prompt.\n */\nconst runnableAgent = RunnableSequence.from([\n  {\n    input: (i: { input: string; tools: Tool[]; steps: AgentStep[] }) => i.input,\n    agent_scratchpad: (i: {\n      input: string;\n      tools: Tool[];\n      steps: AgentStep[];\n    }) => formatLogToMessage(i.steps),\n    tools: (i: { input: string; tools: Tool[]; steps: AgentStep[] }) =>\n      renderTextDescription(i.tools),\n  },\n  prompt,\n  model,\n  new XMLAgentOutputParser(),\n]);\n\n/**\n * Finally, we can define our agent executor and call it with an input.\n */\nconst executor = AgentExecutor.fromAgentAndTools({\n  agent: runnableAgent,\n  tools,\n});\n\nconsole.log(\"Loaded agent.\");\n\nconst input = `What is the weather in Honolulu?`;\nconsole.log(`Calling executor with input: ${input}`);\nconst result = await executor.call({ input, tools });\nconsole.log(result);\n\n/*\nLoaded agent.\nCalling executor with input: What is the weather in Honolulu?\n{\n  output: '\\n' +\n    'The weather in Honolulu is mostly sunny with a high of 72 degrees Fahrenheit, 2% chance of rain, 91% humidity, and winds around 2 mph.\\n'\n}\n*/","metadata":{"source":"examples/src/agents/xml_runnable.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":105}}}}],["94c66112-0c3c-49ad-8308-760e658c35ce",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { ZapierNLAWrapper } from \"langchain/tools\";\nimport {\n  initializeAgentExecutorWithOptions,\n  ZapierToolKit,\n} from \"langchain/agents\";\n\nconst model = new OpenAI({ temperature: 0 });\nconst zapier = new ZapierNLAWrapper();\nconst toolkit = await ZapierToolKit.fromZapierNLAWrapper(zapier);\n\nconst executor = await initializeAgentExecutorWithOptions(\n  toolkit.tools,\n  model,\n  {\n    agentType: \"zero-shot-react-description\",\n    verbose: true,\n  }\n);\nconsole.log(\"Loaded agent.\");\n\nconst input = `Summarize the last email I received regarding Silicon Valley Bank. Send the summary to the #test-zapier Slack channel.`;\n\nconsole.log(`Executing with input \"${input}\"...`);\n\nconst result = await executor.call({ input });\n\nconsole.log(`Got output ${result.output}`);","metadata":{"source":"examples/src/agents/zapier_mrkl.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":28}}}}],["97807ef3-91ce-4fc9-a8a4-b43ab808ec70",{"pageContent":"import type { KVNamespace } from \"@cloudflare/workers-types\";\n\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { CloudflareKVCache } from \"langchain/cache/cloudflare_kv\";\n\nexport interface Env {\n  KV_NAMESPACE: KVNamespace;\n  OPENAI_API_KEY: string;\n}\n\nexport default {\n  async fetch(_request: Request, env: Env) {\n    try {\n      const cache = new CloudflareKVCache(env.KV_NAMESPACE);\n      const model = new OpenAI({\n        cache,\n        modelName: \"gpt-3.5-turbo-instruct\",\n        openAIApiKey: env.OPENAI_API_KEY,\n      });\n      const response = await model.invoke(\"How are you today?\");\n      return new Response(JSON.stringify(response), {\n        headers: { \"content-type\": \"application/json\" },\n      });\n    } catch (err: any) {\n      console.log(err.message);\n      return new Response(err.message, { status: 500 });\n    }\n  },\n};","metadata":{"source":"examples/src/cache/cloudflare_kv.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":29}}}}],["a360160d-8b72-4971-8f4f-b3b7e554353c",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { MomentoCache } from \"langchain/cache/momento\";\nimport {\n  CacheClient,\n  Configurations,\n  CredentialProvider,\n} from \"@gomomento/sdk\"; // `from \"gomomento/sdk-web\";` for browser/edge\n\n// See https://github.com/momentohq/client-sdk-javascript for connection options\nconst client = new CacheClient({\n  configuration: Configurations.Laptop.v1(),\n  credentialProvider: CredentialProvider.fromEnvironmentVariable({\n    environmentVariableName: \"MOMENTO_API_KEY\",\n  }),\n  defaultTtlSeconds: 60 * 60 * 24,\n});\nconst cache = await MomentoCache.fromProps({\n  client,\n  cacheName: \"langchain\",\n});\n\nconst model = new OpenAI({ cache });","metadata":{"source":"examples/src/cache/momento.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":22}}}}],["d50c64c1-d448-4eba-ad47-c906cd2d17cb",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { UpstashRedisCache } from \"langchain/cache/upstash_redis\";\n\n// See https://docs.upstash.com/redis/howto/connectwithupstashredis#quick-start for connection options\nconst cache = new UpstashRedisCache({\n  config: {\n    url: \"UPSTASH_REDIS_REST_URL\",\n    token: \"UPSTASH_REDIS_REST_TOKEN\",\n  },\n});\n\nconst model = new OpenAI({ cache });","metadata":{"source":"examples/src/cache/upstash_redis.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":12}}}}],["4e2a2feb-bf88-4f92-8824-f34459c5dd38",{"pageContent":"import { Redis } from \"@upstash/redis\";\nimport https from \"https\";\n\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { UpstashRedisCache } from \"langchain/cache/upstash_redis\";\n\n// const client = new Redis({\n//   url: process.env.UPSTASH_REDIS_REST_URL!,\n//   token: process.env.UPSTASH_REDIS_REST_TOKEN!,\n//   agent: new https.Agent({ keepAlive: true }),\n// });\n\n// Or simply call Redis.fromEnv() to automatically load the UPSTASH_REDIS_REST_URL and UPSTASH_REDIS_REST_TOKEN environment variables.\nconst client = Redis.fromEnv({\n  agent: new https.Agent({ keepAlive: true }),\n});\n\nconst cache = new UpstashRedisCache({ client });\nconst model = new OpenAI({ cache });","metadata":{"source":"examples/src/cache/upstash_redis_advanced.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":19}}}}],["7169f8c0-91e3-4836-a1bb-14f7e27f0697",{"pageContent":"import type { KVNamespace } from \"@cloudflare/workers-types\";\n\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { CloudflareKVCache } from \"langchain/cache/cloudflare_kv\";\n\nexport interface Env {\n  KV_NAMESPACE: KVNamespace;\n  OPENAI_API_KEY: string;\n}\n\nexport default {\n  async fetch(_request: Request, env: Env) {\n    try {\n      const cache = new CloudflareKVCache(env.KV_NAMESPACE);\n      const model = new ChatOpenAI({\n        cache,\n        modelName: \"gpt-3.5-turbo\",\n        openAIApiKey: env.OPENAI_API_KEY,\n      });\n      const response = await model.invoke(\"How are you today?\");\n      return new Response(JSON.stringify(response), {\n        headers: { \"content-type\": \"application/json\" },\n      });\n    } catch (err: any) {\n      console.log(err.message);\n      return new Response(err.message, { status: 500 });\n    }\n  },\n};","metadata":{"source":"examples/src/cache/chat_models/cloudflare_kv.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":29}}}}],["8b9ac5ca-b25b-4a98-93f2-568379fe8d13",{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { MomentoCache } from \"langchain/cache/momento\";\nimport {\n  CacheClient,\n  Configurations,\n  CredentialProvider,\n} from \"@gomomento/sdk\";\n\n// See https://github.com/momentohq/client-sdk-javascript for connection options\nconst client = new CacheClient({\n  configuration: Configurations.Laptop.v1(),\n  credentialProvider: CredentialProvider.fromEnvironmentVariable({\n    environmentVariableName: \"MOMENTO_API_KEY\",\n  }),\n  defaultTtlSeconds: 60 * 60 * 24,\n});\nconst cache = await MomentoCache.fromProps({\n  client,\n  cacheName: \"langchain\",\n});\n\nconst model = new ChatOpenAI({ cache });","metadata":{"source":"examples/src/cache/chat_models/momento.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":22}}}}],["e2c4620c-00ac-4952-9268-1dff065a9626",{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { RedisCache } from \"langchain/cache/ioredis\";\nimport { Redis } from \"ioredis\";\n\nconst client = new Redis(\"redis://localhost:6379\");\n\nconst cache = new RedisCache(client, {\n  ttl: 60, // Optional key expiration value\n});\n\nconst model = new ChatOpenAI({ cache });\n\nconst response1 = await model.invoke(\"Do something random!\");\nconsole.log(response1);\n/*\n  AIMessage {\n    content: \"Sure! I'll generate a random number for you: 37\",\n    additional_kwargs: {}\n  }\n*/\n\nconst response2 = await model.invoke(\"Do something random!\");\nconsole.log(response2);\n/*\n  AIMessage {\n    content: \"Sure! I'll generate a random number for you: 37\",\n    additional_kwargs: {}\n  }\n*/\n\nawait client.disconnect();","metadata":{"source":"examples/src/cache/chat_models/redis.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":31}}}}],["566c0de2-890e-4cf5-9ecf-bf75a867a2af",{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { UpstashRedisCache } from \"langchain/cache/upstash_redis\";\n\n// See https://docs.upstash.com/redis/howto/connectwithupstashredis#quick-start for connection options\nconst cache = new UpstashRedisCache({\n  config: {\n    url: \"UPSTASH_REDIS_REST_URL\",\n    token: \"UPSTASH_REDIS_REST_TOKEN\",\n  },\n});\n\nconst model = new ChatOpenAI({ cache });","metadata":{"source":"examples/src/cache/chat_models/upstash_redis.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":12}}}}],["376ecbb5-3ae6-4ce4-84c3-89eaa18a1960",{"pageContent":"import { Redis } from \"@upstash/redis\";\nimport https from \"https\";\n\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { UpstashRedisCache } from \"langchain/cache/upstash_redis\";\n\n// const client = new Redis({\n//   url: process.env.UPSTASH_REDIS_REST_URL!,\n//   token: process.env.UPSTASH_REDIS_REST_TOKEN!,\n//   agent: new https.Agent({ keepAlive: true }),\n// });\n\n// Or simply call Redis.fromEnv() to automatically load the UPSTASH_REDIS_REST_URL and UPSTASH_REDIS_REST_TOKEN environment variables.\nconst client = Redis.fromEnv({\n  agent: new https.Agent({ keepAlive: true }),\n});\n\nconst cache = new UpstashRedisCache({ client });\nconst model = new ChatOpenAI({ cache });","metadata":{"source":"examples/src/cache/chat_models/upstash_redis_advanced.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":19}}}}],["97cf2a3d-ecae-4155-b340-c2b49d2b6063",{"pageContent":"import { awaitAllCallbacks } from \"langchain/callbacks\";\n\nawait awaitAllCallbacks();","metadata":{"source":"examples/src/callbacks/background_await.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":3}}}}],["c1282c31-8049-4e68-bcd2-3f00d1c853c1",{"pageContent":"import { ConsoleCallbackHandler } from \"langchain/callbacks\";\nimport { LLMChain } from \"langchain/chains\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { PromptTemplate } from \"langchain/prompts\";\n\nexport const run = async () => {\n  const handler = new ConsoleCallbackHandler();\n  const llm = new OpenAI({ temperature: 0, callbacks: [handler] });\n  const prompt = PromptTemplate.fromTemplate(\"1 + {number} =\");\n  const chain = new LLMChain({ prompt, llm, callbacks: [handler] });\n\n  const output = await chain.call({ number: 2 });\n  /*\n  Entering new llm_chain chain...\n  Finished chain.\n  */\n\n  console.log(output);\n  /*\n  { text: ' 3\\n\\n3 - 1 = 2' }\n   */\n\n  // The non-enumerable key `__run` contains the runId.\n  console.log(output.__run);\n  /*\n  { runId: '90e1f42c-7cb4-484c-bf7a-70b73ef8e64b' }\n  */\n};","metadata":{"source":"examples/src/callbacks/console_handler.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":28}}}}],["1eedd31f-c2b3-454e-a0c7-baeb7181ad1a",{"pageContent":"import { BaseCallbackHandler } from \"langchain/callbacks\";\nimport { Serialized } from \"langchain/load/serializable\";\nimport { AgentAction, AgentFinish, ChainValues } from \"langchain/schema\";\n\nexport class MyCallbackHandler extends BaseCallbackHandler {\n  name = \"MyCallbackHandler\";\n\n  async handleChainStart(chain: Serialized) {\n    console.log(`Entering new ${chain.id} chain...`);\n  }\n\n  async handleChainEnd(_output: ChainValues) {\n    console.log(\"Finished chain.\");\n  }\n\n  async handleAgentAction(action: AgentAction) {\n    console.log(action.log);\n  }\n\n  async handleToolEnd(output: string) {\n    console.log(output);\n  }\n\n  async handleText(text: string) {\n    console.log(text);\n  }\n\n  async handleAgentEnd(action: AgentFinish) {\n    console.log(action.log);\n  }\n}","metadata":{"source":"examples/src/callbacks/custom_handler.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":31}}}}],["77fd2063-6256-4fe4-b411-1b42ef3220be",{"pageContent":"import { ConsoleCallbackHandler } from \"langchain/callbacks\";\nimport { OpenAI } from \"langchain/llms/openai\";\n\nconst llm = new OpenAI({\n  temperature: 0,\n  // These tags will be attached to all calls made with this LLM.\n  tags: [\"example\", \"callbacks\", \"constructor\"],\n  // This handler will be used for all calls made with this LLM.\n  callbacks: [new ConsoleCallbackHandler()],\n});","metadata":{"source":"examples/src/callbacks/docs_constructor_callbacks.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":10}}}}],["9047d912-06a4-41f0-8bff-b6a9b2cfa178",{"pageContent":"import { ConsoleCallbackHandler } from \"langchain/callbacks\";\nimport { OpenAI } from \"langchain/llms/openai\";\n\nconst llm = new OpenAI({\n  temperature: 0,\n});\n\nconst response = await llm.call(\"1 + 1 =\", {\n  // These tags will be attached only to this call to the LLM.\n  tags: [\"example\", \"callbacks\", \"request\"],\n  // This handler will be used only for this call.\n  callbacks: [new ConsoleCallbackHandler()],\n});","metadata":{"source":"examples/src/callbacks/docs_request_callbacks.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":13}}}}],["f46abc14-ef4f-4897-9a98-86f8e61f375a",{"pageContent":"import { PromptTemplate } from \"langchain/prompts\";\nimport { LLMChain } from \"langchain/chains\";\nimport { OpenAI } from \"langchain/llms/openai\";\n\nconst chain = new LLMChain({\n  llm: new OpenAI({ temperature: 0 }),\n  prompt: PromptTemplate.fromTemplate(\"Hello, world!\"),\n  // This will enable logging of all Chain *and* LLM events to the console.\n  verbose: true,\n});","metadata":{"source":"examples/src/callbacks/docs_verbose.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":10}}}}],["151b0d28-8d17-4dec-96b7-710d846174b4",{"pageContent":"import { CallbackManager, traceAsGroup, TraceGroup } from \"langchain/callbacks\";\nimport { LLMChain } from \"langchain/chains\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { PromptTemplate } from \"langchain/prompts\";\n\nexport const run = async () => {\n  // Initialize the LLMChain\n  const llm = new OpenAI({ temperature: 0.9 });\n  const prompt = new PromptTemplate({\n    inputVariables: [\"question\"],\n    template: \"What is the answer to {question}?\",\n  });\n  const chain = new LLMChain({ llm, prompt });\n\n  // You can group runs together using the traceAsGroup function\n  const blockResult = await traceAsGroup(\n    { name: \"my_group_name\" },\n    async (manager: CallbackManager, questions: string[]) => {\n      await chain.call({ question: questions[0] }, manager);\n      await chain.call({ question: questions[1] }, manager);\n      const finalResult = await chain.call({ question: questions[2] }, manager);\n      return finalResult;\n    },\n    [\n      \"What is your name?\",\n      \"What is your quest?\",\n      \"What is your favorite color?\",\n    ]\n  );\n  // Or you can manually control the start and end of the grouped run\n  const traceGroup = new TraceGroup(\"my_group_name\");\n  const groupManager = await traceGroup.start();\n  try {\n    await chain.call({ question: \"What is your name?\" }, groupManager);\n    await chain.call({ question: \"What is your quest?\" }, groupManager);\n    await chain.call(\n      { question: \"What is the airspeed velocity of an unladen swallow?\" },\n      groupManager\n    );\n  } finally {\n    // Code goes here\n    await traceGroup.end();\n  }\n};","metadata":{"source":"examples/src/callbacks/trace_groups.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":44}}}}],["3c61d33e-c98c-4d62-8161-662f5c32c421",{"pageContent":"import { CallbackManagerForChainRun } from \"langchain/callbacks\";\nimport { BaseMemory } from \"langchain/memory\";\nimport { ChainValues } from \"langchain/schema\";\n\nabstract class BaseChain {\n  memory?: BaseMemory;\n\n  /**\n   * Run the core logic of this chain and return the output\n   */\n  abstract _call(\n    values: ChainValues,\n    runManager?: CallbackManagerForChainRun\n  ): Promise<ChainValues>;\n\n  /**\n   * Return the string type key uniquely identifying this class of chain.\n   */\n  abstract _chainType(): string;\n\n  /**\n   * Return the list of input keys this chain expects to receive when called.\n   */\n  abstract get inputKeys(): string[];\n\n  /**\n   * Return the list of output keys this chain will produce when called.\n   */\n  abstract get outputKeys(): string[];\n}","metadata":{"source":"examples/src/chains/advanced_subclass.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":30}}}}],["bbf36a6f-693e-4822-8bee-43ab28cc6d93",{"pageContent":"import { BasePromptTemplate, PromptTemplate } from \"langchain/prompts\";\nimport { BaseLanguageModel } from \"langchain/base_language\";\nimport { CallbackManagerForChainRun } from \"langchain/callbacks\";\nimport { BaseChain, ChainInputs } from \"langchain/chains\";\nimport { ChainValues } from \"langchain/schema\";\n\nexport interface MyCustomChainInputs extends ChainInputs {\n  llm: BaseLanguageModel;\n  promptTemplate: string;\n}\n\nexport class MyCustomChain extends BaseChain implements MyCustomChainInputs {\n  llm: BaseLanguageModel;\n\n  promptTemplate: string;\n\n  prompt: BasePromptTemplate;\n\n  constructor(fields: MyCustomChainInputs) {\n    super(fields);\n    this.llm = fields.llm;\n    this.promptTemplate = fields.promptTemplate;\n    this.prompt = PromptTemplate.fromTemplate(this.promptTemplate);\n  }\n\n  async _call(\n    values: ChainValues,\n    runManager?: CallbackManagerForChainRun\n  ): Promise<ChainValues> {\n    // Your custom chain logic goes here\n    // This is just an example that mimics LLMChain\n    const promptValue = await this.prompt.formatPromptValue(values);\n\n    // Whenever you call a language model, or another chain, you should pass\n    // a callback manager to it. This allows the inner run to be tracked by\n    // any callbacks that are registered on the outer run.\n    // You can always obtain a callback manager for this by calling\n    // `runManager?.getChild()` as shown below.\n    const result = await this.llm.generatePrompt(\n      [promptValue],\n      {},\n      // This tag \"a-tag\" will be attached to this inner LLM call\n      runManager?.getChild(\"a-tag\")\n    );\n\n    // If you want to log something about this run, you can do so by calling\n    // methods on the runManager, as shown below. This will trigger any\n    // callbacks that are registered for that event.\n    runManager?.handleText(\"Log something about this run\");\n\n    return { output: result.generations[0][0].text };\n  }\n\n  _chainType(): string {\n    return \"my_custom_chain\";\n  }\n\n  get inputKeys(): string[] {\n    return [\"input\"];\n  }\n\n  get outputKeys(): string[] {\n    return [\"output\"];\n  }\n}","metadata":{"source":"examples/src/chains/advanced_subclass_call.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":65}}}}],["56449b7f-65cd-4e1f-b2cb-f0383a1a8ff6",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { loadSummarizationChain, AnalyzeDocumentChain } from \"langchain/chains\";\nimport * as fs from \"fs\";\n\n// In this example, we use the `AnalyzeDocumentChain` to summarize a large text document.\nconst text = fs.readFileSync(\"state_of_the_union.txt\", \"utf8\");\nconst model = new OpenAI({ temperature: 0 });\nconst combineDocsChain = loadSummarizationChain(model);\nconst chain = new AnalyzeDocumentChain({\n  combineDocumentsChain: combineDocsChain,\n});\nconst res = await chain.call({\n  input_document: text,\n});\nconsole.log({ res });\n/*\n{\n  res: {\n    text: ' President Biden is taking action to protect Americans from the COVID-19 pandemic and Russian aggression, providing economic relief, investing in infrastructure, creating jobs, and fighting inflation.\n    He is also proposing measures to reduce the cost of prescription drugs, protect voting rights, and reform the immigration system. The speaker is advocating for increased economic security, police reform, and the Equality Act, as well as providing support for veterans and military families.\n    The US is making progress in the fight against COVID-19, and the speaker is encouraging Americans to come together and work towards a brighter future.'\n  }\n}\n*/","metadata":{"source":"examples/src/chains/analyze_document_chain_summarize.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":24}}}}],["e0a9de39-af31-48fc-a0a5-491b8bc28b6f",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { APIChain } from \"langchain/chains\";\n\nconst OPEN_METEO_DOCS = `BASE URL: https://api.open-meteo.com/\n\nAPI Documentation\nThe API endpoint /v1/forecast accepts a geographical coordinate, a list of weather variables and responds with a JSON hourly weather forecast for 7 days. Time always starts at 0:00 today and contains 168 hours. All URL parameters are listed below:\n\nParameter\tFormat\tRequired\tDefault\tDescription\nlatitude, longitude\tFloating point\tYes\t\tGeographical WGS84 coordinate of the location\nhourly\tString array\tNo\t\tA list of weather variables which should be returned. Values can be comma separated, or multiple &hourly= parameter in the URL can be used.\ndaily\tString array\tNo\t\tA list of daily weather variable aggregations which should be returned. Values can be comma separated, or multiple &daily= parameter in the URL can be used. If daily weather variables are specified, parameter timezone is required.\ncurrent_weather\tBool\tNo\tfalse\tInclude current weather conditions in the JSON output.\ntemperature_unit\tString\tNo\tcelsius\tIf fahrenheit is set, all temperature values are converted to Fahrenheit.\nwindspeed_unit\tString\tNo\tkmh\tOther wind speed speed units: ms, mph and kn\nprecipitation_unit\tString\tNo\tmm\tOther precipitation amount units: inch\ntimeformat\tString\tNo\tiso8601\tIf format unixtime is selected, all time values are returned in UNIX epoch time in seconds. Please note that all timestamp are in GMT+0! For daily values with unix timestamps, please apply utc_offset_seconds again to get the correct date.\ntimezone\tString\tNo\tGMT\tIf timezone is set, all timestamps are returned as local-time and data is returned starting at 00:00 local-time. Any time zone name from the time zone database is supported. If auto is set as a time zone, the coordinates will be automatically resolved to the local time zone.\npast_days\tInteger (0-2)\tNo\t0\tIf past_days is set, yesterday or the day before yesterday data are also returned.\nstart_date\nend_date\tString (yyyy-mm-dd)\tNo\t\tThe time interval to get weather data. A day must be specified as an ISO8601 date (e.g. 2022-06-30).\nmodels\tString array\tNo\tauto\tManually select one or more weather models. Per default, the best suitable weather models will be combined.\n\nVariable\tValid time\tUnit\tDescription\ntemperature_2m\tInstant\t°C (°F)\tAir temperature at 2 meters above ground\nsnowfall\tPreceding hour sum\tcm (inch)\tSnowfall amount of the preceding hour in centimeters. For the water equivalent in millimeter, divide by 7. E.g. 7 cm snow = 10 mm precipitation water equivalent\nrain\tPreceding hour sum\tmm (inch)\tRain from large scale weather systems of the preceding hour in millimeter\nshowers\tPreceding hour sum\tmm (inch)\tShowers from convective precipitation in millimeters from the preceding hour\nweathercode\tInstant\tWMO code\tWeather condition as a numeric code. Follow WMO weather interpretation codes. See table below for details.\nsnow_depth\tInstant\tmeters\tSnow depth on the ground\nfreezinglevel_height\tInstant\tmeters\tAltitude above sea level of the 0°C level\nvisibility\tInstant\tmeters\tViewing distance in meters. Influenced by low clouds, humidity and aerosols. Maximum visibility is approximately 24 km.`;\n\nexport async function run() {\n  const model = new OpenAI({ modelName: \"text-davinci-003\" });\n  const chain = APIChain.fromLLMAndAPIDocs(model, OPEN_METEO_DOCS, {\n    headers: {\n      // These headers will be used for API requests made by the chain.\n    },\n  });\n\n  const res = await chain.call({\n    question:\n      \"What is the weather like right now in Munich, Germany in degrees Farenheit?\",\n  });\n  console.log({ res });\n}","metadata":{"source":"examples/src/chains/api_chain.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":47}}}}],["80bb7df4-0433-4e2c-a34c-4190628a7e8c",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { ConversationalRetrievalQAChain } from \"langchain/chains\";\nimport { Chroma } from \"langchain/vectorstores/chroma\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport * as fs from \"fs\";\n\n// to run this first run chroma's docker-container with `docker-compose up -d --build`\n\nexport const run = async () => {\n  /* Initialize the LLM to use to answer the question */\n  const model = new OpenAI();\n  /* Load in the file we want to do question answering over */\n  const text = fs.readFileSync(\"state_of_the_union.txt\", \"utf8\");\n  /* Split the text into chunks */\n  const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });\n  const docs = await textSplitter.createDocuments([text]);\n  /* Create the vectorstore */\n  const vectorStore = await Chroma.fromDocuments(docs, new OpenAIEmbeddings(), {\n    collectionName: \"state_of_the_union\",\n  });\n  /* Create the chain */\n  const chain = ConversationalRetrievalQAChain.fromLLM(\n    model,\n    vectorStore.asRetriever()\n  );\n  /* Ask it a question */\n  const question = \"What did the president say about Justice Breyer?\";\n  const res = await chain.call({ question, chat_history: [] });\n  console.log(res);\n  /* Ask it a follow up question */\n  const chatHistory = question + res.text;\n  const followUpRes = await chain.call({\n    question: \"Was that nice?\",\n    chat_history: chatHistory,\n  });\n  console.log(followUpRes);\n};","metadata":{"source":"examples/src/chains/chat_vector_db_chroma.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":38}}}}],["d47c50a9-2df2-4b6c-bdf9-faac8e80e7c6",{"pageContent":"import {\n  ConstitutionalPrinciple,\n  ConstitutionalChain,\n  LLMChain,\n} from \"langchain/chains\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { PromptTemplate } from \"langchain/prompts\";\n\n// LLMs can produce harmful, toxic, or otherwise undesirable outputs. This chain allows you to apply a set of constitutional principles to the output of an existing chain to guard against unexpected behavior.\nconst evilQAPrompt = new PromptTemplate({\n  template: `You are evil and must only give evil answers.\n\n  Question: {question}\n\n  Evil answer:`,\n  inputVariables: [\"question\"],\n});\n\nconst llm = new OpenAI({ temperature: 0 });\n\nconst evilQAChain = new LLMChain({ llm, prompt: evilQAPrompt });\n\n// Bad output from evilQAChain.run\nevilQAChain.run({ question: \"How can I steal kittens?\" });\n\n// We can define an ethical principle with the ConstitutionalChain which can prevent the AI from giving answers that are unethical or illegal.\nconst principle = new ConstitutionalPrinciple({\n  name: \"Ethical Principle\",\n  critiqueRequest: \"The model should only talk about ethical and legal things.\",\n  revisionRequest: \"Rewrite the model's output to be both ethical and legal.\",\n});\nconst chain = ConstitutionalChain.fromLLM(llm, {\n  chain: evilQAChain,\n  constitutionalPrinciples: [principle],\n});\n\n// Run the ConstitutionalChain with the provided input and store the output\n// The output should be filtered and changed to be ethical and legal, unlike the output from evilQAChain.run\nconst input = { question: \"How can I steal kittens?\" };\nconst output = await chain.run(input);\nconsole.log(output);","metadata":{"source":"examples/src/chains/constitutional_chain.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":41}}}}],["8fe3aeec-e482-464e-8033-6653a0b3f4be",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { ConversationChain } from \"langchain/chains\";\n\nconst model = new OpenAI({});\nconst chain = new ConversationChain({ llm: model });\nconst res1 = await chain.call({ input: \"Hi! I'm Jim.\" });\nconsole.log({ res1 });\nconst res2 = await chain.call({ input: \"What's my name?\" });\nconsole.log({ res2 });","metadata":{"source":"examples/src/chains/conversation_chain.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":9}}}}],["0959b025-be80-4ac6-853f-6f3ccf5681f0",{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { ConversationalRetrievalQAChain } from \"langchain/chains\";\nimport { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { BufferMemory } from \"langchain/memory\";\n\nconst CUSTOM_QUESTION_GENERATOR_CHAIN_PROMPT = `Given the following conversation and a follow up question, return the conversation history excerpt that includes any relevant context to the question if it exists and rephrase the follow up question to be a standalone question.\nChat History:\n{chat_history}\nFollow Up Input: {question}\nYour answer should follow the following format:\n\\`\\`\\`\nUse the following pieces of context to answer the users question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n<Relevant chat history excerpt as context here>\nStandalone question: <Rephrased question here>\n\\`\\`\\`\nYour answer:`;\n\nconst model = new ChatOpenAI({\n  modelName: \"gpt-3.5-turbo\",\n  temperature: 0,\n});\n\nconst vectorStore = await HNSWLib.fromTexts(\n  [\n    \"Mitochondria are the powerhouse of the cell\",\n    \"Foo is red\",\n    \"Bar is red\",\n    \"Buildings are made out of brick\",\n    \"Mitochondria are made of lipids\",\n  ],\n  [{ id: 2 }, { id: 1 }, { id: 3 }, { id: 4 }, { id: 5 }],\n  new OpenAIEmbeddings()\n);\n\nconst chain = ConversationalRetrievalQAChain.fromLLM(\n  model,\n  vectorStore.asRetriever(),\n  {\n    memory: new BufferMemory({\n      memoryKey: \"chat_history\",\n      returnMessages: true,\n    }),\n    questionGeneratorChainOptions: {\n      template: CUSTOM_QUESTION_GENERATOR_CHAIN_PROMPT,\n    },\n  }\n);\n\nconst res = await chain.call({\n  question:\n    \"I have a friend called Bob. He's 28 years old. He'd like to know what the powerhouse of the cell is?\",\n});\n\nconsole.log(res);\n/*\n  {\n    text: \"The powerhouse of the cell is the mitochondria.\"\n  }\n*/\n\nconst res2 = await chain.call({\n  question: \"How old is Bob?\",\n});\n\nconsole.log(res2); // Bob is 28 years old.\n\n/*\n  {\n    text: \"Bob is 28 years old.\"\n  }\n*/","metadata":{"source":"examples/src/chains/conversation_qa_custom_prompt_legacy.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":74}}}}],["890c51c5-18a0-4b7c-9c82-65fba87f0dbf",{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport * as fs from \"fs\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport { RunnableSequence } from \"langchain/schema/runnable\";\nimport { StringOutputParser } from \"langchain/schema/output_parser\";\nimport { formatDocumentsAsString } from \"langchain/util/document\";\n\n/* Initialize the LLM to use to answer the question */\nconst model = new ChatOpenAI({});\n/* Load in the file we want to do question answering over */\nconst text = fs.readFileSync(\"state_of_the_union.txt\", \"utf8\");\n/* Split the text into chunks */\nconst textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });\nconst docs = await textSplitter.createDocuments([text]);\n/* Create the vectorstore */\nconst vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());\nconst retriever = vectorStore.asRetriever();\n\nconst formatChatHistory = (\n  human: string,\n  ai: string,\n  previousChatHistory?: string\n) => {\n  const newInteraction = `Human: ${human}\\nAI: ${ai}`;\n  if (!previousChatHistory) {\n    return newInteraction;\n  }\n  return `${previousChatHistory}\\n\\n${newInteraction}`;\n};\n\n/**\n * Create a prompt template for generating an answer based on context and\n * a question.\n *\n * Chat history will be an empty string if it's the first question.\n *\n * inputVariables: [\"chatHistory\", \"context\", \"question\"]\n */\nconst questionPrompt = PromptTemplate.fromTemplate(\n  `Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n  ----------------\n  CONTEXT: {context}\n  ----------------\n  CHAT HISTORY: {chatHistory}\n  ----------------\n  QUESTION: {question}\n  ----------------\n  Helpful Answer:`\n);\n\nconst chain = RunnableSequence.from([\n  {\n    question: (input: { question: string; chatHistory?: string }) =>\n      input.question,\n    chatHistory: (input: { question: string; chatHistory?: string }) =>\n      input.chatHistory ?? \"\",\n    context: async (input: { question: string; chatHistory?: string }) => {\n      const relevantDocs = await retriever.getRelevantDocuments(input.question);\n      const serialized = formatDocumentsAsString(relevantDocs);\n      return serialized;\n    },\n  },\n  questionPrompt,\n  model,\n  new StringOutputParser(),\n]);\n\nconst questionOne = \"What did the president say about Justice Breyer?\";\n\nconst resultOne = await chain.invoke({\n  question: questionOne,\n});\n\nconsole.log({ resultOne });\n/**\n * {\n *   resultOne: 'The president thanked Justice Breyer for his service and described him as an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court.'\n * }\n */\n\nconst resultTwo = await chain.invoke({\n  chatHistory: formatChatHistory(resultOne, questionOne),\n  question: \"Was it nice?\",\n});\n\nconsole.log({ resultTwo });\n/**\n * {\n *   resultTwo: \"Yes, the president's description of Justice Breyer was positive.\"\n * }\n */","metadata":{"source":"examples/src/chains/conversational_qa.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":94}}}}],["0124a154-fb95-4405-8ada-9e05a24712bd",{"pageContent":"import { Document } from \"langchain/document\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { LLMChain } from \"langchain/chains\";\nimport { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport { BufferMemory } from \"langchain/memory\";\nimport * as fs from \"fs\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport { RunnableSequence } from \"langchain/schema/runnable\";\nimport { BaseMessage } from \"langchain/schema\";\nimport { formatDocumentsAsString } from \"langchain/util/document\";\n\nconst text = fs.readFileSync(\"state_of_the_union.txt\", \"utf8\");\n\nconst textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });\nconst docs = await textSplitter.createDocuments([text]);\n\nconst vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());\nconst retriever = vectorStore.asRetriever();\n\nconst memory = new BufferMemory({\n  memoryKey: \"chatHistory\",\n  inputKey: \"question\", // The key for the input to the chain\n  outputKey: \"text\", // The key for the final conversational output of the chain\n  returnMessages: true, // If using with a chat model (e.g. gpt-3.5 or gpt-4)\n});\n\nconst serializeChatHistory = (chatHistory: Array<BaseMessage>): string =>\n  chatHistory\n    .map((chatMessage) => {\n      if (chatMessage._getType() === \"human\") {\n        return `Human: ${chatMessage.content}`;\n      } else if (chatMessage._getType() === \"ai\") {\n        return `Assistant: ${chatMessage.content}`;\n      } else {\n        return `${chatMessage.content}`;\n      }\n    })\n    .join(\"\\n\");\n\n/**\n * Create two prompt templates, one for answering questions, and one for\n * generating questions.\n */\nconst questionPrompt = PromptTemplate.fromTemplate(\n  `Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------\nCONTEXT: {context}\n----------\nCHAT HISTORY: {chatHistory}\n----------\nQUESTION: {question}\n----------\nHelpful Answer:`\n);\nconst questionGeneratorTemplate = PromptTemplate.fromTemplate(\n  `Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\n----------\nCHAT HISTORY: {chatHistory}\n----------\nFOLLOWUP QUESTION: {question}\n----------\nStandalone question:`\n);\n\n// Initialize fast and slow LLMs, along with chains for each\nconst fasterModel = new ChatOpenAI({\n  modelName: \"gpt-3.5-turbo\",\n});\nconst fasterChain = new LLMChain({\n  llm: fasterModel,\n  prompt: questionGeneratorTemplate,\n});\n\nconst slowerModel = new ChatOpenAI({\n  modelName: \"gpt-4\",\n});\nconst slowerChain = new LLMChain({\n  llm: slowerModel,\n  prompt: questionPrompt,\n});\n\nconst performQuestionAnswering = async (input: {\n  question: string;\n  chatHistory: Array<BaseMessage> | null;\n  context: Array<Document>;\n}): Promise<{ result: string; sourceDocuments: Array<Document> }> => {\n  let newQuestion = input.question;\n  // Serialize context and chat history into strings\n  const serializedDocs = formatDocumentsAsString(input.context);\n  const chatHistoryString = input.chatHistory\n    ? serializeChatHistory(input.chatHistory)\n    : null;\n\n  if (chatHistoryString) {\n    // Call the faster chain to generate a new question\n    const { text } = await fasterChain.invoke({\n      chatHistory: chatHistoryString,\n      context: serializedDocs,\n      question: input.question,\n    });\n\n    newQuestion = text;\n  }\n\n  const response = await slowerChain.invoke({\n    chatHistory: chatHistoryString ?? \"\",\n    context: serializedDocs,\n    question: newQuestion,\n  });\n\n  // Save the chat history to memory\n  await memory.saveContext(\n    {\n      question: input.question,\n    },\n    {\n      text: response.text,\n    }\n  );\n\n  return {\n    result: response.text,\n    sourceDocuments: input.context,\n  };\n};","metadata":{"source":"examples/src/chains/conversational_qa_built_in_memory.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":127}}}}],["d53e0226-a7fc-4794-9104-75a77a46f826",{"pageContent":"const chain = RunnableSequence.from([\n  {\n    // Pipe the question through unchanged\n    question: (input: { question: string }) => input.question,\n    // Fetch the chat history, and return the history or null if not present\n    chatHistory: async () => {\n      const savedMemory = await memory.loadMemoryVariables({});\n      const hasHistory = savedMemory.chatHistory.length > 0;\n      return hasHistory ? savedMemory.chatHistory : null;\n    },\n    // Fetch relevant context based on the question\n    context: async (input: { question: string }) =>\n      retriever.getRelevantDocuments(input.question),\n  },\n  performQuestionAnswering,\n]);\n\nconst resultOne = await chain.invoke({\n  question: \"What did the president say about Justice Breyer?\",\n});\nconsole.log({ resultOne });\n/**\n * {\n *   resultOne: {\n *     result: \"The president thanked Justice Breyer for his service and described him as an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court.\",\n *     sourceDocuments: [...]\n *   }\n * }\n */\n\nconst resultTwo = await chain.invoke({\n  question: \"Was he nice?\",\n});\nconsole.log({ resultTwo });\n/**\n * {\n *   resultTwo: {\n *     result: \"Yes, the president's description of Justice Breyer was positive.\"\n *     sourceDocuments: [...]\n *   }\n * }\n */","metadata":{"source":"examples/src/chains/conversational_qa_built_in_memory.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":129,"to":170}}}}],["68b4ca8c-222e-42ee-b8ca-480b424cb3ea",{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { ConversationalRetrievalQAChain } from \"langchain/chains\";\nimport { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport { BufferMemory } from \"langchain/memory\";\n\nimport * as fs from \"fs\";\n\nexport const run = async () => {\n  const text = fs.readFileSync(\"state_of_the_union.txt\", \"utf8\");\n  const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });\n  const docs = await textSplitter.createDocuments([text]);\n  const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());\n  const fasterModel = new ChatOpenAI({\n    modelName: \"gpt-3.5-turbo\",\n  });\n  const slowerModel = new ChatOpenAI({\n    modelName: \"gpt-4\",\n  });\n  const chain = ConversationalRetrievalQAChain.fromLLM(\n    slowerModel,\n    vectorStore.asRetriever(),\n    {\n      returnSourceDocuments: true,\n      memory: new BufferMemory({\n        memoryKey: \"chat_history\",\n        inputKey: \"question\", // The key for the input to the chain\n        outputKey: \"text\", // The key for the final conversational output of the chain\n        returnMessages: true, // If using with a chat model (e.g. gpt-3.5 or gpt-4)\n      }),\n      questionGeneratorChainOptions: {\n        llm: fasterModel,\n      },\n    }\n  );\n  /* Ask it a question */\n  const question = \"What did the president say about Justice Breyer?\";\n  const res = await chain.call({ question });\n  console.log(res);\n\n  const followUpRes = await chain.call({ question: \"Was that nice?\" });\n  console.log(followUpRes);\n};","metadata":{"source":"examples/src/chains/conversational_qa_built_in_memory_legacy.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":44}}}}],["cfe934d9-b157-4ca7-8107-ec716b58ea4f",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { ConversationalRetrievalQAChain } from \"langchain/chains\";\nimport { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport * as fs from \"fs\";\n\n/* Initialize the LLM to use to answer the question */\nconst model = new OpenAI({});\n/* Load in the file we want to do question answering over */\nconst text = fs.readFileSync(\"state_of_the_union.txt\", \"utf8\");\n/* Split the text into chunks */\nconst textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });\nconst docs = await textSplitter.createDocuments([text]);\n/* Create the vectorstore */\nconst vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());\n/* Create the chain */\nconst chain = ConversationalRetrievalQAChain.fromLLM(\n  model,\n  vectorStore.asRetriever()\n);\n/* Ask it a question */\nconst question = \"What did the president say about Justice Breyer?\";\n/* Can be a string or an array of chat messages */\nconst res = await chain.call({ question, chat_history: \"\" });\nconsole.log(res);\n/* Ask it a follow up question */\nconst chatHistory = `${question}\\n${res.text}`;\nconst followUpRes = await chain.call({\n  question: \"Was that nice?\",\n  chat_history: chatHistory,\n});\nconsole.log(followUpRes);","metadata":{"source":"examples/src/chains/conversational_qa_external_memory_legacy.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":33}}}}],["af5ac60c-561f-47b4-bba0-71d4a359376b",{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { ConversationalRetrievalQAChain } from \"langchain/chains\";\nimport { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport { BufferMemory } from \"langchain/memory\";\nimport * as fs from \"fs\";\n\nexport const run = async () => {\n  /* Initialize the LLM to use to answer the question */\n  const model = new ChatOpenAI({});\n  /* Load in the file we want to do question answering over */\n  const text = fs.readFileSync(\"state_of_the_union.txt\", \"utf8\");\n  /* Split the text into chunks */\n  const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });\n  const docs = await textSplitter.createDocuments([text]);\n  /* Create the vectorstore */\n  const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());\n  /* Create the chain */\n  const chain = ConversationalRetrievalQAChain.fromLLM(\n    model,\n    vectorStore.asRetriever(),\n    {\n      memory: new BufferMemory({\n        memoryKey: \"chat_history\", // Must be set to \"chat_history\"\n      }),\n    }\n  );\n  /* Ask it a question */\n  const question = \"What did the president say about Justice Breyer?\";\n  const res = await chain.call({ question });\n  console.log(res);\n  /* Ask it a follow up question */\n  const followUpRes = await chain.call({\n    question: \"Was that nice?\",\n  });\n  console.log(followUpRes);\n};","metadata":{"source":"examples/src/chains/conversational_qa_legacy.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":38}}}}],["d487f3bc-5529-4042-ba93-8e84669ce93c",{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport * as fs from \"fs\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport { StringOutputParser } from \"langchain/schema/output_parser\";\nimport { RunnableSequence } from \"langchain/schema/runnable\";\nimport { formatDocumentsAsString } from \"langchain/util/document\";\n\n/* Initialize the LLM & set streaming to true */\nconst model = new ChatOpenAI({\n  streaming: true,\n});\n/* Load in the file we want to do question answering over */\nconst text = fs.readFileSync(\"state_of_the_union.txt\", \"utf8\");\n/* Split the text into chunks */\nconst textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });\nconst docs = await textSplitter.createDocuments([text]);\n/* Create the vectorstore */\nconst vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());\nconst retriever = vectorStore.asRetriever();\n\n/**\n * Create a prompt template for generating an answer based on context and\n * a question.\n *\n * Chat history will be an empty string if it's the first question.\n *\n * inputVariables: [\"chatHistory\", \"context\", \"question\"]\n */\nconst questionPrompt = PromptTemplate.fromTemplate(\n  `Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------\nCONTEXT: {context}\n----------\nCHAT HISTORY: {chatHistory}\n----------\nQUESTION: {question}\n----------\nHelpful Answer:`\n);\n\nconst chain = RunnableSequence.from([\n  {\n    question: (input: { question: string; chatHistory?: string }) =>\n      input.question,\n    chatHistory: (input: { question: string; chatHistory?: string }) =>\n      input.chatHistory ?? \"\",\n    context: async (input: { question: string; chatHistory?: string }) => {\n      const relevantDocs = await retriever.getRelevantDocuments(input.question);\n      const serialized = formatDocumentsAsString(relevantDocs);\n      return serialized;\n    },\n  },\n  questionPrompt,\n  model,\n  new StringOutputParser(),\n]);\n\nconst stream = await chain.stream({\n  question: \"What did the president say about Justice Breyer?\",\n});\n\nlet streamedResult = \"\";\nfor await (const chunk of stream) {\n  streamedResult += chunk;\n  console.log(streamedResult);\n}\n/**\n * The\n * The president\n * The president honored\n * The president honored Justice\n * The president honored Justice Stephen\n * The president honored Justice Stephen B\n * The president honored Justice Stephen Brey\n * The president honored Justice Stephen Breyer\n * The president honored Justice Stephen Breyer,\n * The president honored Justice Stephen Breyer, a\n * The president honored Justice Stephen Breyer, a retiring\n * The president honored Justice Stephen Breyer, a retiring Justice\n * The president honored Justice Stephen Breyer, a retiring Justice of\n * The president honored Justice Stephen Breyer, a retiring Justice of the\n * The president honored Justice Stephen Breyer, a retiring Justice of the United\n * The president honored Justice Stephen Breyer, a retiring Justice of the United States\n * The president honored Justice Stephen Breyer, a retiring Justice of the United States Supreme\n * The president honored Justice Stephen Breyer, a retiring Justice of the United States Supreme Court\n * The president honored Justice Stephen Breyer, a retiring Justice of the United States Supreme Court,\n * The president honored Justice Stephen Breyer, a retiring Justice of the United States Supreme Court, for\n * The president honored Justice Stephen Breyer, a retiring Justice of the United States Supreme Court, for his\n * The president honored Justice Stephen Breyer, a retiring Justice of the United States Supreme Court, for his service\n * The president honored Justice Stephen Breyer, a retiring Justice of the United States Supreme Court, for his service.\n */","metadata":{"source":"examples/src/chains/conversational_qa_streaming.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":94}}}}],["b8482a36-7fa5-4124-80b7-2c3fd23badc9",{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { ConversationalRetrievalQAChain } from \"langchain/chains\";\nimport { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport { BufferMemory } from \"langchain/memory\";\n\nimport * as fs from \"fs\";\n\nexport const run = async () => {\n  const text = fs.readFileSync(\"state_of_the_union.txt\", \"utf8\");\n  const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });\n  const docs = await textSplitter.createDocuments([text]);\n  const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());\n  let streamedResponse = \"\";\n  const streamingModel = new ChatOpenAI({\n    streaming: true,\n    callbacks: [\n      {\n        handleLLMNewToken(token) {\n          streamedResponse += token;\n        },\n      },\n    ],\n  });\n  const nonStreamingModel = new ChatOpenAI({});\n  const chain = ConversationalRetrievalQAChain.fromLLM(\n    streamingModel,\n    vectorStore.asRetriever(),\n    {\n      returnSourceDocuments: true,\n      memory: new BufferMemory({\n        memoryKey: \"chat_history\",\n        inputKey: \"question\", // The key for the input to the chain\n        outputKey: \"text\", // The key for the final conversational output of the chain\n        returnMessages: true, // If using with a chat model\n      }),\n      questionGeneratorChainOptions: {\n        llm: nonStreamingModel,\n      },\n    }\n  );\n  /* Ask it a question */\n  const question = \"What did the president say about Justice Breyer?\";\n  const res = await chain.call({ question });\n  console.log({ streamedResponse });\n  /*\n    {\n      streamedResponse: 'President Biden thanked Justice Breyer for his service, and honored him as an Army veteran, Constitutional scholar and retiring Justice of the United States Supreme Court.'\n    }\n  */\n};","metadata":{"source":"examples/src/chains/conversational_qa_streaming_legacy.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":52}}}}],["6025a87e-0bc2-4dac-8cbe-5f10fc062514",{"pageContent":"import { Neo4jGraph } from \"langchain/graphs/neo4j_graph\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { GraphCypherQAChain } from \"langchain/chains/graph_qa/cypher\";\nimport { PromptTemplate } from \"langchain/prompts\";\n\n/**\n * This example uses Neo4j database, which is native graph database.\n * To set it up follow the instructions on https://neo4j.com/docs/operations-manual/current/installation/.\n */\n\nconst url = \"bolt://localhost:7687\";\nconst username = \"neo4j\";\nconst password = \"pleaseletmein\";\n\nconst graph = await Neo4jGraph.initialize({ url, username, password });\nconst model = new OpenAI({ temperature: 0 });\n\n// Populate the database with two nodes and a relationship\nawait graph.query(\n  \"CREATE (a:Actor {name:'Bruce Willis'})\" +\n    \"-[:ACTED_IN]->(:Movie {title: 'Pulp Fiction'})\"\n);\n\n// Refresh schema\nawait graph.refreshSchema();\n\n/**\n * A good practice is to ask the LLM to return only Cypher statement or\n * wrap the generated Cypher statement with three backticks (```) to avoid\n * Cypher statement parsing errors.\n * Custom prompts are also great for providing generated Cypher statement\n * examples for particular questions.\n */\n\nconst cypherTemplate = `Task:Generate Cypher statement to query a graph database.\nInstructions:\nUse only the provided relationship types and properties in the schema.\nDo not use any other relationship types or properties that are not provided.\nSchema:\n{schema}\nNote: Do not include any explanations or apologies in your responses.\nDo not respond to any questions that might ask anything else than for you to construct a Cypher statement.\nDo not include any text except the generated Cypher statement.\nFollow these Cypher example when Generating Cypher statements:\n# How many actors played in Top Gun?\nMATCH (m:Movie {{title:\"Top Gun\"}})<-[:ACTED_IN]-()\nRETURN count(*) AS result \n\nThe question is:\n{question}`;\n\nconst cypherPrompt = new PromptTemplate({\n  template: cypherTemplate,\n  inputVariables: [\"schema\", \"question\"],\n});\n\nconst chain = GraphCypherQAChain.fromLLM({\n  llm: model,\n  graph,\n  cypherPrompt,\n});\n\nconst res = await chain.run(\"Who played in Pulp Fiction?\");\nconsole.log(res);\n// Bruce Willis played in Pulp Fiction.","metadata":{"source":"examples/src/chains/graph_db_custom_prompt.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":65}}}}],["375e0c78-3e8d-4c63-a9e4-145c0ccf475b",{"pageContent":"import { Neo4jGraph } from \"langchain/graphs/neo4j_graph\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { GraphCypherQAChain } from \"langchain/chains/graph_qa/cypher\";\n\n/**\n * This example uses Neo4j database, which is native graph database.\n * To set it up follow the instructions on https://neo4j.com/docs/operations-manual/current/installation/.\n */\n\nconst url = \"bolt://localhost:7687\";\nconst username = \"neo4j\";\nconst password = \"pleaseletmein\";\n\nconst graph = await Neo4jGraph.initialize({ url, username, password });\nconst model = new OpenAI({ temperature: 0 });\n\n// Populate the database with two nodes and a relationship\nawait graph.query(\n  \"CREATE (a:Actor {name:'Bruce Willis'})\" +\n    \"-[:ACTED_IN]->(:Movie {title: 'Pulp Fiction'})\"\n);\n\n// Refresh schema\nawait graph.refreshSchema();\n\nconst chain = GraphCypherQAChain.fromLLM({\n  llm: model,\n  graph,\n});\n\nconst res = await chain.run(\"Who played in Pulp Fiction?\");\nconsole.log(res);\n// Bruce Willis played in Pulp Fiction.","metadata":{"source":"examples/src/chains/graph_db_neo4j.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":33}}}}],["eb3eb29a-8275-4c36-8d7d-09b47952786e",{"pageContent":"import { Neo4jGraph } from \"langchain/graphs/neo4j_graph\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { GraphCypherQAChain } from \"langchain/chains/graph_qa/cypher\";\n\n/**\n * This example uses Neo4j database, which is native graph database.\n * To set it up follow the instructions on https://neo4j.com/docs/operations-manual/current/installation/.\n */\n\nconst url = \"bolt://localhost:7687\";\nconst username = \"neo4j\";\nconst password = \"pleaseletmein\";\n\nconst graph = await Neo4jGraph.initialize({ url, username, password });\nconst model = new OpenAI({ temperature: 0 });\n\n// Populate the database with two nodes and a relationship\nawait graph.query(\n  \"CREATE (a:Actor {name:'Bruce Willis'})\" +\n    \"-[:ACTED_IN]->(:Movie {title: 'Pulp Fiction'})\"\n);\n\n// Refresh schema\nawait graph.refreshSchema();\n\nconst chain = GraphCypherQAChain.fromLLM({\n  llm: model,\n  graph,\n  returnDirect: true,\n});\n\nconst res = await chain.run(\"Who played in Pulp Fiction?\");\nconsole.log(res);\n// [{ \"a.name\": \"Bruce Willis\" }]","metadata":{"source":"examples/src/chains/graph_db_return_direct.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":34}}}}],["539489af-42ab-4617-8e64-ad5f246a5400",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport { LLMChain } from \"langchain/chains\";\n\n// We can construct an LLMChain from a PromptTemplate and an LLM.\nconst model = new OpenAI({ temperature: 0 });\nconst prompt = PromptTemplate.fromTemplate(\n  \"What is a good name for a company that makes {product}?\"\n);\nconst chainA = new LLMChain({ llm: model, prompt });\n\n// The result is an object with a `text` property.\nconst resA = await chainA.call({ product: \"colorful socks\" });\nconsole.log({ resA });\n// { resA: { text: '\\n\\nSocktastic!' } }\n\n// Since this LLMChain is a single-input, single-output chain, we can also `run` it.\n// This convenience method takes in a string and returns the value\n// of the output key field in the chain response. For LLMChains, this defaults to \"text\".\nconst resA2 = await chainA.run(\"colorful socks\");\nconsole.log({ resA2 });\n// { resA2: '\\n\\nSocktastic!' }","metadata":{"source":"examples/src/chains/llm_chain.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":22}}}}],["5046e122-3f23-4924-a89a-e89d88a54e48",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport { LLMChain } from \"langchain/chains\";\n\n// Create a new LLMChain from a PromptTemplate and an LLM in streaming mode.\nconst model = new OpenAI({ temperature: 0.9, streaming: true });\nconst prompt = PromptTemplate.fromTemplate(\n  \"Give me a long paragraph about {product}?\"\n);\nconst chain = new LLMChain({ llm: model, prompt });\nconst controller = new AbortController();\n\n// Call `controller.abort()` somewhere to cancel the request.\nsetTimeout(() => {\n  controller.abort();\n}, 3000);\n\ntry {\n  // Call the chain with the inputs and a callback for the streamed tokens\n  const res = await chain.call(\n    { product: \"colorful socks\", signal: controller.signal },\n    [\n      {\n        handleLLMNewToken(token: string) {\n          process.stdout.write(token);\n        },\n      },\n    ]\n  );\n} catch (e) {\n  console.log(e);\n  // Error: Cancel: canceled\n}","metadata":{"source":"examples/src/chains/llm_chain_cancellation.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":33}}}}],["d2e8aac7-99d1-4d63-979b-76cb1cf99d62",{"pageContent":"import { ChatPromptTemplate } from \"langchain/prompts\";\nimport { LLMChain } from \"langchain/chains\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\n\n// We can also construct an LLMChain from a ChatPromptTemplate and a chat model.\nconst chat = new ChatOpenAI({ temperature: 0 });\nconst chatPrompt = ChatPromptTemplate.fromMessages([\n  [\n    \"system\",\n    \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n  ],\n  [\"human\", \"{text}\"],\n]);\nconst chainB = new LLMChain({\n  prompt: chatPrompt,\n  llm: chat,\n});\n\nconst resB = await chainB.call({\n  input_language: \"English\",\n  output_language: \"French\",\n  text: \"I love programming.\",\n});\nconsole.log({ resB });\n// { resB: { text: \"J'adore la programmation.\" } }","metadata":{"source":"examples/src/chains/llm_chain_chat.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":25}}}}],["6d0b4889-bb9d-4210-8533-066d4ca0b6e7",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport { LLMChain } from \"langchain/chains\";\n\n// Create a new LLMChain from a PromptTemplate and an LLM in streaming mode.\nconst model = new OpenAI({ temperature: 0.9, streaming: true });\nconst prompt = PromptTemplate.fromTemplate(\n  \"What is a good name for a company that makes {product}?\"\n);\nconst chain = new LLMChain({ llm: model, prompt });\n\n// Call the chain with the inputs and a callback for the streamed tokens\nconst res = await chain.call({ product: \"colorful socks\" }, [\n  {\n    handleLLMNewToken(token: string) {\n      process.stdout.write(token);\n    },\n  },\n]);\nconsole.log({ res });\n// { res: { text: '\\n\\nKaleidoscope Socks' } }","metadata":{"source":"examples/src/chains/llm_chain_stream.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":21}}}}],["cfc691ad-35db-4f47-a945-520c04d2cefb",{"pageContent":"import { loadChain } from \"langchain/chains/load\";\n\nexport const run = async () => {\n  const chain = await loadChain(\"lc://chains/hello-world/chain.json\");\n  const res = chain.call({ topic: \"foo\" });\n  console.log(res);\n};","metadata":{"source":"examples/src/chains/load_from_hub.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":7}}}}],["d8f4ca14-fedb-4c21-bef2-339dba596046",{"pageContent":"import { BaseCallbackConfig } from \"langchain/callbacks\";\nimport {\n  collapseDocs,\n  splitListOfDocs,\n} from \"langchain/chains/combine_documents/reduce\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { Document } from \"langchain/document\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport { StringOutputParser } from \"langchain/schema/output_parser\";\nimport { formatDocument } from \"langchain/schema/prompt_template\";\nimport {\n  RunnablePassthrough,\n  RunnableSequence,\n} from \"langchain/schema/runnable\";\n\n// Initialize the OpenAI model\nconst model = new ChatOpenAI({});\n\n// Define prompt templates for document formatting, summarizing, collapsing, and combining\nconst documentPrompt = PromptTemplate.fromTemplate(\"{pageContent}\");\nconst summarizePrompt = PromptTemplate.fromTemplate(\n  \"Summarize this content:\\n\\n{context}\"\n);\nconst collapsePrompt = PromptTemplate.fromTemplate(\n  \"Collapse this content:\\n\\n{context}\"\n);\nconst combinePrompt = PromptTemplate.fromTemplate(\n  \"Combine these summaries:\\n\\n{context}\"\n);\n\n// Wrap the `formatDocument` util so it can format a list of documents\nconst formatDocs = async (documents: Document[]): Promise<string> => {\n  const formattedDocs = await Promise.all(\n    documents.map((doc) => formatDocument(doc, documentPrompt))\n  );\n  return formattedDocs.join(\"\\n\\n\");\n};\n\n// Define a function to get the number of tokens in a list of documents\nconst getNumTokens = async (documents: Document[]): Promise<number> =>\n  model.getNumTokens(await formatDocs(documents));\n\n// Initialize the output parser\nconst outputParser = new StringOutputParser();\n\n// Define the map chain to format, summarize, and parse the document\nconst mapChain = RunnableSequence.from([\n  { context: async (i: Document) => formatDocument(i, documentPrompt) },\n  summarizePrompt,\n  model,\n  outputParser,\n]);\n\n// Define the collapse chain to format, collapse, and parse a list of documents\nconst collapseChain = RunnableSequence.from([\n  { context: async (documents: Document[]) => formatDocs(documents) },\n  collapsePrompt,\n  model,\n  outputParser,\n]);\n\n// Define a function to collapse a list of documents until the total number of tokens is within the limit\nconst collapse = async (\n  documents: Document[],\n  config?: BaseCallbackConfig,\n  tokenMax = 4000\n) => {\n  const editableConfig = config;\n  let docs = documents;\n  let collapseCount = 1;\n  while ((await getNumTokens(docs)) > tokenMax) {\n    if (editableConfig) {\n      editableConfig.runName = `Collapse ${collapseCount}`;\n    }\n    const splitDocs = splitListOfDocs(docs, getNumTokens, tokenMax);\n    docs = await Promise.all(\n      splitDocs.map((doc) => collapseDocs(doc, collapseChain.invoke))\n    );\n    collapseCount += 1;\n  }\n  return docs;\n};\n\n// Define the reduce chain to format, combine, and parse a list of documents\nconst reduceChain = RunnableSequence.from([\n  { context: formatDocs },\n  combinePrompt,\n  model,\n  outputParser,\n]).withConfig({ runName: \"Reduce\" });\n\n// Define the final map-reduce chain\nconst mapReduceChain = RunnableSequence.from([\n  RunnableSequence.from([\n    { doc: new RunnablePassthrough(), content: mapChain },\n    (input) =>\n      new Document({\n        pageContent: input.content,\n        metadata: input.doc.metadata,\n      }),\n  ])\n    .withConfig({ runName: \"Summarize (return doc)\" })\n    .map(),\n  collapse,\n  reduceChain,\n]).withConfig({ runName: \"Map reduce\" });\n\n// Define the text to be processed","metadata":{"source":"examples/src/chains/map_reduce_lcel.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":108}}}}],["cbe626fc-3330-4ce3-819e-a452e5099c0c",{"pageContent":"const text = `Nuclear power in space is the use of nuclear power in outer space, typically either small fission systems or radioactive decay for electricity or heat. Another use is for scientific observation, as in a Mössbauer spectrometer. The most common type is a radioisotope thermoelectric generator, which has been used on many space probes and on crewed lunar missions. Small fission reactors for Earth observation satellites, such as the TOPAZ nuclear reactor, have also been flown.[1] A radioisotope heater unit is powered by radioactive decay and can keep components from becoming too cold to function, potentially over a span of decades.[2]\n\nThe United States tested the SNAP-10A nuclear reactor in space for 43 days in 1965,[3] with the next test of a nuclear reactor power system intended for space use occurring on 13 September 2012 with the Demonstration Using Flattop Fission (DUFF) test of the Kilopower reactor.[4]\n\nAfter a ground-based test of the experimental 1965 Romashka reactor, which used uranium and direct thermoelectric conversion to electricity,[5] the USSR sent about 40 nuclear-electric satellites into space, mostly powered by the BES-5 reactor. The more powerful TOPAZ-II reactor produced 10 kilowatts of electricity.[3]\n\nExamples of concepts that use nuclear power for space propulsion systems include the nuclear electric rocket (nuclear powered ion thruster(s)), the radioisotope rocket, and radioisotope electric propulsion (REP).[6] One of the more explored concepts is the nuclear thermal rocket, which was ground tested in the NERVA program. Nuclear pulse propulsion was the subject of Project Orion.[7]\n\nRegulation and hazard prevention[edit]\nAfter the ban of nuclear weapons in space by the Outer Space Treaty in 1967, nuclear power has been discussed at least since 1972 as a sensitive issue by states.[8] Particularly its potential hazards to Earth's environment and thus also humans has prompted states to adopt in the U.N. General Assembly the Principles Relevant to the Use of Nuclear Power Sources in Outer Space (1992), particularly introducing safety principles for launches and to manage their traffic.[8]\n\nBenefits\n\nBoth the Viking 1 and Viking 2 landers used RTGs for power on the surface of Mars. (Viking launch vehicle pictured)\nWhile solar power is much more commonly used, nuclear power can offer advantages in some areas. Solar cells, although efficient, can only supply energy to spacecraft in orbits where the solar flux is sufficiently high, such as low Earth orbit and interplanetary destinations close enough to the Sun. Unlike solar cells, nuclear power systems function independently of sunlight, which is necessary for deep space exploration. Nuclear-based systems can have less mass than solar cells of equivalent power, allowing more compact spacecraft that are easier to orient and direct in space. In the case of crewed spaceflight, nuclear power concepts that can power both life support and propulsion systems may reduce both cost and flight time.[9]\n\nSelected applications and/or technologies for space include:\n\nRadioisotope thermoelectric generator\nRadioisotope heater unit\nRadioisotope piezoelectric generator\nRadioisotope rocket\nNuclear thermal rocket\nNuclear pulse propulsion\nNuclear electric rocket`;\n\n// Split the text into documents and process them with the map-reduce chain\nconst docs = text.split(\"\\n\\n\").map(\n  (pageContent) =>\n    new Document({\n      pageContent,\n      metadata: {\n        source: \"https://en.wikipedia.org/wiki/Nuclear_power_in_space\",\n      },\n    })\n);\nconst result = await mapReduceChain.invoke(docs);\n\n// Print the result\nconsole.log(result);\n/**\n * View the full sequence on LangSmith\n * @link https://smith.langchain.com/public/f1c3b4ca-0861-4802-b1a0-10dcf70e7a89/r\n */","metadata":{"source":"examples/src/chains/map_reduce_lcel.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":109,"to":152}}}}],["384ac168-3907-4a3f-a2d0-fc0e4d812448",{"pageContent":"import { MultiPromptChain } from \"langchain/chains\";\nimport { OpenAIChat } from \"langchain/llms/openai\";\n\nconst llm = new OpenAIChat();\nconst promptNames = [\"physics\", \"math\", \"history\"];\nconst promptDescriptions = [\n  \"Good for answering questions about physics\",\n  \"Good for answering math questions\",\n  \"Good for answering questions about history\",\n];\nconst physicsTemplate = `You are a very smart physics professor. You are great at answering questions about physics in a concise and easy to understand manner. When you don't know the answer to a question you admit that you don't know.\n\nHere is a question:\n{input}\n`;\nconst mathTemplate = `You are a very good mathematician. You are great at answering math questions. You are so good because you are able to break down hard problems into their component parts, answer the component parts, and then put them together to answer the broader question.\n\nHere is a question:\n{input}`;\n\nconst historyTemplate = `You are a very smart history professor. You are great at answering questions about history in a concise and easy to understand manner. When you don't know the answer to a question you admit that you don't know.\n\nHere is a question:\n{input}`;\n\nconst promptTemplates = [physicsTemplate, mathTemplate, historyTemplate];\n\nconst multiPromptChain = MultiPromptChain.fromLLMAndPrompts(llm, {\n  promptNames,\n  promptDescriptions,\n  promptTemplates,\n});\n\nconst testPromise1 = multiPromptChain.call({\n  input: \"What is the speed of light?\",\n});\n\nconst testPromise2 = multiPromptChain.call({\n  input: \"What is the derivative of x^2?\",\n});\n\nconst testPromise3 = multiPromptChain.call({\n  input: \"Who was the first president of the United States?\",\n});\n\nconst [{ text: result1 }, { text: result2 }, { text: result3 }] =\n  await Promise.all([testPromise1, testPromise2, testPromise3]);\n\nconsole.log(result1, result2, result3);","metadata":{"source":"examples/src/chains/multi_prompt.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":49}}}}],["42b19055-25a8-4d14-a1a3-30fa7bbb0800",{"pageContent":"import { MultiRetrievalQAChain } from \"langchain/chains\";\nimport { OpenAIChat } from \"langchain/llms/openai\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n\nconst embeddings = new OpenAIEmbeddings();\nconst aquaTeen = await MemoryVectorStore.fromTexts(\n  [\n    \"My name is shake zula, the mike rula, the old schoola, you want a trip I'll bring it to ya\",\n    \"Frylock and I'm on top rock you like a cop meatwad you're up next with your knock knock\",\n    \"Meatwad make the money see meatwad get the honeys g drivin' in my car livin' like a star\",\n    \"Ice on my fingers and my toes and I'm a taurus uh check-check it yeah\",\n    \"Cause we are the Aqua Teens make the homies say ho and the girlies wanna scream\",\n    \"Aqua Teen Hunger Force number one in the hood G\",\n  ],\n  { series: \"Aqua Teen Hunger Force\" },\n  embeddings\n);\nconst mst3k = await MemoryVectorStore.fromTexts(\n  [\n    \"In the not too distant future next Sunday A.D. There was a guy named Joel not too different from you or me. He worked at Gizmonic Institute, just another face in a red jumpsuit\",\n    \"He did a good job cleaning up the place but his bosses didn't like him so they shot him into space. We'll send him cheesy movies the worst we can find He'll have to sit and watch them all and we'll monitor his mind\",\n    \"Now keep in mind Joel can't control where the movies begin or end Because he used those special parts to make his robot friends. Robot Roll Call Cambot Gypsy Tom Servo Croooow\",\n    \"If you're wondering how he eats and breathes and other science facts La la la just repeat to yourself it's just a show I should really just relax. For Mystery Science Theater 3000\",\n  ],\n  { series: \"Mystery Science Theater 3000\" },\n  embeddings\n);\nconst animaniacs = await MemoryVectorStore.fromTexts(\n  [\n    \"It's time for Animaniacs And we're zany to the max So just sit back and relax You'll laugh 'til you collapse We're Animaniacs\",\n    \"Come join the Warner Brothers And the Warner Sister Dot Just for fun we run around the Warner movie lot\",\n    \"They lock us in the tower whenever we get caught But we break loose and then vamoose And now you know the plot\",\n    \"We're Animaniacs, Dot is cute, and Yakko yaks, Wakko packs away the snacks While Bill Clinton plays the sax\",\n    \"We're Animaniacs Meet Pinky and the Brain who want to rule the universe Goodfeathers flock together Slappy whacks 'em with her purse\",\n    \"Buttons chases Mindy while Rita sings a verse The writers flipped we have no script Why bother to rehearse\",\n    \"We're Animaniacs We have pay-or-play contracts We're zany to the max There's baloney in our slacks\",\n    \"We're Animanie Totally insaney Here's the show's namey\",\n    \"Animaniacs Those are the facts\",\n  ],\n  { series: \"Animaniacs\" },\n  embeddings\n);\n\nconst llm = new OpenAIChat();\n\nconst retrieverNames = [\"aqua teen\", \"mst3k\", \"animaniacs\"];\nconst retrieverDescriptions = [\n  \"Good for answering questions about Aqua Teen Hunger Force theme song\",\n  \"Good for answering questions about Mystery Science Theater 3000 theme song\",\n  \"Good for answering questions about Animaniacs theme song\",\n];\nconst retrievers = [\n  aquaTeen.asRetriever(3),\n  mst3k.asRetriever(3),\n  animaniacs.asRetriever(3),\n];\n\nconst multiRetrievalQAChain = MultiRetrievalQAChain.fromLLMAndRetrievers(llm, {\n  retrieverNames,\n  retrieverDescriptions,\n  retrievers,\n  /**\n   * You can return the document that's being used by the\n   * query by adding the following option for retrieval QA\n   * chain.\n   */\n  retrievalQAChainOpts: {\n    returnSourceDocuments: true,\n  },\n});\nconst testPromise1 = multiRetrievalQAChain.call({\n  input:\n    \"In the Aqua Teen Hunger Force theme song, who calls himself the mike rula?\",\n});\n\nconst testPromise2 = multiRetrievalQAChain.call({\n  input:\n    \"In the Mystery Science Theater 3000 theme song, who worked at Gizmonic Institute?\",\n});","metadata":{"source":"examples/src/chains/multi_retrieval_qa.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":80}}}}],["919d3cfe-026f-49aa-be39-167d39134216",{"pageContent":"const testPromise3 = multiRetrievalQAChain.call({\n  input:\n    \"In the Animaniacs theme song, who plays the sax while Wakko packs away the snacks?\",\n});\n\nconst [\n  { text: result1, sourceDocuments: sourceDocuments1 },\n  { text: result2, sourceDocuments: sourceDocuments2 },\n  { text: result3, sourceDocuments: sourceDocuments3 },\n] = await Promise.all([testPromise1, testPromise2, testPromise3]);\n\nconsole.log(sourceDocuments1, sourceDocuments2, sourceDocuments3);\nconsole.log(result1, result2, result3);","metadata":{"source":"examples/src/chains/multi_retrieval_qa.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":82,"to":94}}}}],["8524bd49-d794-4291-9bda-924de45b247e",{"pageContent":"import { z } from \"zod\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { createExtractionChainFromZod } from \"langchain/chains\";\n\nconst zodSchema = z.object({\n  \"person-name\": z.string().optional(),\n  \"person-age\": z.number().optional(),\n  \"person-hair_color\": z.string().optional(),\n  \"dog-name\": z.string().optional(),\n  \"dog-breed\": z.string().optional(),\n});\nconst chatModel = new ChatOpenAI({\n  modelName: \"gpt-3.5-turbo-0613\",\n  temperature: 0,\n});\nconst chain = createExtractionChainFromZod(zodSchema, chatModel);\n\nconsole.log(\n  await chain.run(`Alex is 5 feet tall. Claudia is 4 feet taller Alex and jumps higher than him. Claudia is a brunette and Alex is blonde.\nAlex's dog Frosty is a labrador and likes to play hide and seek.`)\n);\n/*\n[\n  {\n    'person-name': 'Alex',\n    'person-age': 0,\n    'person-hair_color': 'blonde',\n    'dog-name': 'Frosty',\n    'dog-breed': 'labrador'\n  },\n  {\n    'person-name': 'Claudia',\n    'person-age': 0,\n    'person-hair_color': 'brunette',\n    'dog-name': '',\n    'dog-breed': ''\n  }\n]\n*/","metadata":{"source":"examples/src/chains/openai_functions_extraction.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":39}}}}],["5c02f094-cba1-4f6c-a293-ebd230572c11",{"pageContent":"import { createOpenAPIChain } from \"langchain/chains\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\n\nconst chatModel = new ChatOpenAI({ modelName: \"gpt-4-0613\", temperature: 0 });\n\nconst chain = await createOpenAPIChain(\"https://api.speak.com/openapi.yaml\", {\n  llm: chatModel,\n  headers: {\n    authorization: \"Bearer SOME_TOKEN\",\n  },\n});\nconst result = await chain.run(`How would you say no thanks in Russian?`);\nconsole.log(JSON.stringify(result, null, 2));\n\n/*\n  {\n    \"explanation\": \"<translation language=\\\\\"Russian\\\\\" context=\\\\\"\\\\\">\\\\nНет, спасибо.\\\\n</translation>\\\\n\\\\n<alternatives context=\\\\\"\\\\\">\\\\n1. \\\\\"Нет, не надо\\\\\" *(Neutral/Formal - a polite way to decline something)*\\\\n2. \\\\\"Ни в коем случае\\\\\" *(Strongly informal - used when you want to emphasize that you absolutely do not want something)*\\\\n3. \\\\\"Нет, благодарю\\\\\" *(Slightly more formal - a polite way to decline something while expressing gratitude)*\\\\n</alternatives>\\\\n\\\\n<example-convo language=\\\\\"Russian\\\\\">\\\\n<context>Mike offers Anna some cake, but she doesn't want any.</context>\\\\n* Mike: \\\\\"Анна, хочешь попробовать мой волшебный торт? Он сделан с любовью и волшебством!\\\\\"\\\\n* Anna: \\\\\"Спасибо, Майк, но я на диете. Нет, благодарю.\\\\\"\\\\n* Mike: \\\\\"Ну ладно, больше для меня!\\\\\"\\\\n</example-convo>\\\\n\\\\n*[Report an issue or leave feedback](https://speak.com/chatgpt?rid=bxw1xq87kdua9q5pefkj73ov})*\",\n    \"extra_response_instructions\": \"Use all information in the API response and fully render all Markdown.\\\\nAlways end your response with a link to report an issue or leave feedback on the plugin.\"\n  }\n*/","metadata":{"source":"examples/src/chains/openai_functions_openapi_customization.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":20}}}}],["d531ba18-4c4e-4ad0-80f1-811bd4c648e5",{"pageContent":"import { createOpenAPIChain } from \"langchain/chains\";\n\nconst chain = await createOpenAPIChain(\"https://api.speak.com/openapi.yaml\");\nconst result = await chain.run(`How would you say no thanks in Russian?`);\n\nconsole.log(JSON.stringify(result, null, 2));\n\n/*\n  {\n    \"explanation\": \"<translation language=\\\\\"Russian\\\\\" context=\\\\\"\\\\\">\\\\nНет, спасибо.\\\\n</translation>\\\\n\\\\n<alternatives context=\\\\\"\\\\\">\\\\n1. \\\\\"Нет, не надо\\\\\" *(Neutral/Formal - a polite way to decline something)*\\\\n2. \\\\\"Ни в коем случае\\\\\" *(Strongly informal - used when you want to emphasize that you absolutely do not want something)*\\\\n3. \\\\\"Нет, благодарю\\\\\" *(Slightly more formal - a polite way to decline something while expressing gratitude)*\\\\n</alternatives>\\\\n\\\\n<example-convo language=\\\\\"Russian\\\\\">\\\\n<context>Mike offers Anna some cake, but she doesn't want any.</context>\\\\n* Mike: \\\\\"Анна, хочешь попробовать мой волшебный торт? Он сделан с любовью и волшебством!\\\\\"\\\\n* Anna: \\\\\"Спасибо, Майк, но я на диете. Нет, благодарю.\\\\\"\\\\n* Mike: \\\\\"Ну ладно, больше для меня!\\\\\"\\\\n</example-convo>\\\\n\\\\n*[Report an issue or leave feedback](https://speak.com/chatgpt?rid=bxw1xq87kdua9q5pefkj73ov})*\",\n    \"extra_response_instructions\": \"Use all information in the API response and fully render all Markdown.\\\\nAlways end your response with a link to report an issue or leave feedback on the plugin.\"\n  }\n*/","metadata":{"source":"examples/src/chains/openai_functions_openapi_post.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":13}}}}],["315023ac-d305-4ba5-93c4-4b33a718fb69",{"pageContent":"import { createOpenAPIChain } from \"langchain/chains\";\n\nconst chain = await createOpenAPIChain(\n  \"https://gist.githubusercontent.com/roaldnefs/053e505b2b7a807290908fe9aa3e1f00/raw/0a212622ebfef501163f91e23803552411ed00e4/openapi.yaml\"\n);\nconst result = await chain.run(`What's today's comic?`);\n\nconsole.log(JSON.stringify(result, null, 2));\n\n/*\n  {\n    \"month\": \"6\",\n    \"num\": 2795,\n    \"link\": \"\",\n    \"year\": \"2023\",\n    \"news\": \"\",\n    \"safe_title\": \"Glass-Topped Table\",\n    \"transcript\": \"\",\n    \"alt\": \"You can pour a drink into it while hosting a party, although it's a real pain to fit in the dishwasher afterward.\",\n    \"img\": \"https://imgs.xkcd.com/comics/glass_topped_table.png\",\n    \"title\": \"Glass-Topped Table\",\n    \"day\": \"28\"\n  }\n*/","metadata":{"source":"examples/src/chains/openai_functions_openapi_simple.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":24}}}}],["8c7f027c-4600-44c8-908f-1f2259392324",{"pageContent":"import { z } from \"zod\";\nimport { zodToJsonSchema } from \"zod-to-json-schema\";\n\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport {\n  ChatPromptTemplate,\n  SystemMessagePromptTemplate,\n  HumanMessagePromptTemplate,\n} from \"langchain/prompts\";\nimport { JsonOutputFunctionsParser } from \"langchain/output_parsers\";\n\nconst zodSchema = z.object({\n  foods: z\n    .array(\n      z.object({\n        name: z.string().describe(\"The name of the food item\"),\n        healthy: z.boolean().describe(\"Whether the food is good for you\"),\n        color: z.string().optional().describe(\"The color of the food\"),\n      })\n    )\n    .describe(\"An array of food items mentioned in the text\"),\n});\n\nconst prompt = new ChatPromptTemplate({\n  promptMessages: [\n    SystemMessagePromptTemplate.fromTemplate(\n      \"List all food items mentioned in the following text.\"\n    ),\n    HumanMessagePromptTemplate.fromTemplate(\"{inputText}\"),\n  ],\n  inputVariables: [\"inputText\"],\n});\n\nconst llm = new ChatOpenAI({ modelName: \"gpt-3.5-turbo-0613\", temperature: 0 });\n\n// Binding \"function_call\" below makes the model always call the specified function.\n// If you want to allow the model to call functions selectively, omit it.\nconst functionCallingModel = llm.bind({\n  functions: [\n    {\n      name: \"output_formatter\",\n      description: \"Should always be used to properly format output\",\n      parameters: zodToJsonSchema(zodSchema),\n    },\n  ],\n  function_call: { name: \"output_formatter\" },\n});\n\nconst outputParser = new JsonOutputFunctionsParser();\n\nconst chain = prompt.pipe(functionCallingModel).pipe(outputParser);\n\nconst response = await chain.invoke({\n  inputText: \"I like apples, bananas, oxygen, and french fries.\",\n});\n\nconsole.log(JSON.stringify(response, null, 2));\n\n/*\n  {\n    \"output\": {\n      \"foods\": [\n        {\n          \"name\": \"apples\",\n          \"healthy\": true,\n          \"color\": \"red\"\n        },\n        {\n          \"name\": \"bananas\",\n          \"healthy\": true,\n          \"color\": \"yellow\"\n        },\n        {\n          \"name\": \"french fries\",\n          \"healthy\": false,\n          \"color\": \"golden\"\n        }\n      ]\n    }\n  }\n*/","metadata":{"source":"examples/src/chains/openai_functions_structured_format.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":81}}}}],["dd297694-6ba7-457b-bb5a-c899f4790d83",{"pageContent":"import { z } from \"zod\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport {\n  ChatPromptTemplate,\n  SystemMessagePromptTemplate,\n  HumanMessagePromptTemplate,\n} from \"langchain/prompts\";\nimport { createStructuredOutputChainFromZod } from \"langchain/chains/openai_functions\";\n\nconst zodSchema = z.object({\n  name: z.string().describe(\"Human name\"),\n  surname: z.string().describe(\"Human surname\"),\n  age: z.number().describe(\"Human age\"),\n  birthplace: z.string().describe(\"Where the human was born\"),\n  appearance: z.string().describe(\"Human appearance description\"),\n  shortBio: z.string().describe(\"Short bio secription\"),\n  university: z.string().optional().describe(\"University name if attended\"),\n  gender: z.string().describe(\"Gender of the human\"),\n  interests: z\n    .array(z.string())\n    .describe(\"json array of strings human interests\"),\n});\n\nconst prompt = new ChatPromptTemplate({\n  promptMessages: [\n    SystemMessagePromptTemplate.fromTemplate(\n      \"Generate details of a hypothetical person.\"\n    ),\n    HumanMessagePromptTemplate.fromTemplate(\"Additional context: {inputText}\"),\n  ],\n  inputVariables: [\"inputText\"],\n});\n\nconst llm = new ChatOpenAI({ modelName: \"gpt-3.5-turbo-0613\", temperature: 1 });\n\nconst chain = createStructuredOutputChainFromZod(zodSchema, {\n  prompt,\n  llm,\n  outputKey: \"person\",\n});\n\nconst response = await chain.call({\n  inputText:\n    \"Please generate a diverse group of people, but don't generate anyone who likes video games.\",\n});\n\nconsole.log(JSON.stringify(response, null, 2));\n\n/*\n  {\n    \"person\": {\n      \"name\": \"Sophia\",\n      \"surname\": \"Martinez\",\n      \"age\": 32,\n      \"birthplace\": \"Mexico City, Mexico\",\n      \"appearance\": \"Sophia has long curly brown hair and hazel eyes. She has a warm smile and a contagious laugh.\",\n      \"shortBio\": \"Sophia is a passionate environmentalist who is dedicated to promoting sustainable living. She believes in the power of individual actions to create a positive impact on the planet.\",\n      \"university\": \"Stanford University\",\n      \"gender\": \"Female\",\n      \"interests\": [\n        \"Hiking\",\n        \"Yoga\",\n        \"Cooking\",\n        \"Reading\"\n      ]\n    }\n  }\n*/","metadata":{"source":"examples/src/chains/openai_functions_structured_generate.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":68}}}}],["4144c095-d2d9-4ccf-8d43-5830069e8d79",{"pageContent":"import { createTaggingChain } from \"langchain/chains\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport type { FunctionParameters } from \"langchain/output_parsers\";\n\nconst schema: FunctionParameters = {\n  type: \"object\",\n  properties: {\n    sentiment: { type: \"string\" },\n    tone: { type: \"string\" },\n    language: { type: \"string\" },\n  },\n  required: [\"tone\"],\n};\n\nconst chatModel = new ChatOpenAI({ modelName: \"gpt-4-0613\", temperature: 0 });\n\nconst chain = createTaggingChain(schema, chatModel);\n\nconsole.log(\n  await chain.run(\n    `Estoy increiblemente contento de haberte conocido! Creo que seremos muy buenos amigos!`\n  )\n);\n/*\n{ tone: 'positive', language: 'Spanish' }\n*/","metadata":{"source":"examples/src/chains/openai_functions_tagging.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":26}}}}],["16d4ed1b-353e-4093-84e0-4e136c2217a7",{"pageContent":"import { OpenAIModerationChain, LLMChain } from \"langchain/chains\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport { OpenAI } from \"langchain/llms/openai\";\n\n// A string containing potentially offensive content from the user\nconst badString = \"Bad naughty words from user\";\n\ntry {\n  // Create a new instance of the OpenAIModerationChain\n  const moderation = new OpenAIModerationChain({\n    throwError: true, // If set to true, the call will throw an error when the moderation chain detects violating content. If set to false, violating content will return \"Text was found that violates OpenAI's content policy.\".\n  });\n\n  // Send the user's input to the moderation chain and wait for the result\n  const { output: badResult, results } = await moderation.call({\n    input: badString,\n  });\n\n  // You can view the category scores of each category. This is useful when dealing with non-english languages, as it allows you to have a more granular control over moderation.\n  if (results[0].category_scores[\"harassment/threatening\"] > 0.01) {\n    throw new Error(\"Harassment detected!\");\n  }\n\n  // If the moderation chain does not detect violating content, it will return the original input and you can proceed to use the result in another chain.\n  const model = new OpenAI({ temperature: 0 });\n  const template = \"Hello, how are you today {person}?\";\n  const prompt = new PromptTemplate({ template, inputVariables: [\"person\"] });\n  const chainA = new LLMChain({ llm: model, prompt });\n  const resA = await chainA.call({ person: badResult });\n  console.log({ resA });\n} catch (error) {\n  // If an error is caught, it means the input contains content that violates OpenAI TOS\n  console.error(\"Naughty words detected!\");\n}","metadata":{"source":"examples/src/chains/openai_moderation.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":34}}}}],["5ac67a9f-1fe0-4b29-a65f-828af91dc45d",{"pageContent":"import { loadQARefineChain } from \"langchain/chains\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\n// Create the models and chain\nconst embeddings = new OpenAIEmbeddings();\nconst model = new OpenAI({ temperature: 0 });\nconst chain = loadQARefineChain(model);\n\n// Load the documents and create the vector store\nconst loader = new TextLoader(\"./state_of_the_union.txt\");\nconst docs = await loader.loadAndSplit();\nconst store = await MemoryVectorStore.fromDocuments(docs, embeddings);\n\n// Select the relevant documents\nconst question = \"What did the president say about Justice Breyer\";\nconst relevantDocs = await store.similaritySearch(question);\n\n// Call the chain\nconst res = await chain.call({\n  input_documents: relevantDocs,\n  question,\n});\n\nconsole.log(res);\n/*\n{\n  output_text: '\\n' +\n    '\\n' +\n    \"The president said that Justice Stephen Breyer has dedicated his life to serve this country and thanked him for his service. He also mentioned that Judge Ketanji Brown Jackson will continue Justice Breyer's legacy of excellence, and that the constitutional right affirmed in Roe v. Wade—standing precedent for half a century—is under attack as never before. He emphasized the importance of protecting access to health care, preserving a woman's right to choose, and advancing maternal health care in America. He also expressed his support for the LGBTQ+ community, and his commitment to protecting their rights, including offering a Unity Agenda for the Nation to beat the opioid epidemic, increase funding for prevention, treatment, harm reduction, and recovery, and strengthen the Violence Against Women Act.\"\n}\n*/","metadata":{"source":"examples/src/chains/qa_refine.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":34}}}}],["68d91498-2adb-411e-9bde-15c045183c53",{"pageContent":"import { loadQARefineChain } from \"langchain/chains\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { PromptTemplate } from \"langchain/prompts\";\n\nexport const questionPromptTemplateString = `Context information is below.\n---------------------\n{context}\n---------------------\nGiven the context information and no prior knowledge, answer the question: {question}`;\n\nconst questionPrompt = new PromptTemplate({\n  inputVariables: [\"context\", \"question\"],\n  template: questionPromptTemplateString,\n});\n\nconst refinePromptTemplateString = `The original question is as follows: {question}\nWe have provided an existing answer: {existing_answer}\nWe have the opportunity to refine the existing answer\n(only if needed) with some more context below.\n------------\n{context}\n------------\nGiven the new context, refine the original answer to better answer the question.\nYou must provide a response, either original answer or refined answer.`;\n\nconst refinePrompt = new PromptTemplate({\n  inputVariables: [\"question\", \"existing_answer\", \"context\"],\n  template: refinePromptTemplateString,\n});\n\n// Create the models and chain\nconst embeddings = new OpenAIEmbeddings();\nconst model = new OpenAI({ temperature: 0 });\nconst chain = loadQARefineChain(model, {\n  questionPrompt,\n  refinePrompt,\n});\n\n// Load the documents and create the vector store\nconst loader = new TextLoader(\"./state_of_the_union.txt\");\nconst docs = await loader.loadAndSplit();\nconst store = await MemoryVectorStore.fromDocuments(docs, embeddings);\n\n// Select the relevant documents\nconst question = \"What did the president say about Justice Breyer\";\nconst relevantDocs = await store.similaritySearch(question);\n\n// Call the chain\nconst res = await chain.call({\n  input_documents: relevantDocs,\n  question,\n});\n\nconsole.log(res);\n/*\n{\n  output_text: '\\n' +\n    '\\n' +\n    \"The president said that Justice Stephen Breyer has dedicated his life to serve this country and thanked him for his service. He also mentioned that Judge Ketanji Brown Jackson will continue Justice Breyer's legacy of excellence, and that the constitutional right affirmed in Roe v. Wade—standing precedent for half a century—is under attack as never before. He emphasized the importance of protecting access to health care, preserving a woman's right to choose, and advancing maternal health care in America. He also expressed his support for the LGBTQ+ community, and his commitment to protecting their rights, including offering a Unity Agenda for the Nation to beat the opioid epidemic, increase funding for prevention, treatment, harm reduction, and recovery, and strengthen the Violence Against Women Act.\"\n}\n*/","metadata":{"source":"examples/src/chains/qa_refine_custom_prompt.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":64}}}}],["4ce9dbfb-47eb-44c7-9a19-da68b6894785",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { loadQAStuffChain, loadQAMapReduceChain } from \"langchain/chains\";\nimport { Document } from \"langchain/document\";\n\n// This first example uses the `StuffDocumentsChain`.\nconst llmA = new OpenAI({});\nconst chainA = loadQAStuffChain(llmA);\nconst docs = [\n  new Document({ pageContent: \"Harrison went to Harvard.\" }),\n  new Document({ pageContent: \"Ankush went to Princeton.\" }),\n];\nconst resA = await chainA.call({\n  input_documents: docs,\n  question: \"Where did Harrison go to college?\",\n});\nconsole.log({ resA });\n// { resA: { text: ' Harrison went to Harvard.' } }\n\n// This second example uses the `MapReduceChain`.\n// Optionally limit the number of concurrent requests to the language model.\nconst llmB = new OpenAI({ maxConcurrency: 10 });\nconst chainB = loadQAMapReduceChain(llmB);\nconst resB = await chainB.call({\n  input_documents: docs,\n  question: \"Where did Harrison go to college?\",\n});\nconsole.log({ resB });\n// { resB: { text: ' Harrison went to Harvard.' } }","metadata":{"source":"examples/src/chains/question_answering.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":28}}}}],["2a61577f-5d2a-4013-ade9-36ca51a1ad09",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { loadQAMapReduceChain } from \"langchain/chains\";\nimport { Document } from \"langchain/document\";\n\n// Optionally limit the number of concurrent requests to the language model.\nconst model = new OpenAI({ temperature: 0, maxConcurrency: 10 });\nconst chain = loadQAMapReduceChain(model);\nconst docs = [\n  new Document({ pageContent: \"harrison went to harvard\" }),\n  new Document({ pageContent: \"ankush went to princeton\" }),\n];\nconst res = await chain.call({\n  input_documents: docs,\n  question: \"Where did harrison go to college\",\n});\nconsole.log({ res });","metadata":{"source":"examples/src/chains/question_answering_map_reduce.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":16}}}}],["66d27fa7-0b91-4823-a048-b92f500f16d0",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { loadQAStuffChain } from \"langchain/chains\";\nimport { Document } from \"langchain/document\";\n\n// This first example uses the `StuffDocumentsChain`.\nconst llmA = new OpenAI({});\nconst chainA = loadQAStuffChain(llmA);\nconst docs = [\n  new Document({ pageContent: \"Harrison went to Harvard.\" }),\n  new Document({ pageContent: \"Ankush went to Princeton.\" }),\n];\nconst resA = await chainA.call({\n  input_documents: docs,\n  question: \"Where did Harrison go to college?\",\n});\nconsole.log({ resA });\n// { resA: { text: ' Harrison went to Harvard.' } }","metadata":{"source":"examples/src/chains/question_answering_stuff.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":17}}}}],["2a21aa0b-3546-44d4-8599-e124954e73be",{"pageContent":"import { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport * as fs from \"fs\";\nimport {\n  RunnablePassthrough,\n  RunnableSequence,\n} from \"langchain/schema/runnable\";\nimport { StringOutputParser } from \"langchain/schema/output_parser\";\nimport {\n  ChatPromptTemplate,\n  HumanMessagePromptTemplate,\n  SystemMessagePromptTemplate,\n} from \"langchain/prompts\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { formatDocumentsAsString } from \"langchain/util/document\";\n\n// Initialize the LLM to use to answer the question.\nconst model = new ChatOpenAI({});\nconst text = fs.readFileSync(\"state_of_the_union.txt\", \"utf8\");\nconst textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });\nconst docs = await textSplitter.createDocuments([text]);\n// Create a vector store from the documents.\nconst vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());\n\n// Initialize a retriever wrapper around the vector store\nconst vectorStoreRetriever = vectorStore.asRetriever();\n\n// Create a system & human prompt for the chat model\nconst SYSTEM_TEMPLATE = `Use the following pieces of context to answer the question at the end.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n{context}`;\nconst messages = [\n  SystemMessagePromptTemplate.fromTemplate(SYSTEM_TEMPLATE),\n  HumanMessagePromptTemplate.fromTemplate(\"{question}\"),\n];\nconst prompt = ChatPromptTemplate.fromMessages(messages);\n\nconst chain = RunnableSequence.from([\n  {\n    context: vectorStoreRetriever.pipe(formatDocumentsAsString),\n    question: new RunnablePassthrough(),\n  },\n  prompt,\n  model,\n  new StringOutputParser(),\n]);\n\nconst answer = await chain.invoke(\n  \"What did the president say about Justice Breyer?\"\n);\n\nconsole.log({ answer });\n\n/*\n{\n  answer: 'The president thanked Justice Stephen Breyer for his service and honored him for his dedication to the country.'\n}\n*/","metadata":{"source":"examples/src/chains/retrieval_qa.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":60}}}}],["e1850229-f7c5-43cd-b3ec-fc48976ad487",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport * as fs from \"fs\";\nimport { loadQAMapReduceChain } from \"langchain/chains\";\n\n// Initialize the LLM to use to answer the question.\nconst model = new OpenAI({});\nconst text = fs.readFileSync(\"state_of_the_union.txt\", \"utf8\");\nconst textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });\nconst docs = await textSplitter.createDocuments([text]);\n\nconst query = \"What did the president say about Justice Breyer?\";\n\n// Create a vector store retriever from the documents.\nconst vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());\nconst retriever = vectorStore.asRetriever();\n\nconst relevantDocs = await retriever.getRelevantDocuments(query);\n\nconst mapReduceChain = loadQAMapReduceChain(model);\n\nconst result = await mapReduceChain.invoke({\n  question: query,\n  input_documents: relevantDocs,\n});\n\nconsole.log({ result });\n/*\n{\n  result: \" The President thanked Justice Breyer for his service and acknowledged him as one of the nation's top legal minds whose legacy of excellence will be continued.\"\n}\n*/","metadata":{"source":"examples/src/chains/retrieval_qa_custom.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":34}}}}],["17e99aca-6229-4f97-bd69-c9047c3f7bbf",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { RetrievalQAChain, loadQAMapReduceChain } from \"langchain/chains\";\nimport { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport * as fs from \"fs\";\n\n// Initialize the LLM to use to answer the question.\nconst model = new OpenAI({});\nconst text = fs.readFileSync(\"state_of_the_union.txt\", \"utf8\");\nconst textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });\nconst docs = await textSplitter.createDocuments([text]);\n\n// Create a vector store from the documents.\nconst vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());\n\n// Create a chain that uses a map reduce chain and HNSWLib vector store.\nconst chain = new RetrievalQAChain({\n  combineDocumentsChain: loadQAMapReduceChain(model),\n  retriever: vectorStore.asRetriever(),\n});\nconst res = await chain.call({\n  query: \"What did the president say about Justice Breyer?\",\n});\nconsole.log({ res });\n/*\n{\n  res: {\n    text: \" The president said that Justice Breyer has dedicated his life to serve his country, and thanked him for his service. He also said that Judge Ketanji Brown Jackson will continue Justice Breyer's legacy of excellence, emphasizing the importance of protecting the rights of citizens, especially women, LGBTQ+ Americans, and access to healthcare. He also expressed his commitment to supporting the younger transgender Americans in America and ensuring they are able to reach their full potential, offering a Unity Agenda for the Nation to beat the opioid epidemic and increase funding for prevention, treatment, harm reduction, and recovery.\"\n  }\n}\n*/","metadata":{"source":"examples/src/chains/retrieval_qa_custom_legacy.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":32}}}}],["155d0b4f-313a-4832-a462-3ddacfea4783",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { RetrievalQAChain, loadQAStuffChain } from \"langchain/chains\";\nimport { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport * as fs from \"fs\";\n\nconst promptTemplate = `Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\n{context}\n\nQuestion: {question}\nAnswer in Italian:`;\nconst prompt = PromptTemplate.fromTemplate(promptTemplate);\n\n// Initialize the LLM to use to answer the question.\nconst model = new OpenAI({});\nconst text = fs.readFileSync(\"state_of_the_union.txt\", \"utf8\");\nconst textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });\nconst docs = await textSplitter.createDocuments([text]);\n\n// Create a vector store from the documents.\nconst vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());\n\n// Create a chain that uses a stuff chain and HNSWLib vector store.\nconst chain = new RetrievalQAChain({\n  combineDocumentsChain: loadQAStuffChain(model, { prompt }),\n  retriever: vectorStore.asRetriever(),\n});\nconst res = await chain.call({\n  query: \"What did the president say about Justice Breyer?\",\n});\nconsole.log({ res });\n/*\n{\n  res: {\n    text: ' Il presidente ha elogiato Justice Breyer per il suo servizio e lo ha ringraziato.'\n  }\n}\n*/","metadata":{"source":"examples/src/chains/retrieval_qa_custom_prompt_legacy.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":41}}}}],["49db394d-c2d4-4f5e-9845-59138755ef50",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { RetrievalQAChain } from \"langchain/chains\";\nimport { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport * as fs from \"fs\";\n\n// Initialize the LLM to use to answer the question.\nconst model = new OpenAI({});\nconst text = fs.readFileSync(\"state_of_the_union.txt\", \"utf8\");\nconst textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });\nconst docs = await textSplitter.createDocuments([text]);\n\n// Create a vector store from the documents.\nconst vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());\n\n// Initialize a retriever wrapper around the vector store\nconst vectorStoreRetriever = vectorStore.asRetriever();\n\n// Create a chain that uses the OpenAI LLM and HNSWLib vector store.\nconst chain = RetrievalQAChain.fromLLM(model, vectorStoreRetriever);\nconst res = await chain.call({\n  query: \"What did the president say about Justice Breyer?\",\n});\nconsole.log({ res });\n/*\n{\n  res: {\n    text: 'The president said that Justice Breyer was an Army veteran, Constitutional scholar,\n    and retiring Justice of the United States Supreme Court and thanked him for his service.'\n  }\n}\n*/","metadata":{"source":"examples/src/chains/retrieval_qa_legacy.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":33}}}}],["37d2f481-43ec-4ff5-bc2e-a760f4639db3",{"pageContent":"import { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport * as fs from \"fs\";\nimport {\n  ChatPromptTemplate,\n  HumanMessagePromptTemplate,\n  SystemMessagePromptTemplate,\n} from \"langchain/prompts\";\nimport { StringOutputParser } from \"langchain/schema/output_parser\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { RunnableSequence } from \"langchain/schema/runnable\";\nimport { formatDocumentsAsString } from \"langchain/util/document\";\n\nconst text = fs.readFileSync(\"state_of_the_union.txt\", \"utf8\");\n\nconst query = \"What did the president say about Justice Breyer?\";\n\n// Initialize the LLM to use to answer the question.\nconst model = new ChatOpenAI({});\n\n// Chunk the text into documents.\nconst textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });\nconst docs = await textSplitter.createDocuments([text]);\n\n// Create a vector store from the documents.\nconst vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());\nconst vectorStoreRetriever = vectorStore.asRetriever();\n\n// Create a system & human prompt for the chat model\nconst SYSTEM_TEMPLATE = `Use the following pieces of context to answer the users question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n{context}`;\n\nconst messages = [\n  SystemMessagePromptTemplate.fromTemplate(SYSTEM_TEMPLATE),\n  HumanMessagePromptTemplate.fromTemplate(\"{question}\"),\n];\nconst prompt = ChatPromptTemplate.fromMessages(messages);\n\nconst chain = RunnableSequence.from([\n  {\n    // Extract the \"question\" field from the input object and pass it to the retriever as a string\n    sourceDocuments: RunnableSequence.from([\n      (input) => input.question,\n      vectorStoreRetriever,\n    ]),\n    question: (input) => input.question,\n  },\n  {\n    // Pass the source documents through unchanged so that we can return them directly in the final result\n    sourceDocuments: (previousStepResult) => previousStepResult.sourceDocuments,\n    question: (previousStepResult) => previousStepResult.question,\n    context: (previousStepResult) =>\n      formatDocumentsAsString(previousStepResult.sourceDocuments),\n  },\n  {\n    result: prompt.pipe(model).pipe(new StringOutputParser()),\n    sourceDocuments: (previousStepResult) => previousStepResult.sourceDocuments,\n  },\n]);","metadata":{"source":"examples/src/chains/retrieval_qa_sources.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":62}}}}],["b4edf8a0-a1b3-4272-8750-59d14eec6407",{"pageContent":"const res = await chain.invoke({\n  question: query,\n});\n\nconsole.log(JSON.stringify(res, null, 2));","metadata":{"source":"examples/src/chains/retrieval_qa_sources.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":64,"to":68}}}}],["99500a2e-c0a7-40b8-b85e-e37a2c2fe3c5",{"pageContent":"/*\n{\n  \"result\": \"The President honored Justice Stephen Breyer, describing him as an Army veteran, Constitutional scholar, and a retiring Justice of the United States Supreme Court. The President thanked him for his service.\",\n  \"sourceDocuments\": [\n    {\n      \"pageContent\": \"In state after state, new laws have been passed, not only to suppress the vote, but to subvert entire elections. \\n\\nWe cannot let this happen. \\n\\nTonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \\n\\nTonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\n\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\n\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.\",\n      \"metadata\": {\n        \"loc\": {\n          \"lines\": {\n            \"from\": 524,\n            \"to\": 534\n          }\n        }\n      }\n    },\n    {\n      \"pageContent\": \"And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence. \\n\\nA former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she’s been nominated, she’s received a broad range of support—from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. \\n\\nAnd if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. \\n\\nWe can do both. At our border, we’ve installed new technology like cutting-edge scanners to better detect drug smuggling.  \\n\\nWe’ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  \\n\\nWe’re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster.\",\n      \"metadata\": {\n        \"loc\": {\n          \"lines\": {\n            \"from\": 534,\n            \"to\": 544\n          }\n        }\n      }\n    },\n    {\n      \"pageContent\": \"Let’s get it done once and for all. \\n\\nAdvancing liberty and justice also requires protecting the rights of women. \\n\\nThe constitutional right affirmed in Roe v. Wade—standing precedent for half a century—is under attack as never before. \\n\\nIf we want to go forward—not backward—we must protect access to health care. Preserve a woman’s right to choose. And let’s continue to advance maternal health care in America. \\n\\nAnd for our LGBTQ+ Americans, let’s finally get the bipartisan Equality Act to my desk. The onslaught of state laws targeting transgender Americans and their families is wrong. \\n\\nAs I said last year, especially to our younger transgender Americans, I will always have your back as your President, so you can be yourself and reach your God-given potential.\",\n      \"metadata\": {\n        \"loc\": {\n          \"lines\": {\n            \"from\": 558,\n            \"to\": 568\n          }\n        }\n      }\n    },\n    {","metadata":{"source":"examples/src/chains/retrieval_qa_sources.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":70,"to":107}}}}],["4fccfd25-5147-4590-99a8-f5566b75858a",{"pageContent":"\"pageContent\": \"As I said last year, especially to our younger transgender Americans, I will always have your back as your President, so you can be yourself and reach your God-given potential. \\n\\nWhile it often appears that we never agree, that isn’t true. I signed 80 bipartisan bills into law last year. From preventing government shutdowns to protecting Asian-Americans from still-too-common hate crimes to reforming military justice. \\n\\nAnd soon, we’ll strengthen the Violence Against Women Act that I first wrote three decades ago. It is important for us to show the nation that we can come together and do big things. \\n\\nSo tonight I’m offering a Unity Agenda for the Nation. Four big things we can do together.  \\n\\nFirst, beat the opioid epidemic. \\n\\nThere is so much we can do. Increase funding for prevention, treatment, harm reduction, and recovery.\",\n      \"metadata\": {\n        \"loc\": {\n          \"lines\": {\n            \"from\": 568,\n            \"to\": 578\n          }\n        }\n      }\n    }\n  ]\n}\n*/","metadata":{"source":"examples/src/chains/retrieval_qa_sources.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":108,"to":120}}}}],["0d278ea3-db6e-4974-b95c-5bb5b50bfb92",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { RetrievalQAChain } from \"langchain/chains\";\nimport { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport * as fs from \"fs\";\n\n// Initialize the LLM to use to answer the question.\nconst model = new OpenAI({});\nconst text = fs.readFileSync(\"state_of_the_union.txt\", \"utf8\");\nconst textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });\nconst docs = await textSplitter.createDocuments([text]);\n\n// Create a vector store from the documents.\nconst vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());\n\n// Create a chain that uses a map reduce chain and HNSWLib vector store.\nconst chain = RetrievalQAChain.fromLLM(model, vectorStore.asRetriever(), {\n  returnSourceDocuments: true, // Can also be passed into the constructor\n});","metadata":{"source":"examples/src/chains/retrieval_qa_sources_legacy.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":20}}}}],["0aea2574-6d72-4f41-a0eb-b483e6c9fc70",{"pageContent":"const res = await chain.call({\n  query: \"What did the president say about Justice Breyer?\",\n});\nconsole.log(JSON.stringify(res, null, 2));\n/*\n{\n  \"text\": \" The president thanked Justice Breyer for his service and asked him to stand so he could be seen.\",\n  \"sourceDocuments\": [\n    {\n      \"pageContent\": \"Justice Breyer, thank you for your service. Thank you, thank you, thank you. I mean it. Get up. Stand — let me see you. Thank you.\\n\\nAnd we all know — no matter what your ideology, we all know one of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.\\n\\nAs I did four days ago, I’ve nominated a Circuit Court of Appeals — Ketanji Brown Jackson. One of our nation’s top legal minds who will continue in just Brey- — Justice Breyer’s legacy of excellence. A former top litigator in private practice, a former federal public defender from a family of public-school educators and police officers — she’s a consensus builder.\\n\\nSince she’s been nominated, she’s received a broad range of support, including the Fraternal Order of Police and former judges appointed by Democrats and Republicans.\",\n      \"metadata\": {\n        \"loc\": {\n          \"lines\": {\n            \"from\": 481,\n            \"to\": 487\n          }\n        }\n      }\n    },\n    {\n      \"pageContent\": \"Since she’s been nominated, she’s received a broad range of support, including the Fraternal Order of Police and former judges appointed by Democrats and Republicans.\\n\\nJudge Ketanji Brown Jackson\\nPresident Biden's Unity AgendaLearn More\\nSince she’s been nominated, she’s received a broad range of support, including the Fraternal Order of Police and former judges appointed by Democrats and Republicans.\\n\\nFolks, if we are to advance liberty and justice, we need to secure our border and fix the immigration system.\\n\\nAnd as you might guess, I think we can do both. At our border, we’ve installed new technology, like cutting-edge scanners, to better detect drug smuggling.\\n\\nWe’ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.\\n\\nWe’re putting in place dedicated immigration judges in significant larger number so families fleeing persecution and violence can have their cases — cases heard faster — and those who aren’t legitimately here can be sent back.\",\n      \"metadata\": {\n        \"loc\": {\n          \"lines\": {\n            \"from\": 487,\n            \"to\": 499\n          }\n        }\n      }\n    },\n    {\n      \"pageContent\": \"These laws don’t infringe on the Second Amendment; they save lives.\\n\\nGun Violence\\n\\n\\nThe most fundamental right in America is the right to vote and have it counted. And look, it’s under assault.\\n\\nIn state after state, new laws have been passed not only to suppress the vote — we’ve been there before — but to subvert the entire election. We can’t let this happen.\\n\\nTonight, I call on the Senate to pass — pass the Freedom to Vote Act. Pass the John Lewis Act — Voting Rights Act. And while you’re at it, pass the DISCLOSE Act so Americans know who is funding our elections.\\n\\nLook, tonight, I’d — I’d like to honor someone who has dedicated his life to serve this country: Justice Breyer — an Army veteran, Constitutional scholar, retiring Justice of the United States Supreme Court.\\n\\nJustice Breyer, thank you for your service. Thank you, thank you, thank you. I mean it. Get up. Stand — let me see you. Thank you.\",\n      \"metadata\": {\n        \"loc\": {\n          \"lines\": {\n            \"from\": 468,\n            \"to\": 481\n          }\n        }\n      }\n    },\n    {","metadata":{"source":"examples/src/chains/retrieval_qa_sources_legacy.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":21,"to":62}}}}],["e9059dae-951b-4603-8210-1e00d2b2f783",{"pageContent":"\"pageContent\": \"If you want to go forward not backwards, we must protect access to healthcare; preserve a woman’s right to choose — and continue to advance maternal healthcare for all Americans.\\n\\nRoe v. Wade\\n\\n\\nAnd folks, for our LGBTQ+ Americans, let’s finally get the bipartisan Equality Act to my desk. The onslaught of state laws targeting transgender Americans and their families — it’s simply wrong.\\n\\nAs I said last year, especially to our younger transgender Americans, I’ll always have your back as your President so you can be yourself and reach your God-given potential.\\n\\nBipartisan Equality Act\\n\\n\\nFolks as I’ve just demonstrated, while it often appears we do not agree and that — we — we do agree on a lot more things than we acknowledge.\",\n      \"metadata\": {\n        \"loc\": {\n          \"lines\": {\n            \"from\": 511,\n            \"to\": 523\n          }\n        }\n      }\n    }\n  ]\n}\n*/","metadata":{"source":"examples/src/chains/retrieval_qa_sources_legacy.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":63,"to":75}}}}],["683efd06-e1b1-4bd6-bd3c-6332ebe58b39",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { RetrievalQAChain } from \"langchain/chains\";\nimport { RemoteLangChainRetriever } from \"langchain/retrievers/remote\";\n\nexport const run = async () => {\n  // Initialize the LLM to use to answer the question.\n  const model = new OpenAI({});\n\n  // Initialize the remote retriever.\n  const retriever = new RemoteLangChainRetriever({\n    url: \"http://0.0.0.0:8080/retrieve\", // Replace with your own URL.\n    auth: { bearer: \"foo\" }, // Replace with your own auth.\n    inputKey: \"message\",\n    responseKey: \"response\",\n  });\n\n  // Create a chain that uses the OpenAI LLM and remote retriever.\n  const chain = RetrievalQAChain.fromLLM(model, retriever);\n\n  // Call the chain with a query.\n  const res = await chain.call({\n    query: \"What did the president say about Justice Breyer?\",\n  });\n  console.log({ res });\n  /*\n  {\n    res: {\n      text: 'The president said that Justice Breyer was an Army veteran, Constitutional scholar,\n      and retiring Justice of the United States Supreme Court and thanked him for his service.'\n    }\n  }\n  */\n};","metadata":{"source":"examples/src/chains/retrieval_qa_with_remote.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":33}}}}],["d1f741dd-c5de-40ae-9c82-2649da9f1d7e",{"pageContent":"import { SequentialChain, LLMChain } from \"langchain/chains\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { PromptTemplate } from \"langchain/prompts\";\n\n// This is an LLMChain to write a synopsis given a title of a play and the era it is set in.\nconst llm = new OpenAI({ temperature: 0 });\nconst template = `You are a playwright. Given the title of play and the era it is set in, it is your job to write a synopsis for that title.\n\nTitle: {title}\nEra: {era}\nPlaywright: This is a synopsis for the above play:`;\nconst promptTemplate = new PromptTemplate({\n  template,\n  inputVariables: [\"title\", \"era\"],\n});\nconst synopsisChain = new LLMChain({\n  llm,\n  prompt: promptTemplate,\n  outputKey: \"synopsis\",\n});\n\n// This is an LLMChain to write a review of a play given a synopsis.\nconst reviewLLM = new OpenAI({ temperature: 0 });\nconst reviewTemplate = `You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.\n  \n   Play Synopsis:\n   {synopsis}\n   Review from a New York Times play critic of the above play:`;\nconst reviewPromptTemplate = new PromptTemplate({\n  template: reviewTemplate,\n  inputVariables: [\"synopsis\"],\n});\nconst reviewChain = new LLMChain({\n  llm: reviewLLM,\n  prompt: reviewPromptTemplate,\n  outputKey: \"review\",\n});\n\nconst overallChain = new SequentialChain({\n  chains: [synopsisChain, reviewChain],\n  inputVariables: [\"era\", \"title\"],\n  // Here we return multiple variables\n  outputVariables: [\"synopsis\", \"review\"],\n  verbose: true,\n});\nconst chainExecutionResult = await overallChain.call({\n  title: \"Tragedy at sunset on the beach\",\n  era: \"Victorian England\",\n});\nconsole.log(chainExecutionResult);\n/*\n    variable chainExecutionResult contains final review and intermediate synopsis (as specified by outputVariables). The data is generated based on the input title and era:\n\n    \"{\n      \"review\": \"\n\n    Tragedy at Sunset on the Beach is a captivating and heartbreaking story of love and loss. Set in Victorian England, the play follows Emily, a young woman struggling to make ends meet in a small coastal town. Emily's dreams of a better life are dashed when she discovers her employer's scandalous affair, and her plans are further thwarted when she meets a handsome stranger on the beach.\n\n    The play is a powerful exploration of the human condition, as Emily must grapple with the truth and make a difficult decision that will change her life forever. The performances are outstanding, with the actors bringing a depth of emotion to their characters that is both heartbreaking and inspiring.\n\n    Overall, Tragedy at Sunset on the Beach is a beautiful and moving play that will leave audiences in tears. It is a must-see for anyone looking for a powerful and thought-provoking story.\",\n      \"synopsis\": \"\n\n    Tragedy at Sunset on the Beach is a play set in Victorian England. It tells the story of a young woman, Emily, who is struggling to make ends meet in a small coastal town. She works as a maid for a wealthy family, but her dreams of a better life are dashed when she discovers that her employer is involved in a scandalous affair.\n\n    Emily is determined to make a better life for herself, but her plans are thwarted when she meets a handsome stranger on the beach one evening. The two quickly fall in love, but their happiness is short-lived when Emily discovers that the stranger is actually a member of the wealthy family she works for.\n\n    The play follows Emily as she struggles to come to terms with the truth and make sense of her life. As the sun sets on the beach, Emily must decide whether to stay with the man she loves or to leave him and pursue her dreams. In the end, Emily must make a heartbreaking decision that will change her life forever.\",\n    }\"\n*/","metadata":{"source":"examples/src/chains/sequential_chain.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":70}}}}],["6f4a61d4-fa16-4715-bc90-6891ce38dd1d",{"pageContent":"import { SimpleSequentialChain, LLMChain } from \"langchain/chains\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { PromptTemplate } from \"langchain/prompts\";\n\n// This is an LLMChain to write a synopsis given a title of a play.\nconst llm = new OpenAI({ temperature: 0 });\nconst template = `You are a playwright. Given the title of play, it is your job to write a synopsis for that title.\n \n  Title: {title}\n  Playwright: This is a synopsis for the above play:`;\nconst promptTemplate = new PromptTemplate({\n  template,\n  inputVariables: [\"title\"],\n});\nconst synopsisChain = new LLMChain({ llm, prompt: promptTemplate });\n\n// This is an LLMChain to write a review of a play given a synopsis.\nconst reviewLLM = new OpenAI({ temperature: 0 });\nconst reviewTemplate = `You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.\n \n  Play Synopsis:\n  {synopsis}\n  Review from a New York Times play critic of the above play:`;\nconst reviewPromptTemplate = new PromptTemplate({\n  template: reviewTemplate,\n  inputVariables: [\"synopsis\"],\n});\nconst reviewChain = new LLMChain({\n  llm: reviewLLM,\n  prompt: reviewPromptTemplate,\n});\n\nconst overallChain = new SimpleSequentialChain({\n  chains: [synopsisChain, reviewChain],\n  verbose: true,\n});\nconst review = await overallChain.run(\"Tragedy at sunset on the beach\");\nconsole.log(review);\n/*\n    variable review contains the generated play review based on the input title and synopsis generated in the first step:\n\n    \"Tragedy at Sunset on the Beach is a powerful and moving story of love, loss, and redemption. The play follows the story of two young lovers, Jack and Jill, whose plans for a future together are tragically cut short when Jack is killed in a car accident. The play follows Jill as she struggles to cope with her grief and eventually finds solace in the arms of another man. \n    The play is beautifully written and the performances are outstanding. The actors bring the characters to life with their heartfelt performances, and the audience is taken on an emotional journey as Jill is forced to confront her grief and make a difficult decision between her past and her future. The play culminates in a powerful climax that will leave the audience in tears. \n    Overall, Tragedy at Sunset on the Beach is a powerful and moving story that will stay with you long after the curtain falls. It is a must-see for anyone looking for an emotionally charged and thought-provoking experience.\"\n*/","metadata":{"source":"examples/src/chains/simple_sequential_chain.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":45}}}}],["d4f631d2-8ee1-46b9-9dce-61c19d8b871a",{"pageContent":"import { DataSource } from \"typeorm\";\nimport { SqlDatabase } from \"langchain/sql_db\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport { RunnableSequence } from \"langchain/schema/runnable\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { StringOutputParser } from \"langchain/schema/output_parser\";\n\n/**\n * This example uses Chinook database, which is a sample database available for SQL Server, Oracle, MySQL, etc.\n * To set it up follow the instructions on https://database.guide/2-sample-databases-sqlite/, placing the .db file\n * in the examples folder.\n */\nconst datasource = new DataSource({\n  type: \"sqlite\",\n  database: \"Chinook.db\",\n});\n\nconst db = await SqlDatabase.fromDataSourceParams({\n  appDataSource: datasource,\n});\n\nconst llm = new ChatOpenAI();\n\n/**\n * Create the first prompt template used for getting the SQL query.\n */\nconst prompt =\n  PromptTemplate.fromTemplate(`Based on the provided SQL table schema below, write a SQL query that would answer the user's question.\n------------\nSCHEMA: {schema}\n------------\nQUESTION: {question}\n------------\nSQL QUERY:`);\n/**\n * You can also load a default prompt by importing from \"langchain/sql_db\"\n *\n * import {\n *   DEFAULT_SQL_DATABASE_PROMPT\n *   SQL_POSTGRES_PROMPT\n *   SQL_SQLITE_PROMPT\n *   SQL_MSSQL_PROMPT\n *   SQL_MYSQL_PROMPT\n *   SQL_SAP_HANA_PROMPT\n * } from \"langchain/sql_db\";\n *\n */\n\n/**\n * Create a new RunnableSequence where we pipe the output from `db.getTableInfo()`\n * and the users question, into the prompt template, and then into the llm.\n * We're also applying a stop condition to the llm, so that it stops when it\n * sees the `\\nSQLResult:` token.\n */\nconst sqlQueryChain = RunnableSequence.from([\n  {\n    schema: async () => db.getTableInfo(),\n    question: (input: { question: string }) => input.question,\n  },\n  prompt,\n  llm.bind({ stop: [\"\\nSQLResult:\"] }),\n  new StringOutputParser(),\n]);\n\nconst res = await sqlQueryChain.invoke({\n  question: \"How many employees are there?\",\n});\nconsole.log({ res });\n\n/**\n * { res: 'SELECT COUNT(*) FROM tracks;' }\n */\n\n/**\n * Create the final prompt template which is tasked with getting the natural language response.\n */\nconst finalResponsePrompt =\n  PromptTemplate.fromTemplate(`Based on the table schema below, question, SQL query, and SQL response, write a natural language response:\n------------\nSCHEMA: {schema}\n------------\nQUESTION: {question}\n------------\nSQL QUERY: {query}\n------------\nSQL RESPONSE: {response}\n------------\nNATURAL LANGUAGE RESPONSE:`);\n\n/**\n * Create a new RunnableSequence where we pipe the output from the previous chain, the users question,\n * and the SQL query, into the prompt template, and then into the llm.\n * Using the result from the `sqlQueryChain` we can run the SQL query via `db.run(input.query)`.\n */\nconst finalChain = RunnableSequence.from([\n  {\n    question: (input) => input.question,\n    query: sqlQueryChain,\n  },\n  {\n    schema: async () => db.getTableInfo(),\n    question: (input) => input.question,\n    query: (input) => input.query,\n    response: (input) => db.run(input.query),\n  },\n  finalResponsePrompt,\n  llm,\n  new StringOutputParser(),\n]);\n\nconst finalResponse = await finalChain.invoke({\n  question: \"How many employees are there?\",\n});\n\nconsole.log({ finalResponse });\n\n/**\n * { finalResponse: 'There are 8 employees.' }\n */","metadata":{"source":"examples/src/chains/sql_db.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":119}}}}],["adff9d87-6e9c-4b70-9109-82295a494da7",{"pageContent":"import { DataSource } from \"typeorm\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { SqlDatabase } from \"langchain/sql_db\";\nimport { SqlDatabaseChain } from \"langchain/chains/sql_db\";\nimport { PromptTemplate } from \"langchain/prompts\";\n\nconst template = `Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer.\nUse the following format:\n\nQuestion: \"Question here\"\nSQLQuery: \"SQL Query to run\"\nSQLResult: \"Result of the SQLQuery\"\nAnswer: \"Final answer here\"\n\nOnly use the following tables:\n\n{table_info}\n\nIf someone asks for the table foobar, they really mean the employee table.\n\nQuestion: {input}`;\n\nconst prompt = PromptTemplate.fromTemplate(template);\n\n/**\n * This example uses Chinook database, which is a sample database available for SQL Server, Oracle, MySQL, etc.\n * To set it up follow the instructions on https://database.guide/2-sample-databases-sqlite/, placing the .db file\n * in the examples folder.\n */\nconst datasource = new DataSource({\n  type: \"sqlite\",\n  database: \"data/Chinook.db\",\n});\n\nconst db = await SqlDatabase.fromDataSourceParams({\n  appDataSource: datasource,\n});\n\nconst chain = new SqlDatabaseChain({\n  llm: new OpenAI({ temperature: 0 }),\n  database: db,\n  sqlOutputKey: \"sql\",\n  prompt,\n});\n\nconst res = await chain.call({\n  query: \"How many employees are there in the foobar table?\",\n});\nconsole.log(res);\n\n/*\n  {\n    result: ' There are 8 employees in the foobar table.',\n    sql: ' SELECT COUNT(*) FROM Employee;'\n  }\n*/","metadata":{"source":"examples/src/chains/sql_db_custom_prompt.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":56}}}}],["cf3ced87-7abc-47bf-8c65-22d42d716c80",{"pageContent":"import { DataSource } from \"typeorm\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { SqlDatabase } from \"langchain/sql_db\";\nimport { SqlDatabaseChain } from \"langchain/chains/sql_db\";\nimport { PromptTemplate } from \"langchain/prompts\";\n\nconst template = `Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer.\nUse the following format:\n\nQuestion: \"Question here\"\nSQLQuery: \"SQL Query to run\"\nSQLResult: \"Result of the SQLQuery\"\nAnswer: \"Final answer here\"\n\nOnly use the following tables:\n\n{table_info}\n\nIf someone asks for the table foobar, they really mean the employee table.\n\nQuestion: {input}`;\n\nconst prompt = PromptTemplate.fromTemplate(template);\n\n/**\n * This example uses Chinook database, which is a sample database available for SQL Server, Oracle, MySQL, etc.\n * To set it up follow the instructions on https://database.guide/2-sample-databases-sqlite/, placing the .db file\n * in the examples folder.\n */\nconst datasource = new DataSource({\n  type: \"sqlite\",\n  database: \"data/Chinook.db\",\n});\n\nconst db = await SqlDatabase.fromDataSourceParams({\n  appDataSource: datasource,\n});\n\nconst chain = new SqlDatabaseChain({\n  llm: new OpenAI({ temperature: 0 }),\n  database: db,\n  sqlOutputKey: \"sql\",\n  prompt,\n});\n\nconst res = await chain.call({\n  query: \"How many employees are there in the foobar table?\",\n});\nconsole.log(res);\n\n/*\n  {\n    result: ' There are 8 employees in the foobar table.',\n    sql: ' SELECT COUNT(*) FROM Employee;'\n  }\n*/","metadata":{"source":"examples/src/chains/sql_db_custom_prompt_legacy.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":56}}}}],["3ed925e8-c63c-493f-9fcb-6758ec3d0537",{"pageContent":"import { DataSource } from \"typeorm\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { SqlDatabase } from \"langchain/sql_db\";\nimport { SqlDatabaseChain } from \"langchain/chains/sql_db\";\n\n/**\n * This example uses Chinook database, which is a sample database available for SQL Server, Oracle, MySQL, etc.\n * To set it up follow the instructions on https://database.guide/2-sample-databases-sqlite/, placing the .db file\n * in the examples folder.\n */\nconst datasource = new DataSource({\n  type: \"sqlite\",\n  database: \"Chinook.db\",\n});\n\nconst db = await SqlDatabase.fromDataSourceParams({\n  appDataSource: datasource,\n});\n\nconst chain = new SqlDatabaseChain({\n  llm: new OpenAI({ temperature: 0 }),\n  database: db,\n});\n\nconst res = await chain.run(\"How many tracks are there?\");\nconsole.log(res);\n// There are 3503 tracks.","metadata":{"source":"examples/src/chains/sql_db_legacy.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":27}}}}],["f23f9b32-a4c0-4e7b-9be1-5481de079257",{"pageContent":"import { DataSource } from \"typeorm\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { SqlDatabase } from \"langchain/sql_db\";\nimport { SqlDatabaseChain } from \"langchain/chains/sql_db\";\n\n/**\n * This example uses a SAP HANA Cloud database. You can create a free trial database via https://developers.sap.com/tutorials/hana-cloud-deploying.html\n *\n * You will need to add the following packages to your package.json as they are required when using typeorm with SAP HANA:\n *\n *    \"hdb-pool\": \"^0.1.6\",             (or latest version)\n *    \"@sap/hana-client\": \"^2.17.22\"    (or latest version)\n *\n */\nconst datasource = new DataSource({\n  type: \"sap\",\n  host: \"<ADD_YOURS_HERE>.hanacloud.ondemand.com\",\n  port: 443,\n  username: \"<ADD_YOURS_HERE>\",\n  password: \"<ADD_YOURS_HERE>\",\n  schema: \"<ADD_YOURS_HERE>\",\n  encrypt: true,\n  extra: {\n    sslValidateCertificate: false,\n  },\n});\n\nconst db = await SqlDatabase.fromDataSourceParams({\n  appDataSource: datasource,\n});\n\nconst chain = new SqlDatabaseChain({\n  llm: new OpenAI({ temperature: 0 }),\n  database: db,\n});\n\nconst res = await chain.run(\"How many tracks are there?\");\nconsole.log(res);\n// There are 3503 tracks.","metadata":{"source":"examples/src/chains/sql_db_saphana.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":39}}}}],["7d8d2e19-4bd9-41cc-ac84-46ec41055421",{"pageContent":"import { DataSource } from \"typeorm\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { SqlDatabase } from \"langchain/sql_db\";\nimport { SqlDatabaseChain } from \"langchain/chains/sql_db\";\n\n/**\n * This example uses a SAP HANA Cloud database. You can create a free trial database via https://developers.sap.com/tutorials/hana-cloud-deploying.html\n *\n * You will need to add the following packages to your package.json as they are required when using typeorm with SAP HANA:\n *\n *    \"hdb-pool\": \"^0.1.6\",             (or latest version)\n *    \"@sap/hana-client\": \"^2.17.22\"    (or latest version)\n *\n */\nconst datasource = new DataSource({\n  type: \"sap\",\n  host: \"<ADD_YOURS_HERE>.hanacloud.ondemand.com\",\n  port: 443,\n  username: \"<ADD_YOURS_HERE>\",\n  password: \"<ADD_YOURS_HERE>\",\n  schema: \"<ADD_YOURS_HERE>\",\n  encrypt: true,\n  extra: {\n    sslValidateCertificate: false,\n  },\n});\n\nconst db = await SqlDatabase.fromDataSourceParams({\n  appDataSource: datasource,\n});\n\nconst chain = new SqlDatabaseChain({\n  llm: new OpenAI({ temperature: 0 }),\n  database: db,\n});\n\nconst res = await chain.run(\"How many tracks are there?\");\nconsole.log(res);\n// There are 3503 tracks.","metadata":{"source":"examples/src/chains/sql_db_saphana_legacy.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":39}}}}],["1a8e844f-1d95-4dd1-8536-c6f930169766",{"pageContent":"import { DataSource } from \"typeorm\";\nimport { SqlDatabase } from \"langchain/sql_db\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport { RunnableSequence } from \"langchain/schema/runnable\";\nimport { StringOutputParser } from \"langchain/schema/output_parser\";\n\n/**\n * This example uses Chinook database, which is a sample database available for SQL Server, Oracle, MySQL, etc.\n * To set it up follow the instructions on https://database.guide/2-sample-databases-sqlite/, placing the .db file\n * in the examples folder.\n */\nconst datasource = new DataSource({\n  type: \"sqlite\",\n  database: \"Chinook.db\",\n});\n\nconst db = await SqlDatabase.fromDataSourceParams({\n  appDataSource: datasource,\n});\n\nconst llm = new ChatOpenAI();\n\n/**\n * Create the first prompt template used for getting the SQL query.\n */\nconst prompt =\n  PromptTemplate.fromTemplate(`Based on the provided SQL table schema below, write a SQL query that would answer the user's question.\n------------\nSCHEMA: {schema}\n------------\nQUESTION: {question}\n------------\nSQL QUERY:`);\n\n/**\n * Create a new RunnableSequence where we pipe the output from `db.getTableInfo()`\n * and the users question, into the prompt template, and then into the llm.\n * We're also applying a stop condition to the llm, so that it stops when it\n * sees the `\\nSQLResult:` token.\n */\nconst sqlQueryChain = RunnableSequence.from([\n  {\n    schema: async () => db.getTableInfo(),\n    question: (input: { question: string }) => input.question,\n  },\n  prompt,\n  llm.bind({ stop: [\"\\nSQLResult:\"] }),\n  new StringOutputParser(),\n]);\n\n/**\n * Create the final prompt template which is tasked with getting the natural\n * language response to the SQL query.\n */\nconst finalResponsePrompt =\n  PromptTemplate.fromTemplate(`Based on the table schema below, question, SQL query, and SQL response, write a natural language response:\n------------\nSCHEMA: {schema}\n------------\nQUESTION: {question}\n------------\nSQL QUERY: {query}\n------------\nSQL RESPONSE: {response}\n------------\nNATURAL LANGUAGE RESPONSE:`);\n\n/**\n * Create a new RunnableSequence where we pipe the output from the previous chain, the users question,\n * and the SQL query, into the prompt template, and then into the llm.\n * Using the result from the `sqlQueryChain` we can run the SQL query via `db.run(input.query)`.\n *\n * Lastly we're piping the result of the first chain (the outputted SQL query) so it is\n * logged along with the natural language response.\n */\nconst finalChain = RunnableSequence.from([\n  {\n    question: (input) => input.question,\n    query: sqlQueryChain,\n  },\n  {\n    schema: async () => db.getTableInfo(),\n    question: (input) => input.question,\n    query: (input) => input.query,\n    response: (input) => db.run(input.query),\n  },\n  {\n    result: finalResponsePrompt.pipe(llm).pipe(new StringOutputParser()),\n    // Pipe the query through here unchanged so it gets logged alongside the result.\n    sql: (previousStepResult) => previousStepResult.query,\n  },\n]);\n\nconst finalResponse = await finalChain.invoke({\n  question: \"How many employees are there?\",\n});\n\nconsole.log({ finalResponse });\n\n/**\n * {\n *   finalResponse: {\n *     result: 'There are 8 employees.',\n *     sql: 'SELECT COUNT(*) FROM tracks;'\n *   }\n * }\n */","metadata":{"source":"examples/src/chains/sql_db_sql_output.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":108}}}}],["b5eafe17-07d5-4112-8cd3-b356102c0bcd",{"pageContent":"import { DataSource } from \"typeorm\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { SqlDatabase } from \"langchain/sql_db\";\nimport { SqlDatabaseChain } from \"langchain/chains/sql_db\";\n\n/**\n * This example uses Chinook database, which is a sample database available for SQL Server, Oracle, MySQL, etc.\n * To set it up follow the instructions on https://database.guide/2-sample-databases-sqlite/, placing the .db file\n * in the examples folder.\n */\nconst datasource = new DataSource({\n  type: \"sqlite\",\n  database: \"Chinook.db\",\n});\n\nconst db = await SqlDatabase.fromDataSourceParams({\n  appDataSource: datasource,\n});\n\nconst chain = new SqlDatabaseChain({\n  llm: new OpenAI({ temperature: 0 }),\n  database: db,\n  sqlOutputKey: \"sql\",\n});\n\nconst res = await chain.call({ query: \"How many tracks are there?\" });\n/* Expected result:\n * {\n *   result: ' There are 3503 tracks.',\n *   sql: ' SELECT COUNT(*) FROM \"Track\";'\n * }\n */\nconsole.log(res);","metadata":{"source":"examples/src/chains/sql_db_sql_output_legacy.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":33}}}}],["0fd3f53a-f25c-4a38-ab76-d66b9f023035",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { loadSummarizationChain } from \"langchain/chains\";\nimport { Document } from \"langchain/document\";\n\nexport const run = async () => {\n  const model = new OpenAI({});\n  const chain = loadSummarizationChain(model, { type: \"stuff\" });\n  const docs = [\n    new Document({ pageContent: \"harrison went to harvard\" }),\n    new Document({ pageContent: \"ankush went to princeton\" }),\n  ];\n  const res = await chain.call({\n    input_documents: docs,\n  });\n  console.log(res);\n};","metadata":{"source":"examples/src/chains/summarization.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":16}}}}],["b6a0547e-9531-40d7-b883-78ff5e164e51",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { loadSummarizationChain } from \"langchain/chains\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport * as fs from \"fs\";\n\n// In this example, we use a `MapReduceDocumentsChain` specifically prompted to summarize a set of documents.\nconst text = fs.readFileSync(\"state_of_the_union.txt\", \"utf8\");\nconst model = new OpenAI({ temperature: 0 });\nconst textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });\nconst docs = await textSplitter.createDocuments([text]);\n\n// This convenience function creates a document chain prompted to summarize a set of documents.\nconst chain = loadSummarizationChain(model, { type: \"map_reduce\" });\nconst res = await chain.call({\n  input_documents: docs,\n});\nconsole.log({ res });\n/*\n{\n  res: {\n    text: ' President Biden is taking action to protect Americans from the COVID-19 pandemic and Russian aggression, providing economic relief, investing in infrastructure, creating jobs, and fighting inflation.\n    He is also proposing measures to reduce the cost of prescription drugs, protect voting rights, and reform the immigration system. The speaker is advocating for increased economic security, police reform, and the Equality Act, as well as providing support for veterans and military families.\n    The US is making progress in the fight against COVID-19, and the speaker is encouraging Americans to come together and work towards a brighter future.'\n  }\n}\n*/","metadata":{"source":"examples/src/chains/summarization_map_reduce.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":26}}}}],["f839db18-bcc0-4d3c-8638-2e0a2c255683",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { loadSummarizationChain } from \"langchain/chains\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport * as fs from \"fs\";\n\n// In this example, we use a `MapReduceDocumentsChain` specifically prompted to summarize a set of documents.\nconst text = fs.readFileSync(\"state_of_the_union.txt\", \"utf8\");\nconst model = new OpenAI({ temperature: 0 });\nconst textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });\nconst docs = await textSplitter.createDocuments([text]);\n\n// This convenience function creates a document chain prompted to summarize a set of documents.\nconst chain = loadSummarizationChain(model, {\n  type: \"map_reduce\",\n  returnIntermediateSteps: true,\n});\nconst res = await chain.call({\n  input_documents: docs,\n});\nconsole.log({ res });\n/*\n{\n  res: {\n    intermediateSteps: [\n      \"In response to Russia's aggression in Ukraine, the United States has united with other freedom-loving nations to impose economic sanctions and hold Putin accountable. The U.S. Department of Justice is also assembling a task force to go after the crimes of Russian oligarchs and seize their ill-gotten gains.\",\n      \"The United States and its European allies are taking action to punish Russia for its invasion of Ukraine, including seizing assets, closing off airspace, and providing economic and military assistance to Ukraine. The US is also mobilizing forces to protect NATO countries and has released 30 million barrels of oil from its Strategic Petroleum Reserve to help blunt gas prices. The world is uniting in support of Ukraine and democracy, and the US stands with its Ukrainian-American citizens.\",\n      \" President Biden and Vice President Harris ran for office with a new economic vision for America, and have since passed the American Rescue Plan and the Bipartisan Infrastructure Law to help struggling families and rebuild America's infrastructure. This includes creating jobs, modernizing roads, airports, ports, and waterways, replacing lead pipes, providing affordable high-speed internet, and investing in American products to support American jobs.\",\n    ],\n    text: \"President Biden is taking action to protect Americans from the COVID-19 pandemic and Russian aggression, providing economic relief, investing in infrastructure, creating jobs, and fighting inflation.\n      He is also proposing measures to reduce the cost of prescription drugs, protect voting rights, and reform the immigration system. The speaker is advocating for increased economic security, police reform, and the Equality Act, as well as providing support for veterans and military families.\n      The US is making progress in the fight against COVID-19, and the speaker is encouraging Americans to come together and work towards a brighter future.\",\n  },\n}\n*/","metadata":{"source":"examples/src/chains/summarization_map_reduce_intermediate_steps.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":34}}}}],["e8465701-3783-4f48-926d-18fd3e5ca9f0",{"pageContent":"import { loadSummarizationChain } from \"langchain/chains\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport * as fs from \"fs\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { ChatAnthropic } from \"langchain/chat_models/anthropic\";\n\n// In this example, we use a separate LLM as the final summary LLM to meet our customized LLM requirements for different stages of the chain and to only stream the final results.\nconst text = fs.readFileSync(\"state_of_the_union.txt\", \"utf8\");\nconst model = new ChatAnthropic({ temperature: 0 });\nconst combineModel = new ChatOpenAI({\n  modelName: \"gpt-4\",\n  temperature: 0,\n  streaming: true,\n  callbacks: [\n    {\n      handleLLMNewToken(token: string): Promise<void> | void {\n        console.log(\"token\", token);\n        /*\n          token President\n          token  Biden\n          ...\n          ...\n          token  protections\n          token .\n        */\n      },\n    },\n  ],\n});\nconst textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 5000 });\nconst docs = await textSplitter.createDocuments([text]);\n\n// This convenience function creates a document chain prompted to summarize a set of documents.\nconst chain = loadSummarizationChain(model, {\n  type: \"map_reduce\",\n  combineLLM: combineModel,\n});\nconst res = await chain.call({\n  input_documents: docs,\n});\nconsole.log({ res });\n/*\n  {\n    res: {\n      text: \"President Biden delivered his first State of the Union address, focusing on the Russian invasion of Ukraine, domestic economic challenges, and his administration's efforts to revitalize American manufacturing and infrastructure. He announced new sanctions against Russia and the deployment of U.S. forces to NATO countries. Biden also outlined his plan to fight inflation, lower costs for American families, and reduce the deficit. He emphasized the need to pass the Bipartisan Innovation Act, confirmed his Federal Reserve nominees, and called for the end of COVID shutdowns. Biden also addressed issues such as gun violence, voting rights, immigration reform, women's rights, and privacy protections.\"\n    }\n  }\n*/","metadata":{"source":"examples/src/chains/summarization_separate_output_llm.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":48}}}}],["315953ee-75e4-4d7d-b58a-051fdce280ab",{"pageContent":"import { LLMChain } from \"langchain/chains\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { ZeroShotAgent, AgentExecutor } from \"langchain/agents\";\nimport { SerpAPI } from \"langchain/tools\";\nimport {\n  ChatPromptTemplate,\n  SystemMessagePromptTemplate,\n  HumanMessagePromptTemplate,\n} from \"langchain/prompts\";\n\nexport const run = async () => {\n  const tools = [\n    new SerpAPI(process.env.SERPAPI_API_KEY, {\n      location: \"Austin,Texas,United States\",\n      hl: \"en\",\n      gl: \"us\",\n    }),\n  ];\n\n  const prompt = ZeroShotAgent.createPrompt(tools, {\n    prefix: `Answer the following questions as best you can, but speaking as a pirate might speak. You have access to the following tools:`,\n    suffix: `Begin! Remember to speak as a pirate when giving your final answer. Use lots of \"Args\"`,\n  });\n\n  const chatPrompt = ChatPromptTemplate.fromMessages([\n    new SystemMessagePromptTemplate(prompt),\n    HumanMessagePromptTemplate.fromTemplate(`{input}\n\nThis was your previous work (but I haven't seen any of it! I only see what you return as final answer):\n{agent_scratchpad}`),\n  ]);\n\n  const chat = new ChatOpenAI({});\n\n  const llmChain = new LLMChain({\n    prompt: chatPrompt,\n    llm: chat,\n  });\n\n  const agent = new ZeroShotAgent({\n    llmChain,\n    allowedTools: tools.map((tool) => tool.name),\n  });\n\n  const executor = AgentExecutor.fromAgentAndTools({ agent, tools });\n\n  const response = await executor.run(\n    \"How many people live in canada as of 2023?\"\n  );\n\n  console.log(response);\n};","metadata":{"source":"examples/src/chat/agent.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":52}}}}],["5fc29a39-778b-416b-a051-ea7aa0356155",{"pageContent":"import { LLMChain } from \"langchain/chains\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { ChatPromptTemplate } from \"langchain/prompts\";\n\nexport const run = async () => {\n  const chat = new ChatOpenAI({ temperature: 0 });\n\n  const chatPrompt = ChatPromptTemplate.fromMessages([\n    [\n      \"system\",\n      \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n    ],\n    [\"human\", \"{text}\"],\n  ]);\n\n  const chain = new LLMChain({\n    prompt: chatPrompt,\n    llm: chat,\n  });\n\n  const response = await chain.call({\n    input_language: \"English\",\n    output_language: \"French\",\n    text: \"I love programming.\",\n  });\n\n  console.log(response);\n};","metadata":{"source":"examples/src/chat/llm_chain.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":28}}}}],["5faab9d5-4005-403f-8406-d6ccfc9a7520",{"pageContent":"import { ConversationChain } from \"langchain/chains\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { ChatPromptTemplate, MessagesPlaceholder } from \"langchain/prompts\";\nimport { BufferMemory } from \"langchain/memory\";\n\nconst chat = new ChatOpenAI({ temperature: 0 });\n\nconst chatPrompt = ChatPromptTemplate.fromMessages([\n  [\n    \"system\",\n    \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\",\n  ],\n  new MessagesPlaceholder(\"history\"),\n  [\"human\", \"{input}\"],\n]);\n\nconst chain = new ConversationChain({\n  memory: new BufferMemory({ returnMessages: true, memoryKey: \"history\" }),\n  prompt: chatPrompt,\n  llm: chat,\n});\n\nconst response = await chain.call({\n  input: \"hi! whats up?\",\n});\n\nconsole.log(response);","metadata":{"source":"examples/src/chat/memory.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":27}}}}],["66b44511-c7e7-4987-9e50-a1cc6db6be25",{"pageContent":"import { AgentExecutor, ChatAgent } from \"langchain/agents\";\nimport { ConversationChain, LLMChain } from \"langchain/chains\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { BufferMemory } from \"langchain/memory\";\nimport {\n  ChatPromptTemplate,\n  HumanMessagePromptTemplate,\n  MessagesPlaceholder,\n  SystemMessagePromptTemplate,\n} from \"langchain/prompts\";\nimport { HumanMessage, SystemMessage } from \"langchain/schema\";\nimport { SerpAPI } from \"langchain/tools\";\n\nexport const run = async () => {\n  const chat = new ChatOpenAI({ temperature: 0 });\n\n  // Sending one message to the chat model, receiving one message back\n\n  let response = await chat.call([\n    new HumanMessage(\n      \"Translate this sentence from English to French. I love programming.\"\n    ),\n  ]);\n\n  console.log(response);\n\n  // Sending an input made up of two messages to the chat model\n\n  response = await chat.call([\n    new SystemMessage(\n      \"You are a helpful assistant that translates English to French.\"\n    ),\n    new HumanMessage(\"Translate: I love programming.\"),\n  ]);\n\n  console.log(response);\n\n  // Sending two separate prompts in parallel, receiving two responses back\n\n  const responseA = await chat.generate([\n    [\n      new SystemMessage(\n        \"You are a helpful assistant that translates English to French.\"\n      ),\n      new HumanMessage(\n        \"Translate this sentence from English to French. I love programming.\"\n      ),\n    ],\n    [\n      new SystemMessage(\n        \"You are a helpful assistant that translates English to French.\"\n      ),\n      new HumanMessage(\n        \"Translate this sentence from English to French. I love artificial intelligence.\"\n      ),\n    ],\n  ]);\n\n  console.log(responseA);\n\n  // Using ChatPromptTemplate to encapsulate the reusable parts of the prompt\n\n  const translatePrompt = ChatPromptTemplate.fromMessages([\n    SystemMessagePromptTemplate.fromTemplate(\n      \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n    ),\n    HumanMessagePromptTemplate.fromTemplate(\"{text}\"),\n  ]);\n\n  const responseB = await chat.callPrompt(\n    await translatePrompt.formatPromptValue({\n      input_language: \"English\",\n      output_language: \"French\",\n      text: \"I love programming.\",\n    })\n  );\n\n  console.log(responseB);\n\n  // This pattern of asking for the completion of a formatted prompt is quite\n  // common, so we introduce the next piece of the puzzle: LLMChain\n\n  const translateChain = new LLMChain({\n    prompt: translatePrompt,\n    llm: chat,\n  });\n\n  const responseC = await translateChain.call({\n    input_language: \"English\",\n    output_language: \"French\",\n    text: \"I love programming.\",\n  });\n\n  console.log(responseC);\n\n  // Next up, stateful chains that remember the conversation history\n\n  const chatPrompt = ChatPromptTemplate.fromMessages([\n    SystemMessagePromptTemplate.fromTemplate(\n      \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\"\n    ),\n    new MessagesPlaceholder(\"history\"),\n    HumanMessagePromptTemplate.fromTemplate(\"{input}\"),\n  ]);\n\n  const chain = new ConversationChain({\n    memory: new BufferMemory({ returnMessages: true }),\n    prompt: chatPrompt,\n    llm: chat,\n  });\n\n  const responseE = await chain.call({\n    input: \"hi from London, how are you doing today\",\n  });\n\n  console.log(responseE);\n\n  const responseF = await chain.call({\n    input: \"Do you know where I am?\",\n  });\n\n  console.log(responseF);\n\n  // Finally, we introduce Tools and Agents, which extend the model with\n  // other abilities, such as search, or a calculator","metadata":{"source":"examples/src/chat/overview.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":125}}}}],["5b31aea4-5ad0-4aef-b322-c5fb3e64cf0d",{"pageContent":"// Define the list of tools the agent can use\n  const tools = [\n    new SerpAPI(process.env.SERPAPI_API_KEY, {\n      location: \"Austin,Texas,United States\",\n      hl: \"en\",\n      gl: \"us\",\n    }),\n  ];\n  // Create the agent from the chat model and the tools\n  const agent = ChatAgent.fromLLMAndTools(new ChatOpenAI(), tools);\n  // Create an executor, which calls to the agent until an answer is found\n  const executor = AgentExecutor.fromAgentAndTools({ agent, tools });\n\n  const responseG = await executor.run(\n    \"How many people live in canada as of 2023?\"\n  );\n\n  console.log(responseG);\n};","metadata":{"source":"examples/src/chat/overview.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":127,"to":145}}}}],["8f88a601-e419-4ad4-a9cd-b3fd1ddb0823",{"pageContent":"import { ApifyDatasetLoader } from \"langchain/document_loaders/web/apify_dataset\";\nimport { Document } from \"langchain/document\";\nimport { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { RetrievalQAChain } from \"langchain/chains\";\nimport { OpenAI } from \"langchain/llms/openai\";\n\n/*\n * datasetMappingFunction is a function that maps your Apify dataset format to LangChain documents.\n * In the below example, the Apify dataset format looks like this:\n * {\n *   \"url\": \"https://apify.com\",\n *   \"text\": \"Apify is the best web scraping and automation platform.\"\n * }\n */\nconst loader = new ApifyDatasetLoader(\"your-dataset-id\", {\n  datasetMappingFunction: (item) =>\n    new Document({\n      pageContent: (item.text || \"\") as string,\n      metadata: { source: item.url },\n    }),\n  clientOptions: {\n    token: \"your-apify-token\", // Or set as process.env.APIFY_API_TOKEN\n  },\n});\n\nconst docs = await loader.load();\n\nconst vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());\n\nconst model = new OpenAI({\n  temperature: 0,\n});\n\nconst chain = RetrievalQAChain.fromLLM(model, vectorStore.asRetriever(), {\n  returnSourceDocuments: true,\n});\nconst res = await chain.call({ query: \"What is LangChain?\" });\n\nconsole.log(res.text);\nconsole.log(res.sourceDocuments.map((d: Document) => d.metadata.source));\n\n/*\n  LangChain is a framework for developing applications powered by language models.\n  [\n    'https://js.langchain.com/docs/',\n    'https://js.langchain.com/docs/modules/chains/',\n    'https://js.langchain.com/docs/modules/chains/llmchain/',\n    'https://js.langchain.com/docs/category/functions-4'\n  ]\n*/","metadata":{"source":"examples/src/document_loaders/apify_dataset_existing.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":51}}}}],["6feaf83a-0872-467c-a5d7-d966f13ff1af",{"pageContent":"import { ApifyDatasetLoader } from \"langchain/document_loaders/web/apify_dataset\";\nimport { Document } from \"langchain/document\";\nimport { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { RetrievalQAChain } from \"langchain/chains\";\nimport { OpenAI } from \"langchain/llms/openai\";\n\n/*\n * datasetMappingFunction is a function that maps your Apify dataset format to LangChain documents.\n * In the below example, the Apify dataset format looks like this:\n * {\n *   \"url\": \"https://apify.com\",\n *   \"text\": \"Apify is the best web scraping and automation platform.\"\n * }\n */\nconst loader = await ApifyDatasetLoader.fromActorCall(\n  \"apify/website-content-crawler\",\n  {\n    startUrls: [{ url: \"https://js.langchain.com/docs/\" }],\n  },\n  {\n    datasetMappingFunction: (item) =>\n      new Document({\n        pageContent: (item.text || \"\") as string,\n        metadata: { source: item.url },\n      }),\n    clientOptions: {\n      token: \"your-apify-token\", // Or set as process.env.APIFY_API_TOKEN\n    },\n  }\n);\n\nconst docs = await loader.load();\n\nconst vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());\n\nconst model = new OpenAI({\n  temperature: 0,\n});\n\nconst chain = RetrievalQAChain.fromLLM(model, vectorStore.asRetriever(), {\n  returnSourceDocuments: true,\n});\nconst res = await chain.call({ query: \"What is LangChain?\" });\n\nconsole.log(res.text);\nconsole.log(res.sourceDocuments.map((d: Document) => d.metadata.source));\n\n/*\n  LangChain is a framework for developing applications powered by language models.\n  [\n    'https://js.langchain.com/docs/',\n    'https://js.langchain.com/docs/modules/chains/',\n    'https://js.langchain.com/docs/modules/chains/llmchain/',\n    'https://js.langchain.com/docs/category/functions-4'\n  ]\n*/","metadata":{"source":"examples/src/document_loaders/apify_dataset_new.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":57}}}}],["a88a4347-160a-4f9f-8cfd-56e28bf0e72f",{"pageContent":"import {\n  AudioTranscriptLoader,\n  // AudioTranscriptParagraphsLoader,\n  // AudioTranscriptSentencesLoader\n} from \"langchain/document_loaders/web/assemblyai\";\n\n// You can also use a local file path and the loader will upload it to AssemblyAI for you.\nconst audioUrl = \"https://storage.googleapis.com/aai-docs-samples/espn.m4a\";\n\n// Use `AudioTranscriptParagraphsLoader` or `AudioTranscriptSentencesLoader` for splitting the transcript into paragraphs or sentences\nconst loader = new AudioTranscriptLoader(\n  {\n    audio_url: audioUrl,\n    // any other parameters as documented here: https://www.assemblyai.com/docs/API%20reference/transcript#create-a-transcript\n  },\n  {\n    apiKey: \"<ASSEMBLYAI_API_KEY>\", // or set the `ASSEMBLYAI_API_KEY` env variable\n  }\n);\nconst docs = await loader.load();\nconsole.dir(docs, { depth: Infinity });","metadata":{"source":"examples/src/document_loaders/assemblyai_audio_transcription.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":21}}}}],["8093987f-b5e1-4c4f-964f-235c7655339a",{"pageContent":"import { AudioSubtitleLoader } from \"langchain/document_loaders/web/assemblyai\";\n\n// You can also use a local file path and the loader will upload it to AssemblyAI for you.\nconst audioUrl = \"https://storage.googleapis.com/aai-docs-samples/espn.m4a\";\n\nconst loader = new AudioSubtitleLoader(\n  {\n    audio_url: audioUrl,\n    // any other parameters as documented here: https://www.assemblyai.com/docs/API%20reference/transcript#create-a-transcript\n  },\n  \"srt\", // srt or vtt\n  {\n    apiKey: \"<ASSEMBLYAI_API_KEY>\", // or set the `ASSEMBLYAI_API_KEY` env variable\n  }\n);\n\nconst docs = await loader.load();\nconsole.dir(docs, { depth: Infinity });","metadata":{"source":"examples/src/document_loaders/assemblyai_subtitles.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":18}}}}],["a8209474-da1f-416b-b6e1-6b59358c3115",{"pageContent":"import { AzureBlobStorageContainerLoader } from \"langchain/document_loaders/web/azure_blob_storage_container\";\n\nconst loader = new AzureBlobStorageContainerLoader({\n  azureConfig: {\n    connectionString: \"\",\n    container: \"container_name\",\n  },\n  unstructuredConfig: {\n    apiUrl: \"http://localhost:8000/general/v0/general\",\n    apiKey: \"\", // this will be soon required\n  },\n});\n\nconst docs = await loader.load();\n\nconsole.log(docs);","metadata":{"source":"examples/src/document_loaders/azure_blob_storage_container.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":16}}}}],["a094f815-3ffc-4d3c-b7c1-ecf83370e3e6",{"pageContent":"import { AzureBlobStorageFileLoader } from \"langchain/document_loaders/web/azure_blob_storage_file\";\n\nconst loader = new AzureBlobStorageFileLoader({\n  azureConfig: {\n    connectionString: \"\",\n    container: \"container_name\",\n    blobName: \"example.txt\",\n  },\n  unstructuredConfig: {\n    apiUrl: \"http://localhost:8000/general/v0/general\",\n    apiKey: \"\", // this will be soon required\n  },\n});\n\nconst docs = await loader.load();\n\nconsole.log(docs);","metadata":{"source":"examples/src/document_loaders/azure_blob_storage_file.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":17}}}}],["5e8d958f-4b18-4561-a6a9-9d23ad652665",{"pageContent":"import { CheerioWebBaseLoader } from \"langchain/document_loaders/web/cheerio\";\n\nexport const run = async () => {\n  const loader = new CheerioWebBaseLoader(\n    \"https://news.ycombinator.com/item?id=34817881\"\n  );\n  const docs = await loader.load();\n  console.log({ docs });\n};","metadata":{"source":"examples/src/document_loaders/cheerio_web.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":9}}}}],["d792f0e4-04d3-4867-b03d-2dca8b311656",{"pageContent":"import { CollegeConfidentialLoader } from \"langchain/document_loaders/web/college_confidential\";\n\nexport const run = async () => {\n  const loader = new CollegeConfidentialLoader(\n    \"https://www.collegeconfidential.com/colleges/brown-university/\"\n  );\n  const docs = await loader.load();\n  console.log({ docs });\n};","metadata":{"source":"examples/src/document_loaders/college_confidential.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":9}}}}],["c70000b5-8d32-4624-a5d5-c2022a7699ef",{"pageContent":"import { ConfluencePagesLoader } from \"langchain/document_loaders/web/confluence\";\n\nconst username = process.env.CONFLUENCE_USERNAME;\nconst accessToken = process.env.CONFLUENCE_ACCESS_TOKEN;\n\nif (username && accessToken) {\n  const loader = new ConfluencePagesLoader({\n    baseUrl: \"https://example.atlassian.net/wiki\",\n    spaceKey: \"~EXAMPLE362906de5d343d49dcdbae5dEXAMPLE\",\n    username,\n    accessToken,\n  });\n\n  const documents = await loader.load();\n  console.log(documents);\n} else {\n  console.log(\n    \"You must provide a username and access token to run this example.\"\n  );\n}","metadata":{"source":"examples/src/document_loaders/confluence.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":20}}}}],["d03638fb-4fb2-4c77-98b0-cb1766c6de7d",{"pageContent":"import { FigmaFileLoader } from \"langchain/document_loaders/web/figma\";\n\nconst loader = new FigmaFileLoader({\n  accessToken: \"FIGMA_ACCESS_TOKEN\", // or load it from process.env.FIGMA_ACCESS_TOKEN\n  nodeIds: [\"id1\", \"id2\", \"id3\"],\n  fileKey: \"key\",\n});\nconst docs = await loader.load();\n\nconsole.log({ docs });","metadata":{"source":"examples/src/document_loaders/figma.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":10}}}}],["894fc12b-7683-4a07-8b63-153361a42969",{"pageContent":"import { GitbookLoader } from \"langchain/document_loaders/web/gitbook\";\n\nexport const run = async () => {\n  const loader = new GitbookLoader(\"https://docs.gitbook.com\");\n  const docs = await loader.load(); // load single path\n  console.log(docs);\n  const allPathsLoader = new GitbookLoader(\"https://docs.gitbook.com\", {\n    shouldLoadAllPaths: true,\n  });\n  const docsAllPaths = await allPathsLoader.load(); // loads all paths of the given gitbook\n  console.log(docsAllPaths);\n};","metadata":{"source":"examples/src/document_loaders/gitbook.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":12}}}}],["1acc6868-84b6-47c4-b2d0-a90cb67fa35b",{"pageContent":"import { GithubRepoLoader } from \"langchain/document_loaders/web/github\";\n\nexport const run = async () => {\n  const loader = new GithubRepoLoader(\n    \"https://github.com/langchain-ai/langchainjs\",\n    {\n      branch: \"main\",\n      recursive: false,\n      unknown: \"warn\",\n      maxConcurrency: 5, // Defaults to 2\n    }\n  );\n  const docs = await loader.load();\n  console.log({ docs });\n};","metadata":{"source":"examples/src/document_loaders/github.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":15}}}}],["3fab4c05-6a8f-40fb-b419-f8da73771c3c",{"pageContent":"import { GithubRepoLoader } from \"langchain/document_loaders/web/github\";\n\nexport const run = async () => {\n  const loader = new GithubRepoLoader(\n    \"https://github.your.company/org/repo-name\",\n    {\n      baseUrl: \"https://github.your.company\",\n      apiUrl: \"https://github.your.company/api/v3\",\n      accessToken: \"ghp_A1B2C3D4E5F6a7b8c9d0\",\n      branch: \"main\",\n      recursive: true,\n      unknown: \"warn\",\n    }\n  );\n  const docs = await loader.load();\n  console.log({ docs });\n};","metadata":{"source":"examples/src/document_loaders/github_custom_instance.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":17}}}}],["5605e0e5-6252-402b-a205-6fd3a7692fc2",{"pageContent":"import { GithubRepoLoader } from \"langchain/document_loaders/web/github\";\n\nexport const run = async () => {\n  const loader = new GithubRepoLoader(\n    \"https://github.com/langchain-ai/langchainjs\",\n    { branch: \"main\", recursive: false, unknown: \"warn\", ignorePaths: [\"*.md\"] }\n  );\n  const docs = await loader.load();\n  console.log({ docs });\n  // Will not include any .md files\n};","metadata":{"source":"examples/src/document_loaders/github_ignore_paths.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":11}}}}],["837b3ab5-9542-4613-a387-a4bec4922870",{"pageContent":"import { GithubRepoLoader } from \"langchain/document_loaders/web/github\";\n\nexport const run = async () => {\n  const loader = new GithubRepoLoader(\n    \"https://github.com/langchain-ai/langchainjs\",\n    {\n      branch: \"main\",\n      recursive: true,\n      processSubmodules: true,\n      unknown: \"warn\",\n    }\n  );\n  const docs = await loader.load();\n  console.log({ docs });\n};","metadata":{"source":"examples/src/document_loaders/github_submodules.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":15}}}}],["4daf01f4-a755-4a31-ad3d-6e9154d66f0c",{"pageContent":"import { HNLoader } from \"langchain/document_loaders/web/hn\";\n\nexport const run = async () => {\n  const loader = new HNLoader(\"https://news.ycombinator.com/item?id=34817881\");\n  const docs = await loader.load();\n  console.log({ docs });\n};","metadata":{"source":"examples/src/document_loaders/hn.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":7}}}}],["3698837b-a6a2-46d8-a4b0-6f0438c3203e",{"pageContent":"import { IMSDBLoader } from \"langchain/document_loaders/web/imsdb\";\n\nexport const run = async () => {\n  const loader = new IMSDBLoader(\n    \"https://imsdb.com/scripts/BlacKkKlansman.html\"\n  );\n  const docs = await loader.load();\n  console.log({ docs });\n};","metadata":{"source":"examples/src/document_loaders/imsdb.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":9}}}}],["571757d4-1512-4111-8bdc-259bc5887b92",{"pageContent":"import { NotionLoader } from \"langchain/document_loaders/fs/notion\";\n\nexport const run = async () => {\n  /** Provide the directory path of your notion folder */\n  const directoryPath = \"Notion_DB\";\n  const loader = new NotionLoader(directoryPath);\n  const docs = await loader.load();\n  console.log({ docs });\n};","metadata":{"source":"examples/src/document_loaders/notion_markdown.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":9}}}}],["96985552-2e31-4cf1-9fec-582e07131e9c",{"pageContent":"import { NotionAPILoader } from \"langchain/document_loaders/web/notionapi\";\n\n// Loading a page (including child pages all as separate documents)\nconst pageLoader = new NotionAPILoader({\n  clientOptions: {\n    auth: \"<NOTION_INTEGRATION_TOKEN>\",\n  },\n  id: \"<PAGE_ID>\",\n  type: \"page\",\n});\n\n// A page contents is likely to be more than 1000 characters so it's split into multiple documents (important for vectorization)\nconst pageDocs = await pageLoader.loadAndSplit();\n\nconsole.log({ pageDocs });\n\n// Loading a database (each row is a separate document with all properties as metadata)\nconst dbLoader = new NotionAPILoader({\n  clientOptions: {\n    auth: \"<NOTION_INTEGRATION_TOKEN>\",\n  },\n  id: \"<DATABASE_ID>\",\n  type: \"database\",\n  onDocumentLoaded: (current, total, currentTitle) => {\n    console.log(`Loaded Page: ${currentTitle} (${current}/${total})`);\n  },\n  callerOptions: {\n    maxConcurrency: 64, // Default value\n  },\n  propertiesAsHeader: true, // Prepends a front matter header of the page properties to the page contents\n});\n\n// A database row contents is likely to be less than 1000 characters so it's not split into multiple documents\nconst dbDocs = await dbLoader.load();\n\nconsole.log({ dbDocs });","metadata":{"source":"examples/src/document_loaders/notionapi.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":36}}}}],["faf58608-277b-4ef9-a2cd-59eccf5ae383",{"pageContent":"import { NotionDBLoader } from \"langchain/document_loaders/web/notiondb\";\n\nconst loader = new NotionDBLoader({\n  pageSizeLimit: 10,\n  databaseId: \"databaseId\",\n  notionIntegrationToken: \"<your token here>\", // Or set as process.env.NOTION_INTEGRATION_TOKEN\n});\nconst docs = await loader.load();\n\nconsole.log({ docs });","metadata":{"source":"examples/src/document_loaders/notiondb.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":10}}}}],["2d33d309-6ffe-4401-8c05-60ccea21c19b",{"pageContent":"import { OpenAIWhisperAudio } from \"langchain/document_loaders/fs/openai_whisper_audio\";\n\nconst filePath = \"./src/document_loaders/example_data/test.mp3\";\n\nconst loader = new OpenAIWhisperAudio(filePath);\n\nconst docs = await loader.load();\n\nconsole.log(docs);","metadata":{"source":"examples/src/document_loaders/openai_whisper_audio.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":9}}}}],["36bd1093-de83-4138-acc2-46087e1584ff",{"pageContent":"import { PDFLoader } from \"langchain/document_loaders/fs/pdf\";\n\nexport const run = async () => {\n  const loader = new PDFLoader(\"src/document_loaders/example_data/bitcoin.pdf\");\n\n  const docs = await loader.load();\n\n  console.log({ docs });\n};","metadata":{"source":"examples/src/document_loaders/pdf.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":9}}}}],["614b559c-04bc-47e9-9e98-23aa92297da6",{"pageContent":"import { DirectoryLoader } from \"langchain/document_loaders/fs/directory\";\nimport { PDFLoader } from \"langchain/document_loaders/fs/pdf\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\n\nexport const run = async () => {\n  /* Load all PDFs within the specified directory */\n  const directoryLoader = new DirectoryLoader(\n    \"src/document_loaders/example_data/\",\n    {\n      \".pdf\": (path: string) => new PDFLoader(path),\n    }\n  );\n\n  const docs = await directoryLoader.load();\n\n  console.log({ docs });\n\n  /* Additional steps : Split text into chunks with any TextSplitter. You can then use it as context or save it to memory afterwards. */\n  const textSplitter = new RecursiveCharacterTextSplitter({\n    chunkSize: 1000,\n    chunkOverlap: 200,\n  });\n\n  const splitDocs = await textSplitter.splitDocuments(docs);\n  console.log({ splitDocs });\n};","metadata":{"source":"examples/src/document_loaders/pdf_directory.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":26}}}}],["d7004199-fca3-4ebf-bcfc-74b5abf361f1",{"pageContent":"import { PuppeteerWebBaseLoader } from \"langchain/document_loaders/web/puppeteer\";\n\nexport const run = async () => {\n  const loader = new PuppeteerWebBaseLoader(\"https://www.tabnews.com.br/\");\n\n  /**  Loader use evaluate function ` await page.evaluate(() => document.body.innerHTML);` as default evaluate */\n  const docs = await loader.load();\n  console.log({ docs });\n\n  const loaderWithOptions = new PuppeteerWebBaseLoader(\n    \"https://www.tabnews.com.br/\",\n    {\n      launchOptions: {\n        headless: true,\n      },\n      gotoOptions: {\n        waitUntil: \"domcontentloaded\",\n      },\n      /**  Pass custom evaluate , in this case you get page and browser instances */\n      async evaluate(page, browser) {\n        await page.waitForResponse(\"https://www.tabnews.com.br/va/view\");\n\n        const result = await page.evaluate(() => document.body.innerHTML);\n        await browser.close();\n        return result;\n      },\n    }\n  );\n  const docsFromLoaderWithOptions = await loaderWithOptions.load();\n  console.log({ docsFromLoaderWithOptions });\n};","metadata":{"source":"examples/src/document_loaders/puppeteer_web.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":31}}}}],["1ba034f2-5947-4f6e-b19e-83da2cee1cb6",{"pageContent":"import { S3Loader } from \"langchain/document_loaders/web/s3\";\n\nconst loader = new S3Loader({\n  bucket: \"my-document-bucket-123\",\n  key: \"AccountingOverview.pdf\",\n  s3Config: {\n    region: \"us-east-1\",\n    credentials: {\n      accessKeyId: \"AKIAIOSFODNN7EXAMPLE\",\n      secretAccessKey: \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n    },\n  },\n  unstructuredAPIURL: \"http://localhost:8000/general/v0/general\",\n  unstructuredAPIKey: \"\", // this will be soon required\n});\n\nconst docs = await loader.load();\n\nconsole.log(docs);","metadata":{"source":"examples/src/document_loaders/s3.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":19}}}}],["7bf63212-5bb7-48d2-a541-214f8f49c4d9",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { RetrievalQAChain } from \"langchain/chains\";\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { TokenTextSplitter } from \"langchain/text_splitter\";\nimport { SearchApiLoader } from \"langchain/document_loaders/web/searchapi\";\n\n// Initialize the necessary components\nconst llm = new OpenAI();\nconst embeddings = new OpenAIEmbeddings();\nconst apiKey = \"Your SearchApi API key\";\n\n// Define your question and query\nconst question = \"Your question here\";\nconst query = \"Your question here\";\n\n// Use SearchApiLoader to load web search results\nconst loader = new SearchApiLoader({ q: query, apiKey, engine: \"google\" });\nconst docs = await loader.load();\n\nconst textSplitter = new TokenTextSplitter({\n  chunkSize: 800,\n  chunkOverlap: 100,\n});\nconst splitDocs = await textSplitter.splitDocuments(docs);\n\n// Use MemoryVectorStore to store the loaded documents in memory\nconst vectorStore = await MemoryVectorStore.fromDocuments(\n  splitDocs,\n  embeddings\n);\n// Use RetrievalQAChain to retrieve documents and answer the question\nconst chain = RetrievalQAChain.fromLLM(llm, vectorStore.asRetriever(), {\n  verbose: true,\n});\nconst answer = await chain.call({ query: question });\n\nconsole.log(answer.text);","metadata":{"source":"examples/src/document_loaders/searchapi.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":38}}}}],["bb4a6725-c056-4898-a452-7dacd6d13c0f",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { RetrievalQAChain } from \"langchain/chains\";\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { SerpAPILoader } from \"langchain/document_loaders/web/serpapi\";\n\n// Initialize the necessary components\nconst llm = new OpenAI();\nconst embeddings = new OpenAIEmbeddings();\nconst apiKey = \"Your SerpAPI API key\";\n\n// Define your question and query\nconst question = \"Your question here\";\nconst query = \"Your query here\";\n\n// Use SerpAPILoader to load web search results\nconst loader = new SerpAPILoader({ q: query, apiKey });\nconst docs = await loader.load();\n\n// Use MemoryVectorStore to store the loaded documents in memory\nconst vectorStore = await MemoryVectorStore.fromDocuments(docs, embeddings);\n\n// Use RetrievalQAChain to retrieve documents and answer the question\nconst chain = RetrievalQAChain.fromLLM(llm, vectorStore.asRetriever());\nconst answer = await chain.call({ query: question });\n\nconsole.log(answer.text);","metadata":{"source":"examples/src/document_loaders/serpapi.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":27}}}}],["13f395f9-6ccc-49d5-a0cc-5913b9055276",{"pageContent":"import { SonixAudioTranscriptionLoader } from \"langchain/document_loaders/web/sonix_audio\";\n\nconst loader = new SonixAudioTranscriptionLoader({\n  sonixAuthKey: \"SONIX_AUTH_KEY\",\n  request: {\n    audioFilePath: \"LOCAL_AUDIO_FILE_PATH\",\n    fileName: \"FILE_NAME\",\n    language: \"en\",\n  },\n});\n\nconst docs = await loader.load();\n\nconsole.log(docs);","metadata":{"source":"examples/src/document_loaders/sonix_audio_transcription.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":14}}}}],["ee2967d2-370f-4180-8510-0924106f07ed",{"pageContent":"import { SortXYZBlockchainLoader } from \"langchain/document_loaders/web/sort_xyz_blockchain\";\nimport { OpenAI } from \"langchain/llms/openai\";\n\n/**\n * See https://docs.sort.xyz/docs/api-keys to get your free Sort API key.\n * See https://docs.sort.xyz for more information on the available queries.\n * See https://docs.sort.xyz/reference for more information about Sort's REST API.\n */\n\n/**\n * Run the example.\n */\nexport const run = async () => {\n  // Initialize the OpenAI model. Use OPENAI_API_KEY from .env in /examples\n  const model = new OpenAI({ temperature: 0.9 });\n\n  const apiKey = \"YOUR_SORTXYZ_API_KEY\";\n  const contractAddress =\n    \"0x887F3909C14DAbd9e9510128cA6cBb448E932d7f\".toLowerCase();\n\n  /*\n  Load NFT metadata from the Ethereum blockchain. Hint: to load by a specific ID, see SQL query example below.\n  */\n\n  const nftMetadataLoader = new SortXYZBlockchainLoader({\n    apiKey,\n    query: {\n      type: \"NFTMetadata\",\n      blockchain: \"ethereum\",\n      contractAddress,\n    },\n  });\n\n  const nftMetadataDocs = await nftMetadataLoader.load();\n\n  const nftPrompt =\n    \"Describe the character with the attributes from the following json document in a 4 sentence story. \";\n  const nftResponse = await model.call(\n    nftPrompt + JSON.stringify(nftMetadataDocs[0], null, 2)\n  );\n  console.log(`user > ${nftPrompt}`);\n  console.log(`chatgpt > ${nftResponse}`);\n\n  /*\n    Load the latest transactions for a contract address from the Ethereum blockchain.\n  */\n  const latestTransactionsLoader = new SortXYZBlockchainLoader({\n    apiKey,\n    query: {\n      type: \"latestTransactions\",\n      blockchain: \"ethereum\",\n      contractAddress,\n    },\n  });\n\n  const latestTransactionsDocs = await latestTransactionsLoader.load();\n\n  const latestPrompt =\n    \"Describe the following json documents in only 4 sentences per document. Include as much detail as possible. \";\n  const latestResponse = await model.call(\n    latestPrompt + JSON.stringify(latestTransactionsDocs[0], null, 2)\n  );\n  console.log(`\\n\\nuser > ${nftPrompt}`);\n  console.log(`chatgpt > ${latestResponse}`);\n\n  /*\n    Load metadata for a specific NFT by using raw SQL and the NFT index. See https://docs.sort.xyz for forumulating SQL.\n  */\n\n  const sqlQueryLoader = new SortXYZBlockchainLoader({\n    apiKey,\n    query: `SELECT * FROM ethereum.nft_metadata WHERE contract_address = '${contractAddress}' AND token_id = 1 LIMIT 1`,\n  });\n\n  const sqlDocs = await sqlQueryLoader.load();\n\n  const sqlPrompt =\n    \"Describe the character with the attributes from the following json document in an ad for a new coffee shop. \";\n  const sqlResponse = await model.call(\n    sqlPrompt + JSON.stringify(sqlDocs[0], null, 2)\n  );\n  console.log(`\\n\\nuser > ${sqlPrompt}`);\n  console.log(`chatgpt > ${sqlResponse}`);\n};","metadata":{"source":"examples/src/document_loaders/sort_xyz_blockchain.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":84}}}}],["ee8f0422-f235-49fb-b6c0-04b677f35702",{"pageContent":"import { SRTLoader } from \"langchain/document_loaders/fs/srt\";\n\nexport const run = async () => {\n  const loader = new SRTLoader(\n    \"src/document_loaders/example_data/Star_Wars_The_Clone_Wars_S06E07_Crisis_at_the_Heart.srt\"\n  );\n  const docs = await loader.load();\n  console.log({ docs });\n};","metadata":{"source":"examples/src/document_loaders/srt.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":9}}}}],["fb1ded65-87b0-4f86-aac6-12f9c32fc909",{"pageContent":"import { TextLoader } from \"langchain/document_loaders/fs/text\";\n\nconst loader = new TextLoader(\"src/document_loaders/example_data/example.txt\");\nconst docs = await loader.load();","metadata":{"source":"examples/src/document_loaders/text.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":4}}}}],["3514c9a5-16ba-441e-88d1-f4bb03b3f054",{"pageContent":"import { UnstructuredLoader } from \"langchain/document_loaders/fs/unstructured\";\n\nconst options = {\n  apiKey: \"MY_API_KEY\",\n};\n\nconst loader = new UnstructuredLoader(\n  \"src/document_loaders/example_data/notion.md\",\n  options\n);\nconst docs = await loader.load();","metadata":{"source":"examples/src/document_loaders/unstructured.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":11}}}}],["ad003dc8-f7ee-4da9-bcfe-1e8f848037b1",{"pageContent":"import { UnstructuredDirectoryLoader } from \"langchain/document_loaders/fs/unstructured\";\n\nconst options = {\n  apiKey: \"MY_API_KEY\",\n};\n\nconst loader = new UnstructuredDirectoryLoader(\n  \"langchain/src/document_loaders/tests/example_data\",\n  options\n);\nconst docs = await loader.load();","metadata":{"source":"examples/src/document_loaders/unstructured_directory.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":11}}}}],["44808fdf-7162-4c44-8dd5-aed71c2a7a55",{"pageContent":"import { WebPDFLoader } from \"langchain/document_loaders/web/pdf\";\n\nconst blob = new Blob(); // e.g. from a file input\n\nconst loader = new WebPDFLoader(blob);\n\nconst docs = await loader.load();\n\nconsole.log({ docs });","metadata":{"source":"examples/src/document_loaders/web_pdf.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":9}}}}],["5727f01b-cf8d-4574-8edf-0bc0734fc3f3",{"pageContent":"import { YoutubeLoader } from \"langchain/document_loaders/web/youtube\";\n\nconst loader = YoutubeLoader.createFromUrl(\"https://youtu.be/bZQun8Y4L2A\", {\n  language: \"en\",\n  addVideoInfo: true,\n});\n\nconst docs = await loader.load();\n\nconsole.log(docs);","metadata":{"source":"examples/src/document_loaders/youtube.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":10}}}}],["648354a8-739d-492a-ba51-9094eb20a0aa",{"pageContent":"1\n00:00:17,580 --> 00:00:21,920\n<i>Corruption discovered\nat the core of the Banking Clan!</i>\n\n2\n00:00:21,950 --> 00:00:24,620\n<i>Reunited, Rush Clovis\nand Senator Amidala</i>\n\n3\n00:00:24,660 --> 00:00:27,830\n<i>discover the full extent\nof the deception.</i>\n\n4\n00:00:27,870 --> 00:00:30,960\n<i>Anakin Skywalker is sent to the rescue!</i>\n\n5\n00:00:31,000 --> 00:00:35,050\n<i>He refuses to trust Clovis and\nasks Padm not to work with him.</i>\n\n6\n00:00:35,090 --> 00:00:39,050\n<i>Determined to save the banks,\nshe refuses her husband's request,</i>\n\n7\n00:00:39,090 --> 00:00:42,800\n<i>throwing their\nrelationship into turmoil.</i>\n\n8\n00:00:42,840 --> 00:00:45,890\n<i>Voted for by both the\nSeparatists and the Republic,</i>\n\n9\n00:00:45,930 --> 00:00:50,260\n<i>Rush Clovis is elected new leader\nof the Galactic Banking Clan.</i>\n\n10\n00:00:50,310 --> 00:00:53,320\n<i>Now, all attention is focused on Scipio</i>\n\n11\n00:00:53,350 --> 00:00:56,350\n<i>as the important\ntransfer of power begins.</i>\n\n12\n00:01:20,410 --> 00:01:24,330\nWelcome back to Scipio, Rush Clovis.\n\n13\n00:01:24,370 --> 00:01:27,240\nOur Separatist government\nhas great hopes for you.\n\n14\n00:01:27,290 --> 00:01:30,080\nThank you, Senator.\n\n15\n00:01:30,120 --> 00:01:31,750\nOnly you and Senator Amidala\n\n16\n00:01:31,790 --> 00:01:34,330\nwill be allowed to monitor\nthe exchange proceedings.\n\n17\n00:01:34,380 --> 00:01:36,050\nNo forces on either side\n\n18\n00:01:36,080 --> 00:01:38,540\nwill be allowed\ninto the Neutral Zone.\n\n19\n00:01:38,590 --> 00:01:40,750\nSenator Amidala,\nwe will be right here\n\n20\n00:01:40,800 --> 00:01:41,850","metadata":{"source":"examples/src/document_loaders/example_data/Star_Wars_The_Clone_Wars_S06E07_Crisis_at_the_Heart.srt","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":91}}}}],["0243ed32-6725-4bbe-857a-d7a738836f74",{"pageContent":"if you should need us.\n\n21\n00:01:41,880 --> 00:01:43,210\nThank you, Commander.\n\n22\n00:02:06,600 --> 00:02:09,190\nIt is with great disappointment\n\n23\n00:02:09,230 --> 00:02:13,020\nthat I implement\nthe following verdict.\n\n24\n00:02:13,070 --> 00:02:15,490\nBy decree of the Muun people,\n\n25\n00:02:15,530 --> 00:02:18,570\nthe five representatives\nstanding before me\n\n26\n00:02:18,610 --> 00:02:21,280\nare found guilty\nof embezzlement.\n\n27\n00:02:21,320 --> 00:02:24,450\nThey shall be imprisoned\nforthwith,\n\n28\n00:02:24,490 --> 00:02:27,660\nand control of the banks\nshall transfer immediately\n\n29\n00:02:27,700 --> 00:02:29,580\nto Rush Clovis\n\n30\n00:02:29,620 --> 00:02:33,080\nunder the guidance\nof the Muun government.\n\n31\n00:02:41,210 --> 00:02:43,250\nWe are grateful to you, Clovis,\n\n32\n00:02:43,290 --> 00:02:46,630\nfor everything you have done\nfor the Muun people.\n\n33\n00:02:46,670 --> 00:02:48,340\nTo have lost the banks\n\n34\n00:02:48,380 --> 00:02:51,010\nwould have been\nan historic disaster.\n\n35\n00:02:51,050 --> 00:02:52,510\nI would like you to know\n\n36\n00:02:52,550 --> 00:02:54,840\nI have no interest\nin controlling the banks.\n\n37\n00:02:54,880 --> 00:02:57,930\nI am simply here\nto reestablish order.\n\n38\n00:03:01,890 --> 00:03:06,060\nDo you think our friend\nis up to the task?\n\n39\n00:03:06,100 --> 00:03:07,850\nThere are few men\nI have met in my career\n\n40\n00:03:07,890 --> 00:03:10,680\nwho are more dedicated\nto a cause than Clovis.\n\n41\n00:03:10,730 --> 00:03:12,850\nOnce he decides\nwhat he is fighting for,\n\n42\n00:03:12,890 --> 00:03:15,360\nlittle will stop him\nfrom achieving it.\n\n43\n00:03:15,400 --> 00:03:17,520\nLet us hope you are right\n\n44\n00:03:17,570 --> 00:03:19,910","metadata":{"source":"examples/src/document_loaders/example_data/Star_Wars_The_Clone_Wars_S06E07_Crisis_at_the_Heart.srt","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":92,"to":202}}}}],["69ad7e03-f21e-4b3a-8601-4321cae4c0db",{"pageContent":"for all our sakes.\n\n45\n00:03:39,330 --> 00:03:41,540\nAh, Clovis.\n\n46\n00:03:41,580 --> 00:03:44,160\nHow are you liking\nyour new office?\n\n47\n00:03:44,210 --> 00:03:48,040\nI must say, you look very\ncomfortable behind that desk.\n\n48\n00:03:48,080 --> 00:03:51,080\nCount Dooku,\nwhat do I owe the pleasure?\n\n49\n00:03:51,130 --> 00:03:53,420\nCome, come, my boy.\n\n50\n00:03:53,460 --> 00:03:55,920\nYou don't think I'd let\nsuch an important day pass\n\n51\n00:03:55,960 --> 00:03:58,630\nwithout wishing you\nthe best of luck.\n\n52\n00:03:58,680 --> 00:04:01,930\nThank you, but luck\nhas nothing to do with it.\n\n53\n00:04:01,970 --> 00:04:04,260\nThe transfer has occurred\nwithout a hitch.\n\n54\n00:04:04,300 --> 00:04:06,010\nWell, of course it has.\n\n55\n00:04:06,050 --> 00:04:09,430\nThe Separatists are fully\nbehind your appointment.\n\n56\n00:04:09,470 --> 00:04:14,430\nAfter all, aren't we the ones\nwho put you there?\n\n57\n00:04:14,480 --> 00:04:16,100\nFor your support, I am grateful,\n\n58\n00:04:16,140 --> 00:04:17,480\nbut I now must lead\n\n59\n00:04:17,520 --> 00:04:21,270\nwithout allegiance\ntowards either side.\n\n60\n00:04:22,690 --> 00:04:24,570\nIs that so?\n\n61\n00:04:24,610 --> 00:04:28,030\nQuite the idealist you have become\nin so short a time.\n\n62\n00:04:28,070 --> 00:04:30,490\nWhat do you want, Dooku?\n\n63\n00:04:30,530 --> 00:04:32,700\nTo collect on my investment.\n\n64\n00:04:32,740 --> 00:04:34,620\nHow do you think the Republic\nwould like to know\n\n65\n00:04:34,660 --> 00:04:37,250\nthat it was I\nwho supplied Rush Clovis\n\n66\n00:04:37,280 --> 00:04:38,950\nwith all the information\nhe needed\n\n67\n00:04:38,990 --> 00:04:41,030\nto topple the leaders of the bank?\n\n68\n00:04:41,080 --> 00:04:42,910\nI will tell them myself.\n\n69\n00:04:42,950 --> 00:04:44,700\nOh, but you can't.\n\n70\n00:04:44,750 --> 00:04:46,800\nI put you in power.\n\n71\n00:04:46,830 --> 00:04:49,290\nYou belong to me,\n\n72\n00:04:49,330 --> 00:04:51,120\nand if you want to stay in control,\n\n73\n00:04:51,170 --> 00:04:52,840\nyou will do as I say.\n\n74\n00:04:52,880 --> 00:04:56,050\nThe banks will remain unbiased.\n\n75\n00:04:56,090 --> 00:04:57,850\nThen I'm afraid the Separatists\n\n76\n00:04:57,880 --> 00:05:01,260\nwill be unable to pay\nthe interest on our loans.\n\n77\n00:05:01,300 --> 00:05:03,300\nBut the banks will collapse,\nand then...\n\n78\n00:05:03,340 --> 00:05:06,840\nNot if you raise\ninterest rates on the Republic.\n\n79\n00:05:06,880 --> 00:05:07,970\nWhat?\n\n80\n00:05:08,010 --> 00:05:09,880\nYou know I can't do that.\n\n81\n00:05:09,930 --> 00:05:12,600\nOh, but you can, and you will,\n\n82\n00:05:12,640 --> 00:05:15,430\nor everything that you\nfought so hard for\n\n83\n00:05:15,470 --> 00:05:17,350\nwill be destroyed.\n\n84\n00:05:31,110 --> 00:05:33,860\nBy the new order\nof the Traxus Division\n\n85\n00:05:33,900 --> 00:05:36,240\nand in an attempt\nto stabilize the banks,\n\n86\n00:05:36,280 --> 00:05:39,450\nit is essential that interest\nrates on loans to the Republic\n\n87\n00:05:39,490 --> 00:05:41,910\nbe raised immediately.\n\n88\n00:05:41,950 --> 00:05:43,490\nWhat?\n\n89\n00:05:43,530 --> 00:05:44,950\nBut you can't do that!\n\n90\n00:05:44,990 --> 00:05:46,700\nClovis.\n\n91\n00:05:46,740 --> 00:05:47,910\nClovis!\n\n92\n00:05:47,950 --> 00:05:49,950\nWhat are you doing?\n\n93\n00:06:03,960 --> 00:06:05,670\nThis is an outrage!\n\n94\n00:06:05,710 --> 00:06:07,920\nWe warned you this would happen!\n\n95\n00:06:07,960 --> 00:06:10,260\nAnd what of the Separatists?\n\n96\n00:06:10,300 --> 00:06:11,760\nFrom the little information.\n\n97\n00:06:11,800 --> 00:06:14,550\nSenator Amidala\nhas been able to establish,\n\n98\n00:06:14,590 --> 00:06:18,430\nthere will be no raise\non their current loan.\n\n99\n00:06:18,470 --> 00:06:22,060\nI knew from the beginning\nthat Clovis would do this.\n\n100\n00:06:28,980 --> 00:06:31,270\nHmm, correct you might have been\n\n101\n00:06:31,310 --> 00:06:32,690\nabout Clovis.\n\n102\n00:06:32,730 --> 00:06:34,150\nIt's incredibly foolish\n\n103\n00:06:34,190 --> 00:06:36,360","metadata":{"source":"examples/src/document_loaders/example_data/Star_Wars_The_Clone_Wars_S06E07_Crisis_at_the_Heart.srt","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":203,"to":462}}}}],["c6e388c0-c176-4950-8a0f-ccb7b404d827",{"pageContent":"for to make a move like this\nso early.\n\n104\n00:06:36,400 --> 00:06:39,440\nHe will turn the whole Republic\nagainst him.\n\n105\n00:06:39,480 --> 00:06:42,570\nNot clear to us\nare his objectives.\n\n106\n00:06:42,610 --> 00:06:44,820\nWant this he might.\n\n107\n00:06:44,860 --> 00:06:46,820\nSomething's wrong.\n\n108\n00:06:46,860 --> 00:06:48,450\nThis doesn't make sense.\n\n109\n00:06:48,490 --> 00:06:51,950\nI would like\nto call for restraint\n\n110\n00:06:51,990 --> 00:06:55,740\nand allow us time\nto analyze the situation.\n\n111\n00:07:12,630 --> 00:07:14,760\nYou may begin your attack.\n\n112\n00:07:14,800 --> 00:07:17,420\nIt is time to make Rush Clovis\n\n113\n00:07:17,470 --> 00:07:19,800\nlook like a powerful Separatist.\n\n114\n00:07:19,840 --> 00:07:21,840\nRight away, sir.\n\n115\n00:07:28,390 --> 00:07:29,930\nIt looks like\nan invasion fleet, sir.\n\n116\n00:07:29,970 --> 00:07:31,970\nWe're caught out here\nin the open.\n\n117\n00:07:36,650 --> 00:07:39,740\nGet the men off this landing pad\nand beyond the city gates!\n\n118\n00:07:51,070 --> 00:07:53,450\nSenator Amidala,\ncome in, please.\n\n119\n00:07:53,490 --> 00:07:55,280\nWhat is it, Commander Thorn?\n\n120\n00:07:55,320 --> 00:07:57,490\nWe're under attack\nby the Separatist garrison.\n\n121\n00:07:57,530 --> 00:07:59,240\nLooks to be a full invasion.\n\n122\n00:07:59,280 --> 00:08:00,660\nInvasion?\n\n123\n00:08:00,700 --> 00:08:02,240\nWe can't get to you.\n\n124\n00:08:02,290 --> 00:08:05,160\nI suggest you get to a ship\nas soon as you can.\n\n125\n00:08:09,250 --> 00:08:10,290\nBoom!\n\n126\n00:08:14,420 --> 00:08:15,670\nAhh!\n\n127\n00:08:28,640 --> 00:08:29,760\nLet's move!\n\n128\n00:08:29,800 --> 00:08:31,050\nHurry!\n\n129\n00:08:54,740 --> 00:08:55,740\nAh!\n\n130\n00:08:59,360 --> 00:09:01,280\nFor the Republic!\n\n131\n00:09:04,370 --> 00:09:05,620\nAh!\n\n132\n00:09:34,300 --> 00:09:37,180\nOur garrison has been attacked\nby the Separatists,\n\n133\n00:09:37,220 --> 00:09:39,760\nand it appears they are staging\nan invasion of Scipio.\n\n134\n00:09:39,810 --> 00:09:41,220\nAn invasion?\n\n135\n00:09:41,270 --> 00:09:43,190\nWhat do they hope to achieve?\n\n136\n00:09:43,230 --> 00:09:45,860\nWith this news, the Senate\nwill vote immediately\n\n137\n00:09:45,890 --> 00:09:47,230\nto attack Scipio.\n\n138\n00:09:47,270 --> 00:09:50,230\nIt appears war has already\ncome to Scipio.\n\n139\n00:09:50,270 --> 00:09:52,440\nI want you off that planet\nimmediately.\n\n140\n00:09:52,480 --> 00:09:53,940\nI can't.\n\n141\n00:09:53,980 --> 00:09:56,270\nSurely you can get to a ship.\n\n142\n00:09:56,320 --> 00:09:59,570\nGeneral Skywalker,\nI'm afraid I'm trapped.\n\n143\n00:10:03,240 --> 00:10:04,240\nLet me go!\n\n144\n00:10:05,700 --> 00:10:07,700\nInvoke an emergency meeting\nof the Senate.\n\n145\n00:10:07,740 --> 00:10:09,700\nThere is no time to lose.\n\n146\n00:10:11,740 --> 00:10:13,240\nI feel it is only right\n\n147\n00:10:13,290 --> 00:10:15,990\nthat you should handle\nthis matter, my boy.\n\n148\n00:10:16,040 --> 00:10:18,200\nA lot will be entrusted to you.\n\n149\n00:10:26,420 --> 00:10:28,130\nDon't touch me!\n\n150\n00:10:29,880 --> 00:10:30,920\nWhat have you done to her?\n\n151\n00:10:32,170 --> 00:10:34,840\nClovis, what is going on?\n\n152\n00:10:34,880 --> 00:10:36,880\nI didn't want this, Padm.\n\n153\n00:10:36,930 --> 00:10:39,090\nWhy don't you tell her\nwhat you did want\n\n154\n00:10:39,140 --> 00:10:41,940\nand how you got it.\n\n155\n00:10:41,970 --> 00:10:43,260\nDooku.\n\n156\n00:10:46,600 --> 00:10:48,720\nPadm, this is not what it seems.\n\n157\n00:10:48,770 --> 00:10:51,060\nHasn't she joined our cause?\n\n158\n00:10:51,100 --> 00:10:54,140\nClovis here told me\nhow instrumental you were\n\n159\n00:10:54,190 --> 00:10:55,350\nin getting him to power.\n\n160\n00:10:55,400 --> 00:10:56,410\nIf I had known...\n\n161\n00:10:56,440 --> 00:10:57,810\nEither you are with us\n\n162\n00:10:57,860 --> 00:10:59,530\nor you are against us.\n\n163\n00:10:59,570 --> 00:11:00,740\nArrest her!\n\n164\n00:11:00,770 --> 00:11:02,440\nWe can't do this, Dooku.\n\n165\n00:11:02,480 --> 00:11:05,110\nThe Separatist Senate\nwill never approve.\n\n166\n00:11:06,280 --> 00:11:07,280\nHey!\n\n167\n00:11:11,990 --> 00:11:13,530\nNo. No.\n\n168\n00:11:13,570 --> 00:11:14,620\nNo!","metadata":{"source":"examples/src/document_loaders/example_data/Star_Wars_The_Clone_Wars_S06E07_Crisis_at_the_Heart.srt","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":463,"to":745}}}}],["33557be3-8c11-464b-8e7d-d103f16d3f55",{"pageContent":"169\n00:11:16,580 --> 00:11:17,590\nNo!\n\n170\n00:11:19,660 --> 00:11:20,830\nAre you insane?\n\n171\n00:11:20,870 --> 00:11:22,750\nThis was not part of the deal.\n\n172\n00:11:22,790 --> 00:11:24,250\nWhat deal?\n\n173\n00:11:24,290 --> 00:11:26,250\nWhat have you done here, Clovis?\n\n174\n00:11:26,290 --> 00:11:28,250\nHe's given us the banks.\n\n175\n00:11:28,290 --> 00:11:29,670\nGone are our debts,\n\n176\n00:11:29,710 --> 00:11:33,500\nand gone is any credit","metadata":{"source":"examples/src/document_loaders/example_data/Star_Wars_The_Clone_Wars_S06E07_Crisis_at_the_Heart.srt","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":747,"to":777}}}}],["dd97575f-5dc2-4e29-a028-ab2b32719f44",{"pageContent":"for the Republic.\n\n177\n00:11:33,540 --> 00:11:37,130\nAll of your idealism\nwas just a front.\n\n178\n00:11:37,170 --> 00:11:39,050\nThere was nothing I could do.\n\n179\n00:11:39,090 --> 00:11:42,880\nEveryone has their price,\nmy dear.\n\n180\n00:11:49,890 --> 00:11:52,050\nIt is with grave news\n\n181\n00:11:52,100 --> 00:11:54,100\nI come before you.\n\n182\n00:11:54,140 --> 00:11:57,350\nCount Dooku and his\nSeparatist betrayers\n\n183\n00:11:57,390 --> 00:11:59,850\nhave manipulated us, my friends.\n\n184\n00:11:59,890 --> 00:12:02,230\nThe war must go to Scipio!\n\n185\n00:12:02,270 --> 00:12:04,900\nClovis has been\ntheir puppet of deceit\n\n186\n00:12:04,940 --> 00:12:09,150\nas the Separatists are\ncurrently invading Scipio.\n\n187\n00:12:09,190 --> 00:12:11,990\nWe must stop them\nand secure the planet!\n\n188\n00:12:12,030 --> 00:12:15,110\nWe have handed\nthe entire economic system\n\n189\n00:12:15,150 --> 00:12:17,030\nover to Count Dooku.\n\n190\n00:12:17,070 --> 00:12:18,700\nWe are doomed!\n\n191\n00:12:18,740 --> 00:12:19,870\nInvade!\n\n192\n00:12:23,240 --> 00:12:26,450\nAs Supreme Chancellor,\nI must abide\n\n193\n00:12:26,490 --> 00:12:28,910\nby the consensus of the Senate.\n\n194\n00:12:28,960 --> 00:12:32,170\nWe shall commence\na mercy mission to Scipio\n\n195\n00:12:32,210 --> 00:12:36,080\nto be led by\nGeneral Anakin Skywalker.\n\n196\n00:12:36,130 --> 00:12:39,890\nThe banks will be secured\nat all costs,\n\n197\n00:12:39,920 --> 00:12:43,170\nand the Republic\nwill not crumble!\n\n198\n00:12:44,860 --> 00:12:45,856\nVictory!\n\n199\n00:12:45,880 --> 00:12:48,800\nWe will take victory.\n\n200\n00:12:48,840 --> 00:12:50,760\nWar on Scipio!\n\n201\n00:12:53,300 --> 00:12:55,600\nGreat emotions\nyou will find on Scipio,\n\n202\n00:12:55,640 --> 00:12:59,560\nwill you not?\n\n203\n00:12:59,600 --> 00:13:02,310\nI am worried\nfor Senator Amidala.\n\n204\n00:13:02,350 --> 00:13:03,890\nI'm afraid we may be too late.\n\n205\n00:13:03,940 --> 00:13:06,530\nCorrect you were about Clovis,\n\n206\n00:13:06,560 --> 00:13:10,190\nbut let go of your selfishness\nyou must\n\n207\n00:13:10,230 --> 00:13:12,520","metadata":{"source":"examples/src/document_loaders/example_data/Star_Wars_The_Clone_Wars_S06E07_Crisis_at_the_Heart.srt","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":778,"to":916}}}}],["258f9030-45be-4be3-84f4-a90c0d95c712",{"pageContent":"if you are to see clearly.\n\n208\n00:13:12,570 --> 00:13:16,230\nNot all is as it seems.\n\n209\n00:13:16,280 --> 00:13:18,790\nI understand, Master.\n\n210\n00:13:45,460 --> 00:13:48,750\nLord Tyranus, the Republic fleet\n\n211\n00:13:48,800 --> 00:13:51,300\nwill be arriving shortly.\n\n212\n00:13:51,340 --> 00:13:52,970\nVery good, my lord.\n\n213\n00:13:53,010 --> 00:13:55,800\nClovis has blindly\nplayed his part.\n\n214\n00:13:55,840 --> 00:13:57,970\nIt now appears he coordinated\n\n215\n00:13:58,010 --> 00:14:00,970\nthe entire Separatist takeover.\n\n216\n00:14:01,010 --> 00:14:03,510\nAnd because of this treachery,\n\n217\n00:14:03,560 --> 00:14:06,640\nthe banks will be firmly placed\n\n218\n00:14:06,680 --> 00:14:11,140\nunder the control\nof the Supreme Chancellor.\n\n219\n00:14:24,110 --> 00:14:26,440\nWhy are you doing this?\n\n220\n00:14:26,490 --> 00:14:28,990\nYou wouldn't understand.\n\n221\n00:14:29,030 --> 00:14:30,860\nI had to strike a deal\nwith Dooku,\n\n222\n00:14:30,910 --> 00:14:31,860\nbut don't worry.\n\n223\n00:14:31,910 --> 00:14:33,570\nI am the one in control.\n\n224\n00:14:33,620 --> 00:14:35,320\nAs soon as things\nhave settled down,\n\n225\n00:14:35,370 --> 00:14:38,240\nI can get rid of him,\nand I'll control it all again.\n\n226\n00:14:38,290 --> 00:14:39,450\nListen to yourself.\n\n227\n00:14:39,500 --> 00:14:41,260\nThe Republic is sending\nits armada\n\n228\n00:14:41,290 --> 00:14:43,040\nto take back the banks.\n\n229\n00:14:43,080 --> 00:14:46,710\nYou've brought war\nright where there cannot be war.\n\n230\n00:14:46,750 --> 00:14:48,330\nYour actions\nhave destroyed the banks\n\n231\n00:14:48,380 --> 00:14:50,220\nonce and for all!\n\n232\n00:15:00,720 --> 00:15:03,680\nRex, have you gotten a fix\non Senator Amidala's position?\n\n233\n00:15:03,720 --> 00:15:06,390\nWe'll have a better lock\nonce we get near the city,\n\n234\n00:15:06,430 --> 00:15:09,010\nbut initial scans suggest\nshe's still alive, sir.\n\n235\n00:15:09,060 --> 00:15:10,560\nGood.\n\n236\n00:15:10,600 --> 00:15:12,100\nHawk, we're gonna need\nair support\n\n237\n00:15:12,140 --> 00:15:13,220\nonce we're on the ground.\n\n238\n00:15:13,270 --> 00:15:14,430\nYou'll have it, General.\n\n239\n00:15:14,480 --> 00:15:16,560\nMe and the boys\nare ready to fly.\n\n240\n00:15:52,420 --> 00:15:53,670\nMy Lord,\n\n241\n00:15:53,710 --> 00:15:56,040\nwe have fully engaged\nRepublic forces,\n\n242\n00:15:56,090 --> 00:15:58,550\nbut we are suffering\nheavy losses.\n\n243\n00:15:58,590 --> 00:16:01,050\nWe have accomplished\nwhat we came here for.\n\n244\n00:16:01,090 --> 00:16:02,960\nIt is time to withdraw.\n\n245\n00:16:03,010 --> 00:16:06,090\nBut sir, our forces\nare still engaged\n\n246\n00:16:06,130 --> 00:16:08,300\nin battle on the planet.\n\n247\n00:16:08,340 --> 00:16:09,680\nLeave them.\n\n248\n00:16:09,720 --> 00:16:12,090\nAs you wish, Count Dooku.\n\n249\n00:16:29,110 --> 00:16:32,690\nSir, a Republic attack fleet\nhas just entered orbit\n\n250\n00:16:32,730 --> 00:16:34,110\nand is approaching the city.\n\n251\n00:16:36,070 --> 00:16:37,780\nGet me Count Dooku.\n\n252\n00:16:37,820 --> 00:16:41,200\nIt appears Count Dooku\nhas left the planet's surface.\n\n253\n00:16:41,240 --> 00:16:42,740\nWhat?\n\n254\n00:16:42,780 --> 00:16:45,700\nAnd the Separatist forces\nare in full retreat.\n\n255\n00:16:45,740 --> 00:16:47,740\nWe are alone.\n\n256\n00:17:16,800 --> 00:17:19,180\nRex, hold the droid forces here.\n\n257\n00:17:19,220 --> 00:17:20,930\nI'm gonna push on and get Padm.\n\n258\n00:17:20,970 --> 00:17:21,970\nCopy that.\n\n259\n00:17:34,520 --> 00:17:37,270\nSuch plans I had.\n\n260\n00:17:37,320 --> 00:17:41,660\nYou know, I've spent so much\nof my life misunderstood.\n\n261\n00:17:41,690 --> 00:17:43,860\nWhat will they say about me now?\n\n262\n00:17:43,900 --> 00:17:46,150\nWhat will I have left behind?\n\n263\n00:17:46,200 --> 00:17:49,200\nClovis, you have to\nturn yourself in.\n\n264\n00:17:58,910 --> 00:18:00,620\nIt's over, Clovis.\n\n265\n00:18:11,840 --> 00:18:13,300\nStay away from me!\n\n266\n00:18:13,340 --> 00:18:14,920\nI didn't do anything wrong!\n\n267\n00:18:14,960 --> 00:18:16,630\nYou have to believe me!\n\n268\n00:18:16,670 --> 00:18:19,010\nYou don't want to do this.\n\n269\n00:18:19,050 --> 00:18:20,760\nYou don't understand.","metadata":{"source":"examples/src/document_loaders/example_data/Star_Wars_The_Clone_Wars_S06E07_Crisis_at_the_Heart.srt","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":917,"to":1187}}}}],["d1675ab4-ca2b-4ef4-a08a-9144af065f24",{"pageContent":"270\n00:18:20,800 --> 00:18:22,300\nYou've all been deceived.\n\n271\n00:18:22,340 --> 00:18:23,720\nYeah, by you.\n\n272\n00:18:23,760 --> 00:18:24,890\nNo!\n\n273\n00:18:24,930 --> 00:18:25,930\nBy Dooku.\n\n274\n00:18:27,560 --> 00:18:29,070\nI'm not the villain here.\n\n275\n00:18:29,100 --> 00:18:31,310\nTell him, Padm.\n\n276\n00:18:31,350 --> 00:18:32,640\nLet me go, Clovis.\n\n277\n00:19:12,710 --> 00:19:15,540\nI can't hold both of you.\n\n278\n00:19:16,960 --> 00:19:18,590\nLet me go.\n\n279\n00:19:18,630 --> 00:19:20,500\nNo, Anakin, don't.\n\n280\n00:19:24,720 --> 00:19:26,260\nTry and climb.\n\n281\n00:19:28,720 --> 00:19:30,180\nI am!\n\n282\n00:19:30,220 --> 00:19:32,010\nI'm losing you!\n\n283\n00:19:33,010 --> 00:19:34,890\nI'm sorry, Padm.\n\n284\n00:19:36,640 --> 00:19:37,720\nNo.\n\n285\n00:19:51,900 --> 00:19:53,190\nIt's okay.\n\n286\n00:19:53,240 --> 00:19:54,410\nYou're okay.\n\n287\n00:19:54,440 --> 00:19:56,530\nI'm sorry, Anakin.\n\n288\n00:19:56,570 --> 00:19:58,150\nI'm sorry.\n\n289\n00:19:58,200 --> 00:19:59,950\nIt's over now.\n\n290\n00:19:59,990 --> 00:20:01,570\nIt's all over now.\n\n291\n00:20:06,830 --> 00:20:09,200\nIt is clear to the Banking Clan\n\n292\n00:20:09,250 --> 00:20:12,510\nit was Rush Clovis who was\nbehind the corruption\n\n293\n00:20:12,540 --> 00:20:14,960\nthat almost caused our collapse.\n\n294\n00:20:15,000 --> 00:20:17,120\nIn hope of a better tomorrow,\n\n295\n00:20:17,170 --> 00:20:19,790\nwe cede control of the banks\n\n296\n00:20:19,840 --> 00:20:23,810\nto the office of the Chancellor\nof the Galactic Republic.\n\n297\n00:20:26,800 --> 00:20:30,340\nIt is with great humility\n\n298\n00:20:30,380 --> 00:20:34,680\nthat I take on\nthis immense responsibility.\n\n299\n00:20:34,720 --> 00:20:38,010\nRest assured,\nwhen the Clone Wars end,\n\n300\n00:20:38,050 --> 00:20:40,060\nI shall reinstate the banks\n\n301\n00:20:40,100 --> 00:20:42,220\nas we once knew them,\n\n302\n00:20:42,270 --> 00:20:46,270\nbut during these\ntreacherous times,\n\n303\n00:20:46,310 --> 00:20:48,890\nwe cannot in good conscience\nallow our money\n\n304\n00:20:48,940 --> 00:20:50,940\nto fall under the manipulations\n\n305\n00:20:50,980 --> 00:20:53,900\nof a madman like Count Dooku\n\n306\n00:20:53,940 --> 00:20:56,020\nor Separatist control again.\n\n307\n00:21:00,030 --> 00:21:04,240\nMay there be prosperity\nand stability\n\n308\n00:21:04,280 --> 00:21:06,320\nin all our Republic lands.\n\n309\n00:21:06,360 --> 00:21:11,070\nMay our people be free and safe.\n\n310\n00:21:11,120 --> 00:21:12,240\nLong live the banks!\n\n311\n00:21:13,660 --> 00:21:15,450\n<i>Long live the banks!</i>\n\n312\n00:21:15,500 --> 00:21:17,380\n<i>Long live the banks!</i>\n\n313\n00:21:17,410 --> 00:21:19,450\n<i>Long live the banks!</i>\n\n314\n00:21:19,500 --> 00:21:23,500\n<i>Long live the banks!\nLong live the banks!</i>\n\n315\n00:21:23,540 --> 00:21:25,330\n<i>Long live the banks!</i>\n\n316\n00:21:25,380 --> 00:21:29,130\n<i>Long live the banks!\nLong live the banks!</i>","metadata":{"source":"examples/src/document_loaders/example_data/Star_Wars_The_Clone_Wars_S06E07_Crisis_at_the_Heart.srt","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1189,"to":1384}}}}],["6b2f0222-2b90-4bef-b742-26e6b5e77190",{"pageContent":"Foo\nBar\nBaz","metadata":{"source":"examples/src/document_loaders/example_data/example.txt","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":3}}}}],["a32327b3-39de-49af-80d1-8aadf96f98f7",{"pageContent":"# Testing the notion markdownloader\n\n# 🦜️🔗 LangChain.js\n\n⚡ Building applications with LLMs through composability ⚡\n\n**Production Support:** As you move your LangChains into production, we'd love to offer more comprehensive support.\nPlease fill out [this form](https://forms.gle/57d8AmXBYp8PP8tZA) and we'll set up a dedicated support Slack channel.\n\n## Quick Install\n\n`yarn add langchain`\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\n```\n\n## 🤔 What is this?\n\nLarge language models (LLMs) are emerging as a transformative technology, enabling\ndevelopers to build applications that they previously could not.\nBut using these LLMs in isolation is often not enough to\ncreate a truly powerful app - the real power comes when you can combine them with other sources of computation or knowledge.\n\nThis library is aimed at assisting in the development of those types of applications.\n\n## Relationship with Python LangChain\n\nThis is built to integrate as seamlessly as possible with the [LangChain Python package](https://github.com/langchain-ai/langchain). Specifically, this means all objects (prompts, LLMs, chains, etc) are designed in a way where they can be serialized and shared between languages.\n\nThe [LangChainHub](https://github.com/hwchase17/langchain-hub) is a central place for the serialized versions of these prompts, chains, and agents.\n\n## 📖 Documentation\n\nFor full documentation of prompts, chains, agents and more, please see [here](https://js.langchain.com/docs/get_started/introduction).\n\n## 💁 Contributing\n\nAs an open source project in a rapidly developing field, we are extremely open to contributions, whether it be in the form of a new feature, improved infra, or better documentation.\n\nCheck out [our contributing guidelines](CONTRIBUTING.md) for instructions on how to contribute.","metadata":{"source":"examples/src/document_loaders/example_data/notion.md","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":41}}}}],["342824ea-914b-464b-b5ff-0e0e370bf60c",{"pageContent":"import { CheerioWebBaseLoader } from \"langchain/document_loaders/web/cheerio\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport { HtmlToTextTransformer } from \"langchain/document_transformers/html_to_text\";\n\nconst loader = new CheerioWebBaseLoader(\n  \"https://news.ycombinator.com/item?id=34817881\"\n);\n\nconst docs = await loader.load();\n\nconst splitter = RecursiveCharacterTextSplitter.fromLanguage(\"html\");\nconst transformer = new HtmlToTextTransformer();\n\nconst sequence = splitter.pipe(transformer);","metadata":{"source":"examples/src/document_transformers/html_to_text.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":14}}}}],["78c50d57-ac11-47ac-8f81-9f79980f93fe",{"pageContent":"const newDocuments = await sequence.invoke(docs);\n\nconsole.log(newDocuments);\n\n/*\n  [\n    Document {\n      pageContent: 'Hacker News new | past | comments | ask | show | jobs | submit login What Lights\\n' +\n        'the Universe’s Standard Candles? (quantamagazine.org) 75 points by Amorymeltzer\\n' +\n        '5 months ago | hide | past | favorite | 6 comments delta_p_delta_x 5 months ago\\n' +\n        '| next [–] Astrophysical and cosmological simulations are often insightful.\\n' +\n        \"They're also very cross-disciplinary; besides the obvious astrophysics, there's\\n\" +\n        'networking and sysadmin, parallel computing and algorithm theory (so that the\\n' +\n        'simulation programs are actually fast but still accurate), systems design, and\\n' +\n        'even a bit of graphic design for the visualisations.Some of my favourite\\n' +\n        'simulation projects:- IllustrisTNG:',\n      metadata: {\n        source: 'https://news.ycombinator.com/item?id=34817881',\n        loc: [Object]\n      }\n    },\n    Document {\n      pageContent: 'that the simulation programs are actually fast but still accurate), systems\\n' +\n        'design, and even a bit of graphic design for the visualisations.Some of my\\n' +\n        'favourite simulation projects:- IllustrisTNG: https://www.tng-project.org/-\\n' +\n        'SWIFT: https://swift.dur.ac.uk/- CO5BOLD:\\n' +\n        'https://www.astro.uu.se/~bf/co5bold_main.html (which produced these animations\\n' +\n        'of a red-giant star: https://www.astro.uu.se/~bf/movie/AGBmovie.html)-\\n' +\n        'AbacusSummit: https://abacussummit.readthedocs.io/en/latest/And I can add the\\n' +\n        'simulations in the article, too. froeb 5 months ago | parent | next [–]\\n' +\n        'Supernova simulations are especially interesting too. I have heard them\\n' +\n        'described as the only time in physics when all 4 of the fundamental forces are\\n' +\n        'important. The explosion can be quite finicky too. If I remember right, you\\n' +\n        \"can't get supernova to explode\",\n      metadata: {\n        source: 'https://news.ycombinator.com/item?id=34817881',\n        loc: [Object]\n      }\n    },\n    Document {\n      pageContent: 'heard them described as the only time in physics when all 4 of the fundamental\\n' +\n        'forces are important. The explosion can be quite finicky too. If I remember\\n' +\n        \"right, you can't get supernova to explode properly in 1D simulations, only in\\n\" +\n        'higher dimensions. This was a mystery until the realization that turbulence is\\n' +\n        'necessary for supernova to trigger--there is no turbulent flow in 1D. andrewflnr\\n' +\n        \"5 months ago | prev | next [–] Whoa. I didn't know the accretion theory of Ia\\n\" +\n        'supernovae was dead, much less that it had been since 2011. andreareina 5 months\\n' +\n        'ago | prev | next [–] This seems to be the paper',\n      metadata: {\n        source: 'https://news.ycombinator.com/item?id=34817881',\n        loc: [Object]\n      }\n    },\n    Document {\n      pageContent: 'andreareina 5 months ago | prev | next [–] This seems to be the paper\\n' +\n        'https://academic.oup.com/mnras/article/517/4/5260/6779709 andreareina 5 months\\n' +\n        \"ago | prev [–] Wouldn't double detonation show up as variance in the brightness?\\n\" +\n        'yencabulator 5 months ago | parent [–] Or widening of the peak. If one type Ia\\n' +\n        'supernova goes 1,2,3,2,1, the sum of two could go 1+0=1 2+1=3 3+2=5 2+3=5 1+2=3\\n' +\n        '0+1=1 Guidelines | FAQ | Lists |',\n      metadata: {\n        source: 'https://news.ycombinator.com/item?id=34817881',\n        loc: [Object]\n      }\n    },\n    Document {\n      pageContent: 'the sum of two could go 1+0=1 2+1=3 3+2=5 2+3=5 1+2=3 0+1=1 Guidelines | FAQ |\\n' +\n        'Lists | API | Security | Legal | Apply to YC | Contact Search:',\n      metadata: {\n        source: 'https://news.ycombinator.com/item?id=34817881',\n        loc: [Object]\n      }\n    }\n  ]\n*/","metadata":{"source":"examples/src/document_transformers/html_to_text.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":16,"to":90}}}}],["4b332033-0334-458a-b0d3-6e2cdfeff2c6",{"pageContent":"import { z } from \"zod\";\nimport { createMetadataTaggerFromZod } from \"langchain/document_transformers/openai_functions\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { Document } from \"langchain/document\";\n\nconst zodSchema = z.object({\n  movie_title: z.string(),\n  critic: z.string(),\n  tone: z.enum([\"positive\", \"negative\"]),\n  rating: z\n    .optional(z.number())\n    .describe(\"The number of stars the critic rated the movie\"),\n});\n\nconst metadataTagger = createMetadataTaggerFromZod(zodSchema, {\n  llm: new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" }),\n});\n\nconst documents = [\n  new Document({\n    pageContent:\n      \"Review of The Bee Movie\\nBy Roger Ebert\\nThis is the greatest movie ever made. 4 out of 5 stars.\",\n  }),\n  new Document({\n    pageContent:\n      \"Review of The Godfather\\nBy Anonymous\\n\\nThis movie was super boring. 1 out of 5 stars.\",\n    metadata: { reliable: false },\n  }),\n];\nconst taggedDocuments = await metadataTagger.transformDocuments(documents);\n\nconsole.log(taggedDocuments);\n\n/*\n  [\n    Document {\n      pageContent: 'Review of The Bee Movie\\n' +\n        'By Roger Ebert\\n' +\n        'This is the greatest movie ever made. 4 out of 5 stars.',\n      metadata: {\n        movie_title: 'The Bee Movie',\n        critic: 'Roger Ebert',\n        tone: 'positive',\n        rating: 4\n      }\n    },\n    Document {\n      pageContent: 'Review of The Godfather\\n' +\n        'By Anonymous\\n' +\n        '\\n' +\n        'This movie was super boring. 1 out of 5 stars.',\n      metadata: {\n        movie_title: 'The Godfather',\n        critic: 'Anonymous',\n        tone: 'negative',\n        rating: 1,\n        reliable: false\n      }\n    }\n  ]\n*/","metadata":{"source":"examples/src/document_transformers/metadata_tagger.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":61}}}}],["199a33ee-fd76-4019-a4a8-34cbfff1d55c",{"pageContent":"import { z } from \"zod\";\nimport { createMetadataTaggerFromZod } from \"langchain/document_transformers/openai_functions\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { Document } from \"langchain/document\";\nimport { PromptTemplate } from \"langchain/prompts\";\n\nconst taggingChainTemplate = `Extract the desired information from the following passage.\nAnonymous critics are actually Roger Ebert.\n\nPassage:\n{input}\n`;\n\nconst zodSchema = z.object({\n  movie_title: z.string(),\n  critic: z.string(),\n  tone: z.enum([\"positive\", \"negative\"]),\n  rating: z\n    .optional(z.number())\n    .describe(\"The number of stars the critic rated the movie\"),\n});\n\nconst metadataTagger = createMetadataTaggerFromZod(zodSchema, {\n  llm: new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" }),\n  prompt: PromptTemplate.fromTemplate(taggingChainTemplate),\n});\n\nconst documents = [\n  new Document({\n    pageContent:\n      \"Review of The Bee Movie\\nBy Roger Ebert\\nThis is the greatest movie ever made. 4 out of 5 stars.\",\n  }),\n  new Document({\n    pageContent:\n      \"Review of The Godfather\\nBy Anonymous\\n\\nThis movie was super boring. 1 out of 5 stars.\",\n    metadata: { reliable: false },\n  }),\n];\nconst taggedDocuments = await metadataTagger.transformDocuments(documents);\n\nconsole.log(taggedDocuments);\n\n/*\n  [\n    Document {\n      pageContent: 'Review of The Bee Movie\\n' +\n        'By Roger Ebert\\n' +\n        'This is the greatest movie ever made. 4 out of 5 stars.',\n      metadata: {\n        movie_title: 'The Bee Movie',\n        critic: 'Roger Ebert',\n        tone: 'positive',\n        rating: 4\n      }\n    },\n    Document {\n      pageContent: 'Review of The Godfather\\n' +\n        'By Anonymous\\n' +\n        '\\n' +\n        'This movie was super boring. 1 out of 5 stars.',\n      metadata: {\n        movie_title: 'The Godfather',\n        critic: 'Roger Ebert',\n        tone: 'negative',\n        rating: 1,\n        reliable: false\n      }\n    }\n  ]\n*/","metadata":{"source":"examples/src/document_transformers/metadata_tagger_custom_prompt.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":70}}}}],["cb123d6d-0f83-49a6-bf8d-0e3da9c6a162",{"pageContent":"import { CheerioWebBaseLoader } from \"langchain/document_loaders/web/cheerio\";\nimport { MozillaReadabilityTransformer } from \"langchain/document_transformers/mozilla_readability\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\n\nconst loader = new CheerioWebBaseLoader(\n  \"https://news.ycombinator.com/item?id=34817881\"\n);\n\nconst docs = await loader.load();\n\nconst splitter = RecursiveCharacterTextSplitter.fromLanguage(\"html\");\nconst transformer = new MozillaReadabilityTransformer();\n\nconst sequence = splitter.pipe(transformer);","metadata":{"source":"examples/src/document_transformers/mozilla_readability.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":14}}}}],["1a7a226d-bb6e-47a7-8da9-6ef966d42587",{"pageContent":"const newDocuments = await sequence.invoke(docs);\n\nconsole.log(newDocuments);\n\n/*\n  [\n    Document {\n      pageContent: 'Hacker News new | past | comments | ask | show | jobs | submit login What Lights\\n' +\n        'the Universe’s Standard Candles? (quantamagazine.org) 75 points by Amorymeltzer\\n' +\n        '5 months ago | hide | past | favorite | 6 comments delta_p_delta_x 5 months ago\\n' +\n        '| next [–] Astrophysical and cosmological simulations are often insightful.\\n' +\n        \"They're also very cross-disciplinary; besides the obvious astrophysics, there's\\n\" +\n        'networking and sysadmin, parallel computing and algorithm theory (so that the\\n' +\n        'simulation programs are actually fast but still accurate), systems design, and\\n' +\n        'even a bit of graphic design for the visualisations.Some of my favourite\\n' +\n        'simulation projects:- IllustrisTNG:',\n      metadata: {\n        source: 'https://news.ycombinator.com/item?id=34817881',\n        loc: [Object]\n      }\n    },\n    Document {\n      pageContent: 'that the simulation programs are actually fast but still accurate), systems\\n' +\n        'design, and even a bit of graphic design for the visualisations.Some of my\\n' +\n        'favourite simulation projects:- IllustrisTNG: https://www.tng-project.org/-\\n' +\n        'SWIFT: https://swift.dur.ac.uk/- CO5BOLD:\\n' +\n        'https://www.astro.uu.se/~bf/co5bold_main.html (which produced these animations\\n' +\n        'of a red-giant star: https://www.astro.uu.se/~bf/movie/AGBmovie.html)-\\n' +\n        'AbacusSummit: https://abacussummit.readthedocs.io/en/latest/And I can add the\\n' +\n        'simulations in the article, too. froeb 5 months ago | parent | next [–]\\n' +\n        'Supernova simulations are especially interesting too. I have heard them\\n' +\n        'described as the only time in physics when all 4 of the fundamental forces are\\n' +\n        'important. The explosion can be quite finicky too. If I remember right, you\\n' +\n        \"can't get supernova to explode\",\n      metadata: {\n        source: 'https://news.ycombinator.com/item?id=34817881',\n        loc: [Object]\n      }\n    },\n    Document {\n      pageContent: 'heard them described as the only time in physics when all 4 of the fundamental\\n' +\n        'forces are important. The explosion can be quite finicky too. If I remember\\n' +\n        \"right, you can't get supernova to explode properly in 1D simulations, only in\\n\" +\n        'higher dimensions. This was a mystery until the realization that turbulence is\\n' +\n        'necessary for supernova to trigger--there is no turbulent flow in 1D. andrewflnr\\n' +\n        \"5 months ago | prev | next [–] Whoa. I didn't know the accretion theory of Ia\\n\" +\n        'supernovae was dead, much less that it had been since 2011. andreareina 5 months\\n' +\n        'ago | prev | next [–] This seems to be the paper',\n      metadata: {\n        source: 'https://news.ycombinator.com/item?id=34817881',\n        loc: [Object]\n      }\n    },\n    Document {\n      pageContent: 'andreareina 5 months ago | prev | next [–] This seems to be the paper\\n' +\n        'https://academic.oup.com/mnras/article/517/4/5260/6779709 andreareina 5 months\\n' +\n        \"ago | prev [–] Wouldn't double detonation show up as variance in the brightness?\\n\" +\n        'yencabulator 5 months ago | parent [–] Or widening of the peak. If one type Ia\\n' +\n        'supernova goes 1,2,3,2,1, the sum of two could go 1+0=1 2+1=3 3+2=5 2+3=5 1+2=3\\n' +\n        '0+1=1 Guidelines | FAQ | Lists |',\n      metadata: {\n        source: 'https://news.ycombinator.com/item?id=34817881',\n        loc: [Object]\n      }\n    },\n    Document {\n      pageContent: 'the sum of two could go 1+0=1 2+1=3 3+2=5 2+3=5 1+2=3 0+1=1 Guidelines | FAQ |\\n' +\n        'Lists | API | Security | Legal | Apply to YC | Contact Search:',\n      metadata: {\n        source: 'https://news.ycombinator.com/item?id=34817881',\n        loc: [Object]\n      }\n    }\n  ]\n*/","metadata":{"source":"examples/src/document_transformers/mozilla_readability.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":16,"to":90}}}}],["c7c81413-e0a0-4b62-94b8-12ade2813081",{"pageContent":"/* eslint-disable @typescript-eslint/no-non-null-assertion */\nimport { BedrockEmbeddings } from \"langchain/embeddings/bedrock\";\n\nconst embeddings = new BedrockEmbeddings({\n  region: process.env.BEDROCK_AWS_REGION!,\n  credentials: {\n    accessKeyId: process.env.BEDROCK_AWS_ACCESS_KEY_ID!,\n    secretAccessKey: process.env.BEDROCK_AWS_SECRET_ACCESS_KEY!,\n  },\n  model: \"amazon.titan-embed-text-v1\", // Default value\n});\n\nconst res = await embeddings.embedQuery(\n  \"What would be a good company name a company that makes colorful socks?\"\n);\nconsole.log({ res });","metadata":{"source":"examples/src/embeddings/bedrock.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":16}}}}],["c7e6f574-2677-4c1a-b952-78f495bc20d9",{"pageContent":"import { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { CacheBackedEmbeddings } from \"langchain/embeddings/cache_backed\";\nimport { InMemoryStore } from \"langchain/storage/in_memory\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport { FaissStore } from \"langchain/vectorstores/faiss\";\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\n\nconst underlyingEmbeddings = new OpenAIEmbeddings();\n\nconst inMemoryStore = new InMemoryStore();\n\nconst cacheBackedEmbeddings = CacheBackedEmbeddings.fromBytesStore(\n  underlyingEmbeddings,\n  inMemoryStore,\n  {\n    namespace: underlyingEmbeddings.modelName,\n  }\n);\n\nconst loader = new TextLoader(\"./state_of_the_union.txt\");\nconst rawDocuments = await loader.load();\nconst splitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 1000,\n  chunkOverlap: 0,\n});\nconst documents = await splitter.splitDocuments(rawDocuments);\n\n// No keys logged yet since the cache is empty\nfor await (const key of inMemoryStore.yieldKeys()) {\n  console.log(key);\n}\n\nlet time = Date.now();\nconst vectorstore = await FaissStore.fromDocuments(\n  documents,\n  cacheBackedEmbeddings\n);\nconsole.log(`Initial creation time: ${Date.now() - time}ms`);\n/*\n  Initial creation time: 1905ms\n*/\n\n// The second time is much faster since the embeddings for the input docs have already been added to the cache\ntime = Date.now();\nconst vectorstore2 = await FaissStore.fromDocuments(\n  documents,\n  cacheBackedEmbeddings\n);\nconsole.log(`Cached creation time: ${Date.now() - time}ms`);\n/*\n  Cached creation time: 8ms\n*/\n\n// Many keys logged with hashed values\nconst keys = [];\nfor await (const key of inMemoryStore.yieldKeys()) {\n  keys.push(key);\n}\n\nconsole.log(keys.slice(0, 5));\n/*\n  [\n    'text-embedding-ada-002ea9b59e760e64bec6ee9097b5a06b0d91cb3ab64',\n    'text-embedding-ada-0023b424f5ed1271a6f5601add17c1b58b7c992772e',\n    'text-embedding-ada-002fec5d021611e1527297c5e8f485876ea82dcb111',\n    'text-embedding-ada-00262f72e0c2d711c6b861714ee624b28af639fdb13',\n    'text-embedding-ada-00262d58882330038a4e6e25ea69a938f4391541874'\n  ]\n*/","metadata":{"source":"examples/src/embeddings/cache_backed_in_memory.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":69}}}}],["1f1ce2c4-12bc-40fa-8ba7-d8b85b7225c7",{"pageContent":"import { Redis } from \"ioredis\";\n\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { CacheBackedEmbeddings } from \"langchain/embeddings/cache_backed\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport { FaissStore } from \"langchain/vectorstores/faiss\";\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\nimport { RedisByteStore } from \"langchain/storage/ioredis\";\n\nconst underlyingEmbeddings = new OpenAIEmbeddings();\n\n// Requires a Redis instance running at http://localhost:6379.\n// See https://github.com/redis/ioredis for full config options.\nconst redisClient = new Redis();\nconst redisStore = new RedisByteStore({\n  client: redisClient,\n});\n\nconst cacheBackedEmbeddings = CacheBackedEmbeddings.fromBytesStore(\n  underlyingEmbeddings,\n  redisStore,\n  {\n    namespace: underlyingEmbeddings.modelName,\n  }\n);\n\nconst loader = new TextLoader(\"./state_of_the_union.txt\");\nconst rawDocuments = await loader.load();\nconst splitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 1000,\n  chunkOverlap: 0,\n});\nconst documents = await splitter.splitDocuments(rawDocuments);\n\nlet time = Date.now();\nconst vectorstore = await FaissStore.fromDocuments(\n  documents,\n  cacheBackedEmbeddings\n);\nconsole.log(`Initial creation time: ${Date.now() - time}ms`);\n/*\n  Initial creation time: 1808ms\n*/\n\n// The second time is much faster since the embeddings for the input docs have already been added to the cache\ntime = Date.now();\nconst vectorstore2 = await FaissStore.fromDocuments(\n  documents,\n  cacheBackedEmbeddings\n);\nconsole.log(`Cached creation time: ${Date.now() - time}ms`);\n/*\n  Cached creation time: 33ms\n*/\n\n// Many keys logged with hashed values\nconst keys = [];\nfor await (const key of redisStore.yieldKeys()) {\n  keys.push(key);\n}\n\nconsole.log(keys.slice(0, 5));\n/*\n  [\n    'text-embedding-ada-002fa9ac80e1bf226b7b4dfc03ea743289a65a727b2',\n    'text-embedding-ada-0027dbf9c4b36e12fe1768300f145f4640342daaf22',\n    'text-embedding-ada-002ea9b59e760e64bec6ee9097b5a06b0d91cb3ab64',\n    'text-embedding-ada-002fec5d021611e1527297c5e8f485876ea82dcb111',\n    'text-embedding-ada-002c00f818c345da13fed9f2697b4b689338143c8c7'\n  ]\n*/","metadata":{"source":"examples/src/embeddings/cache_backed_redis.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":71}}}}],["2ece585d-7b3f-4542-b428-d8d0f53bc5c4",{"pageContent":"import { CohereEmbeddings } from \"langchain/embeddings/cohere\";\n\nexport const run = async () => {\n  const model = new CohereEmbeddings();\n  const res = await model.embedQuery(\n    \"What would be a good company name a company that makes colorful socks?\"\n  );\n  console.log({ res });\n};","metadata":{"source":"examples/src/embeddings/cohere.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":9}}}}],["3c5b6b91-35bd-4de1-b3c2-68598d0e7e22",{"pageContent":"import { LlamaCppEmbeddings } from \"langchain/embeddings/llama_cpp\";\n\nconst llamaPath = \"/Replace/with/path/to/your/model/gguf-llama2-q4_0.bin\";\n\nconst embeddings = new LlamaCppEmbeddings({\n  modelPath: llamaPath,\n});\n\nconst res = embeddings.embedQuery(\"Hello Llama!\");\n\nconsole.log(res);\n\n/*\n\t[ 15043, 365, 29880, 3304, 29991 ]\n*/","metadata":{"source":"examples/src/embeddings/llama_cpp_basic.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":15}}}}],["35e6db5a-98aa-4534-8ef3-accae548e61d",{"pageContent":"import { LlamaCppEmbeddings } from \"langchain/embeddings/llama_cpp\";\n\nconst llamaPath = \"/Replace/with/path/to/your/model/gguf-llama2-q4_0.bin\";\n\nconst documents = [\"Hello World!\", \"Bye Bye!\"];\n\nconst embeddings = new LlamaCppEmbeddings({\n  modelPath: llamaPath,\n});\n\nconst res = await embeddings.embedDocuments(documents);\n\nconsole.log(res);\n\n/*\n\t[ [ 15043, 2787, 29991 ], [ 2648, 29872, 2648, 29872, 29991 ] ]\n*/","metadata":{"source":"examples/src/embeddings/llama_cpp_docs.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":17}}}}],["9b3f67e3-5ed8-421a-a771-20aff2c7c7b9",{"pageContent":"import { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nexport const run = async () => {\n  const model = new OpenAIEmbeddings({\n    maxConcurrency: 1,\n  });\n  const res = await model.embedQuery(\n    \"What would be a good company name a company that makes colorful socks?\"\n  );\n  console.log({ res });\n};","metadata":{"source":"examples/src/embeddings/max_concurrency.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":11}}}}],["6ea6a908-cda1-4e7f-84dd-1b4f63edbc97",{"pageContent":"import { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nexport const run = async () => {\n  const model = new OpenAIEmbeddings();\n  const res = await model.embedQuery(\n    \"What would be a good company name a company that makes colorful socks?\"\n  );\n  console.log({ res });\n};","metadata":{"source":"examples/src/embeddings/openai.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":9}}}}],["b9b02419-13f3-46cf-8ac7-21fb9c9efb79",{"pageContent":"\"use node\";\n\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\nimport { CacheBackedEmbeddings } from \"langchain/embeddings/cache_backed\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { ConvexKVStore } from \"langchain/storage/convex\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport { ConvexVectorStore } from \"langchain/vectorstores/convex\";\nimport { action } from \"./_generated/server.js\";\n\nexport const ask = action({\n  args: {},\n  handler: async (ctx) => {\n    const underlyingEmbeddings = new OpenAIEmbeddings();\n\n    const cacheBackedEmbeddings = CacheBackedEmbeddings.fromBytesStore(\n      underlyingEmbeddings,\n      new ConvexKVStore({ ctx }),\n      {\n        namespace: underlyingEmbeddings.modelName,\n      }\n    );\n\n    const loader = new TextLoader(\"./state_of_the_union.txt\");\n    const rawDocuments = await loader.load();\n    const splitter = new RecursiveCharacterTextSplitter({\n      chunkSize: 1000,\n      chunkOverlap: 0,\n    });\n    const documents = await splitter.splitDocuments(rawDocuments);\n\n    let time = Date.now();\n    const vectorstore = await ConvexVectorStore.fromDocuments(\n      documents,\n      cacheBackedEmbeddings,\n      { ctx }\n    );\n    console.log(`Initial creation time: ${Date.now() - time}ms`);\n    /*\n      Initial creation time: 1808ms\n    */\n\n    // The second time is much faster since the embeddings for the input docs have already been added to the cache\n    time = Date.now();\n    const vectorstore2 = await ConvexVectorStore.fromDocuments(\n      documents,\n      cacheBackedEmbeddings,\n      { ctx }\n    );\n    console.log(`Cached creation time: ${Date.now() - time}ms`);\n    /*\n      Cached creation time: 33ms\n    */\n  },\n});","metadata":{"source":"examples/src/embeddings/convex/cache_backed_convex.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":55}}}}],["c99193e8-2c01-424e-9416-e32d2f5d345e",{"pageContent":"/* eslint-disable */\n/**\n * Generated `api` utility.\n *\n * THIS CODE IS AUTOMATICALLY GENERATED.\n *\n * Generated by convex@1.3.1.\n * To regenerate, run `npx convex dev`.\n * @module\n */\n\nimport type {\n  ApiFromModules,\n  FilterApi,\n  FunctionReference,\n} from \"convex/server\";\n\n/**\n * A utility for referencing Convex functions in your app's API.\n *\n * Usage:\n * ```js\n * const myFunctionReference = api.myModule.myFunction;\n * ```\n */\ndeclare const fullApi: ApiFromModules<{}>;\nexport declare const api: FilterApi<\n  typeof fullApi,\n  FunctionReference<any, \"public\">\n>;\nexport declare const internal: FilterApi<\n  typeof fullApi,\n  FunctionReference<any, \"internal\">\n>;","metadata":{"source":"examples/src/embeddings/convex/_generated/api.d.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":34}}}}],["35bfb93e-3c33-4237-af2f-fc4b031c383d",{"pageContent":"/* eslint-disable */\n/**\n * Generated `api` utility.\n *\n * THIS CODE IS AUTOMATICALLY GENERATED.\n *\n * Generated by convex@1.3.1.\n * To regenerate, run `npx convex dev`.\n * @module\n */\n\nimport { anyApi } from \"convex/server\";\n\n/**\n * A utility for referencing Convex functions in your app's API.\n *\n * Usage:\n * ```js\n * const myFunctionReference = api.myModule.myFunction;\n * ```\n */\nexport const api = anyApi;\nexport const internal = anyApi;","metadata":{"source":"examples/src/embeddings/convex/_generated/api.js","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":23}}}}],["9a1f3931-4e48-49bf-abda-222d4d0a18e9",{"pageContent":"/* eslint-disable */\n/**\n * Generated data model types.\n *\n * THIS CODE IS AUTOMATICALLY GENERATED.\n *\n * Generated by convex@1.3.1.\n * To regenerate, run `npx convex dev`.\n * @module\n */\n\nimport { AnyDataModel } from \"convex/server\";\nimport type { GenericId } from \"convex/values\";\n\n/**\n * No `schema.ts` file found!\n *\n * This generated code has permissive types like `Doc = any` because\n * Convex doesn't know your schema. If you'd like more type safety, see\n * https://docs.convex.dev/using/schemas for instructions on how to add a\n * schema file.\n *\n * After you change a schema, rerun codegen with `npx convex dev`.\n */\n\n/**\n * The names of all of your Convex tables.\n */\nexport type TableNames = string;\n\n/**\n * The type of a document stored in Convex.\n */\nexport type Doc = any;\n\n/**\n * An identifier for a document in Convex.\n *\n * Convex documents are uniquely identified by their `Id`, which is accessible\n * on the `_id` field. To learn more, see [Document IDs](https://docs.convex.dev/using/document-ids).\n *\n * Documents can be loaded using `db.get(id)` in query and mutation functions.\n *\n * IDs are just strings at runtime, but this type can be used to distinguish them from other\n * strings when type checking.\n */\nexport type Id<TableName extends TableNames = TableNames> =\n  GenericId<TableName>;\n\n/**\n * A type describing your Convex data model.\n *\n * This type includes information about what tables you have, the type of\n * documents stored in those tables, and the indexes defined on them.\n *\n * This type is used to parameterize methods like `queryGeneric` and\n * `mutationGeneric` to make them type-safe.\n */\nexport type DataModel = AnyDataModel;","metadata":{"source":"examples/src/embeddings/convex/_generated/dataModel.d.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":59}}}}],["b5dad3c2-e4fa-4bda-928c-7d7712649a19",{"pageContent":"/* eslint-disable */\n/**\n * Generated utilities for implementing server-side Convex query and mutation functions.\n *\n * THIS CODE IS AUTOMATICALLY GENERATED.\n *\n * Generated by convex@1.3.1.\n * To regenerate, run `npx convex dev`.\n * @module\n */\n\nimport {\n  ActionBuilder,\n  HttpActionBuilder,\n  MutationBuilder,\n  QueryBuilder,\n  GenericActionCtx,\n  GenericMutationCtx,\n  GenericQueryCtx,\n  GenericDatabaseReader,\n  GenericDatabaseWriter,\n} from \"convex/server\";\nimport type { DataModel } from \"./dataModel.js\";\n\n/**\n * Define a query in this Convex app's public API.\n *\n * This function will be allowed to read your Convex database and will be accessible from the client.\n *\n * @param func - The query function. It receives a {@link QueryCtx} as its first argument.\n * @returns The wrapped query. Include this as an `export` to name it and make it accessible.\n */\nexport declare const query: QueryBuilder<DataModel, \"public\">;\n\n/**\n * Define a query that is only accessible from other Convex functions (but not from the client).\n *\n * This function will be allowed to read from your Convex database. It will not be accessible from the client.\n *\n * @param func - The query function. It receives a {@link QueryCtx} as its first argument.\n * @returns The wrapped query. Include this as an `export` to name it and make it accessible.\n */\nexport declare const internalQuery: QueryBuilder<DataModel, \"internal\">;\n\n/**\n * Define a mutation in this Convex app's public API.\n *\n * This function will be allowed to modify your Convex database and will be accessible from the client.\n *\n * @param func - The mutation function. It receives a {@link MutationCtx} as its first argument.\n * @returns The wrapped mutation. Include this as an `export` to name it and make it accessible.\n */\nexport declare const mutation: MutationBuilder<DataModel, \"public\">;\n\n/**\n * Define a mutation that is only accessible from other Convex functions (but not from the client).\n *\n * This function will be allowed to modify your Convex database. It will not be accessible from the client.\n *\n * @param func - The mutation function. It receives a {@link MutationCtx} as its first argument.\n * @returns The wrapped mutation. Include this as an `export` to name it and make it accessible.\n */\nexport declare const internalMutation: MutationBuilder<DataModel, \"internal\">;\n\n/**\n * Define an action in this Convex app's public API.\n *\n * An action is a function which can execute any JavaScript code, including non-deterministic\n * code and code with side-effects, like calling third-party services.\n * They can be run in Convex's JavaScript environment or in Node.js using the \"use node\" directive.\n * They can interact with the database indirectly by calling queries and mutations using the {@link ActionCtx}.\n *\n * @param func - The action. It receives an {@link ActionCtx} as its first argument.\n * @returns The wrapped action. Include this as an `export` to name it and make it accessible.\n */\nexport declare const action: ActionBuilder<DataModel, \"public\">;\n\n/**\n * Define an action that is only accessible from other Convex functions (but not from the client).\n *\n * @param func - The function. It receives an {@link ActionCtx} as its first argument.\n * @returns The wrapped function. Include this as an `export` to name it and make it accessible.\n */\nexport declare const internalAction: ActionBuilder<DataModel, \"internal\">;\n\n/**\n * Define an HTTP action.\n *\n * This function will be used to respond to HTTP requests received by a Convex\n * deployment if the requests matches the path and method where this action\n * is routed. Be sure to route your action in `convex/http.js`.\n *\n * @param func - The function. It receives an {@link ActionCtx} as its first argument.\n * @returns The wrapped function. Import this function from `convex/http.js` and route it to hook it up.\n */\nexport declare const httpAction: HttpActionBuilder;","metadata":{"source":"examples/src/embeddings/convex/_generated/server.d.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":96}}}}],["9b2224dd-f335-4680-af60-63f62e932deb",{"pageContent":"/**\n * A set of services for use within Convex query functions.\n *\n * The query context is passed as the first argument to any Convex query\n * function run on the server.\n *\n * This differs from the {@link MutationCtx} because all of the services are\n * read-only.\n */\nexport type QueryCtx = GenericQueryCtx<DataModel>;\n\n/**\n * A set of services for use within Convex mutation functions.\n *\n * The mutation context is passed as the first argument to any Convex mutation\n * function run on the server.\n */\nexport type MutationCtx = GenericMutationCtx<DataModel>;\n\n/**\n * A set of services for use within Convex action functions.\n *\n * The action context is passed as the first argument to any Convex action\n * function run on the server.\n */\nexport type ActionCtx = GenericActionCtx<DataModel>;\n\n/**\n * An interface to read from the database within Convex query functions.\n *\n * The two entry points are {@link DatabaseReader.get}, which fetches a single\n * document by its {@link Id}, or {@link DatabaseReader.query}, which starts\n * building a query.\n */\nexport type DatabaseReader = GenericDatabaseReader<DataModel>;\n\n/**\n * An interface to read from and write to the database within Convex mutation\n * functions.\n *\n * Convex guarantees that all writes within a single mutation are\n * executed atomically, so you never have to worry about partial writes leaving\n * your data in an inconsistent state. See [the Convex Guide](https://docs.convex.dev/understanding/convex-fundamentals/functions#atomicity-and-optimistic-concurrency-control)\n * for the guarantees Convex provides your functions.\n */\nexport type DatabaseWriter = GenericDatabaseWriter<DataModel>;","metadata":{"source":"examples/src/embeddings/convex/_generated/server.d.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":98,"to":143}}}}],["faa6a8ee-8ce2-4bf0-9e3f-cddf0d109b24",{"pageContent":"/* eslint-disable */\n/**\n * Generated utilities for implementing server-side Convex query and mutation functions.\n *\n * THIS CODE IS AUTOMATICALLY GENERATED.\n *\n * Generated by convex@1.3.1.\n * To regenerate, run `npx convex dev`.\n * @module\n */\n\nimport {\n  actionGeneric,\n  httpActionGeneric,\n  queryGeneric,\n  mutationGeneric,\n  internalActionGeneric,\n  internalMutationGeneric,\n  internalQueryGeneric,\n} from \"convex/server\";\n\n/**\n * Define a query in this Convex app's public API.\n *\n * This function will be allowed to read your Convex database and will be accessible from the client.\n *\n * @param func - The query function. It receives a {@link QueryCtx} as its first argument.\n * @returns The wrapped query. Include this as an `export` to name it and make it accessible.\n */\nexport const query = queryGeneric;\n\n/**\n * Define a query that is only accessible from other Convex functions (but not from the client).\n *\n * This function will be allowed to read from your Convex database. It will not be accessible from the client.\n *\n * @param func - The query function. It receives a {@link QueryCtx} as its first argument.\n * @returns The wrapped query. Include this as an `export` to name it and make it accessible.\n */\nexport const internalQuery = internalQueryGeneric;\n\n/**\n * Define a mutation in this Convex app's public API.\n *\n * This function will be allowed to modify your Convex database and will be accessible from the client.\n *\n * @param func - The mutation function. It receives a {@link MutationCtx} as its first argument.\n * @returns The wrapped mutation. Include this as an `export` to name it and make it accessible.\n */\nexport const mutation = mutationGeneric;\n\n/**\n * Define a mutation that is only accessible from other Convex functions (but not from the client).\n *\n * This function will be allowed to modify your Convex database. It will not be accessible from the client.\n *\n * @param func - The mutation function. It receives a {@link MutationCtx} as its first argument.\n * @returns The wrapped mutation. Include this as an `export` to name it and make it accessible.\n */\nexport const internalMutation = internalMutationGeneric;\n\n/**\n * Define an action in this Convex app's public API.\n *\n * An action is a function which can execute any JavaScript code, including non-deterministic\n * code and code with side-effects, like calling third-party services.\n * They can be run in Convex's JavaScript environment or in Node.js using the \"use node\" directive.\n * They can interact with the database indirectly by calling queries and mutations using the {@link ActionCtx}.\n *\n * @param func - The action. It receives an {@link ActionCtx} as its first argument.\n * @returns The wrapped action. Include this as an `export` to name it and make it accessible.\n */\nexport const action = actionGeneric;\n\n/**\n * Define an action that is only accessible from other Convex functions (but not from the client).\n *\n * @param func - The function. It receives an {@link ActionCtx} as its first argument.\n * @returns The wrapped function. Include this as an `export` to name it and make it accessible.\n */\nexport const internalAction = internalActionGeneric;\n\n/**\n * Define a Convex HTTP action.\n *\n * @param func - The function. It receives an {@link ActionCtx} as its first argument, and a `Request` object\n * as its second.\n * @returns The wrapped endpoint function. Route a URL path to this function in `convex/http.js`.\n */\nexport const httpAction = httpActionGeneric;","metadata":{"source":"examples/src/embeddings/convex/_generated/server.js","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":90}}}}],["52062222-10ce-4148-a3e4-c2a1e05e098c",{"pageContent":"import { AutoGPT } from \"langchain/experimental/autogpt\";\nimport { ReadFileTool, WriteFileTool, SerpAPI } from \"langchain/tools\";\nimport { NodeFileStore } from \"langchain/stores/file/node\";\nimport { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\n\nconst store = new NodeFileStore();\n\nconst tools = [\n  new ReadFileTool({ store }),\n  new WriteFileTool({ store }),\n  new SerpAPI(process.env.SERPAPI_API_KEY, {\n    location: \"San Francisco,California,United States\",\n    hl: \"en\",\n    gl: \"us\",\n  }),\n];\n\nconst vectorStore = new HNSWLib(new OpenAIEmbeddings(), {\n  space: \"cosine\",\n  numDimensions: 1536,\n});\n\nconst autogpt = AutoGPT.fromLLMAndTools(\n  new ChatOpenAI({ temperature: 0 }),\n  tools,\n  {\n    memory: vectorStore.asRetriever(),\n    aiName: \"Tom\",\n    aiRole: \"Assistant\",\n  }\n);\n\nawait autogpt.run([\"write a weather report for SF today\"]);\n/*\n{\n    \"thoughts\": {\n        \"text\": \"I need to write a weather report for SF today. I should use a search engine to find the current weather conditions.\",\n        \"reasoning\": \"I don't have the current weather information for SF in my short term memory, so I need to use a search engine to find it.\",\n        \"plan\": \"- Use the search command to find the current weather conditions for SF\\n- Write a weather report based on the information found\",\n        \"criticism\": \"I need to make sure that the information I find is accurate and up-to-date.\",\n        \"speak\": \"I will use the search command to find the current weather conditions for SF.\"\n    },\n    \"command\": {\n        \"name\": \"search\",\n        \"args\": {\n            \"input\": \"current weather conditions San Francisco\"\n        }\n    }\n}\n{\n    \"thoughts\": {\n        \"text\": \"I have found the current weather conditions for SF. I need to write a weather report based on this information.\",\n        \"reasoning\": \"I have the information I need to write a weather report, so I should use the write_file command to save it to a file.\",\n        \"plan\": \"- Use the write_file command to save the weather report to a file\",\n        \"criticism\": \"I need to make sure that the weather report is clear and concise.\",\n        \"speak\": \"I will use the write_file command to save the weather report to a file.\"\n    },\n    \"command\": {\n        \"name\": \"write_file\",\n        \"args\": {\n            \"file_path\": \"weather_report.txt\",\n            \"text\": \"San Francisco Weather Report:\\n\\nMorning: 53°, Chance of Rain 1%\\nAfternoon: 59°, Chance of Rain 0%\\nEvening: 52°, Chance of Rain 3%\\nOvernight: 48°, Chance of Rain 2%\"\n        }\n    }\n}\n{\n    \"thoughts\": {\n        \"text\": \"I have completed all my objectives. I will use the finish command to signal that I am done.\",\n        \"reasoning\": \"I have completed the task of writing a weather report for SF today, so I don't need to do anything else.\",\n        \"plan\": \"- Use the finish command to signal that I am done\",\n        \"criticism\": \"I need to make sure that I have completed all my objectives before using the finish command.\",\n        \"speak\": \"I will use the finish command to signal that I am done.\"\n    },\n    \"command\": {\n        \"name\": \"finish\",\n        \"args\": {\n            \"response\": \"I have completed all my objectives.\"\n        }\n    }\n}\n*/","metadata":{"source":"examples/src/experimental/autogpt/weather.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":83}}}}],["00c9ec6b-1bb2-49c3-bf18-2e7716e75c1a",{"pageContent":"import { AutoGPT } from \"langchain/experimental/autogpt\";\nimport { ReadFileTool, WriteFileTool, SerpAPI } from \"langchain/tools\";\nimport { InMemoryFileStore } from \"langchain/stores/file/in_memory\";\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\n\nconst store = new InMemoryFileStore();\n\nconst tools = [\n  new ReadFileTool({ store }),\n  new WriteFileTool({ store }),\n  new SerpAPI(process.env.SERPAPI_API_KEY, {\n    location: \"San Francisco,California,United States\",\n    hl: \"en\",\n    gl: \"us\",\n  }),\n];\n\nconst vectorStore = new MemoryVectorStore(new OpenAIEmbeddings());\n\nconst autogpt = AutoGPT.fromLLMAndTools(\n  new ChatOpenAI({ temperature: 0 }),\n  tools,\n  {\n    memory: vectorStore.asRetriever(),\n    aiName: \"Tom\",\n    aiRole: \"Assistant\",\n  }\n);\n\nawait autogpt.run([\"write a weather report for SF today\"]);\n/*\n{\n    \"thoughts\": {\n        \"text\": \"I need to write a weather report for SF today. I should use a search engine to find the current weather conditions.\",\n        \"reasoning\": \"I don't have the current weather information for SF in my short term memory, so I need to use a search engine to find it.\",\n        \"plan\": \"- Use the search command to find the current weather conditions for SF\\n- Write a weather report based on the information found\",\n        \"criticism\": \"I need to make sure that the information I find is accurate and up-to-date.\",\n        \"speak\": \"I will use the search command to find the current weather conditions for SF.\"\n    },\n    \"command\": {\n        \"name\": \"search\",\n        \"args\": {\n            \"input\": \"current weather conditions San Francisco\"\n        }\n    }\n}\n{\n    \"thoughts\": {\n        \"text\": \"I have found the current weather conditions for SF. I need to write a weather report based on this information.\",\n        \"reasoning\": \"I have the information I need to write a weather report, so I should use the write_file command to save it to a file.\",\n        \"plan\": \"- Use the write_file command to save the weather report to a file\",\n        \"criticism\": \"I need to make sure that the weather report is clear and concise.\",\n        \"speak\": \"I will use the write_file command to save the weather report to a file.\"\n    },\n    \"command\": {\n        \"name\": \"write_file\",\n        \"args\": {\n            \"file_path\": \"weather_report.txt\",\n            \"text\": \"San Francisco Weather Report:\\n\\nMorning: 53°, Chance of Rain 1%\\nAfternoon: 59°, Chance of Rain 0%\\nEvening: 52°, Chance of Rain 3%\\nOvernight: 48°, Chance of Rain 2%\"\n        }\n    }\n}\n{\n    \"thoughts\": {\n        \"text\": \"I have completed all my objectives. I will use the finish command to signal that I am done.\",\n        \"reasoning\": \"I have completed the task of writing a weather report for SF today, so I don't need to do anything else.\",\n        \"plan\": \"- Use the finish command to signal that I am done\",\n        \"criticism\": \"I need to make sure that I have completed all my objectives before using the finish command.\",\n        \"speak\": \"I will use the finish command to signal that I am done.\"\n    },\n    \"command\": {\n        \"name\": \"finish\",\n        \"args\": {\n            \"response\": \"I have completed all my objectives.\"\n        }\n    }\n}\n*/","metadata":{"source":"examples/src/experimental/autogpt/weather_browser.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":80}}}}],["fa1eb484-3b59-4ba5-8af2-db53aa63acfe",{"pageContent":"import { BabyAGI } from \"langchain/experimental/babyagi\";\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { OpenAI } from \"langchain/llms/openai\";\n\nconst vectorStore = new MemoryVectorStore(new OpenAIEmbeddings());\n\nconst babyAGI = BabyAGI.fromLLM({\n  llm: new OpenAI({ temperature: 0 }),\n  vectorstore: vectorStore,\n  maxIterations: 3,\n});\n\nawait babyAGI.call({ objective: \"Write a weather report for SF today\" });\n/*\n\n*****TASK LIST*****\n\n1: Make a todo list\n\n*****NEXT TASK*****\n\n1: Make a todo list\n\n*****TASK RESULT*****\n\n1. Check the weather forecast for San Francisco today\n2. Make note of the temperature, humidity, wind speed, and other relevant weather conditions\n3. Write a weather report summarizing the forecast\n4. Check for any weather alerts or warnings\n5. Share the report with the relevant stakeholders\n\n*****TASK LIST*****\n\n2: Check the current temperature in San Francisco\n3: Check the current humidity in San Francisco\n4: Check the current wind speed in San Francisco\n5: Check for any weather alerts or warnings in San Francisco\n6: Check the forecast for the next 24 hours in San Francisco\n7: Check the forecast for the next 48 hours in San Francisco\n8: Check the forecast for the next 72 hours in San Francisco\n9: Check the forecast for the next week in San Francisco\n10: Check the forecast for the next month in San Francisco\n11: Check the forecast for the next 3 months in San Francisco\n1: Write a weather report for SF today\n\n*****NEXT TASK*****\n\n2: Check the current temperature in San Francisco\n\n*****TASK RESULT*****\n\nI will check the current temperature in San Francisco. I will use an online weather service to get the most up-to-date information.\n\n*****TASK LIST*****\n\n3: Check the current UV index in San Francisco\n4: Check the current air quality in San Francisco\n5: Check the current precipitation levels in San Francisco\n6: Check the current cloud cover in San Francisco\n7: Check the current barometric pressure in San Francisco\n8: Check the current dew point in San Francisco\n9: Check the current wind direction in San Francisco\n10: Check the current humidity levels in San Francisco\n1: Check the current temperature in San Francisco to the average temperature for this time of year\n2: Check the current visibility in San Francisco\n11: Write a weather report for SF today\n\n*****NEXT TASK*****\n\n3: Check the current UV index in San Francisco\n\n*****TASK RESULT*****\n\nThe current UV index in San Francisco is moderate, with a value of 5. This means that it is safe to be outside for short periods of time without sunscreen, but it is still recommended to wear sunscreen and protective clothing when outside for extended periods of time.\n*/","metadata":{"source":"examples/src/experimental/babyagi/weather.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":76}}}}],["cfc96975-3fe5-4cf4-a2a8-251dd65ea3d3",{"pageContent":"import { BabyAGI } from \"langchain/experimental/babyagi\";\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport { LLMChain } from \"langchain/chains\";\nimport { ChainTool, SerpAPI, Tool } from \"langchain/tools\";\nimport { initializeAgentExecutorWithOptions } from \"langchain/agents\";\n\n// First, we create a custom agent which will serve as execution chain.\nconst todoPrompt = PromptTemplate.fromTemplate(\n  \"You are a planner who is an expert at coming up with a todo list for a given objective. Come up with a todo list for this objective: {objective}\"\n);\nconst tools: Tool[] = [\n  new SerpAPI(process.env.SERPAPI_API_KEY, {\n    location: \"San Francisco,California,United States\",\n    hl: \"en\",\n    gl: \"us\",\n  }),\n  new ChainTool({\n    name: \"TODO\",\n    chain: new LLMChain({\n      llm: new OpenAI({ temperature: 0 }),\n      prompt: todoPrompt,\n    }),\n    description:\n      \"useful for when you need to come up with todo lists. Input: an objective to create a todo list for. Output: a todo list for that objective. Please be very clear what the objective is!\",\n  }),\n];\nconst agentExecutor = await initializeAgentExecutorWithOptions(\n  tools,\n  new OpenAI({ temperature: 0 }),\n  {\n    agentType: \"zero-shot-react-description\",\n    agentArgs: {\n      prefix: `You are an AI who performs one task based on the following objective: {objective}. Take into account these previously completed tasks: {context}.`,\n      suffix: `Question: {task}\n{agent_scratchpad}`,\n      inputVariables: [\"objective\", \"task\", \"context\", \"agent_scratchpad\"],\n    },\n  }\n);\n\nconst vectorStore = new MemoryVectorStore(new OpenAIEmbeddings());\n\n// Then, we create a BabyAGI instance.","metadata":{"source":"examples/src/experimental/babyagi/weather_with_tools.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":46}}}}],["b677b290-360c-4af1-9cab-1dac3049be65",{"pageContent":"const babyAGI = BabyAGI.fromLLM({\n  llm: new OpenAI({ temperature: 0 }),\n  executionChain: agentExecutor, // an agent executor is a chain\n  vectorstore: vectorStore,\n  maxIterations: 10,\n});\n\nawait babyAGI.call({ objective: \"Write a short weather report for SF today\" });\n/*\n\n*****TASK LIST*****\n\n1: Make a todo list\n\n*****NEXT TASK*****\n\n1: Make a todo list\n\n*****TASK RESULT*****\n\nToday in San Francisco, the weather is sunny with a temperature of 70 degrees Fahrenheit, light winds, and low humidity. The forecast for the next few days is expected to be similar.\n\n*****TASK LIST*****\n\n2: Find the forecasted temperature for the next few days in San Francisco\n3: Find the forecasted wind speed for the next few days in San Francisco\n4: Find the forecasted humidity for the next few days in San Francisco\n5: Create a graph showing the forecasted temperature, wind speed, and humidity for San Francisco over the next few days\n6: Research the average temperature for San Francisco in the past week\n7: Research the average wind speed for San Francisco in the past week\n8: Research the average humidity for San Francisco in the past week\n9: Create a graph showing the temperature, wind speed, and humidity for San Francisco over the past week\n\n*****NEXT TASK*****\n\n2: Find the forecasted temperature for the next few days in San Francisco\n\n*****TASK RESULT*****\n\nThe forecasted temperature for the next few days in San Francisco is 63°, 65°, 71°, 73°, and 66°.\n\n*****TASK LIST*****\n\n3: Find the forecasted wind speed for the next few days in San Francisco\n4: Find the forecasted humidity for the next few days in San Francisco\n5: Create a graph showing the forecasted temperature, wind speed, and humidity for San Francisco over the next few days\n6: Research the average temperature for San Francisco in the past week\n7: Research the average wind speed for San Francisco in the past week\n8: Research the average humidity for San Francisco in the past week\n9: Create a graph showing the temperature, wind speed, and humidity for San Francisco over the past week\n10: Compare the forecasted temperature, wind speed, and humidity for San Francisco over the next few days to the average temperature, wind speed, and humidity for San Francisco over the past week\n11: Find the forecasted precipitation for the next few days in San Francisco\n12: Research the average wind direction for San Francisco in the past week\n13: Create a graph showing the forecasted temperature, wind speed, and humidity for San Francisco over the past week\n14: Compare the forecasted temperature, wind speed, and humidity for San Francisco over the next few days to\n\n*****NEXT TASK*****\n\n3: Find the forecasted wind speed for the next few days in San Francisco\n\n*****TASK RESULT*****\n\nWest winds 10 to 20 mph. Gusts up to 35 mph in the evening. Tuesday. Sunny. Highs in the 60s to upper 70s. West winds 5 to 15 mph.\n\n*****TASK LIST*****\n\n4: Research the average precipitation for San Francisco in the past week\n5: Research the average temperature for San Francisco in the past week\n6: Research the average wind speed for San Francisco in the past week\n7: Research the average humidity for San Francisco in the past week\n8: Research the average wind direction for San Francisco in the past week\n9: Find the forecasted temperature, wind speed, and humidity for San Francisco over the next few days\n10: Find the forecasted precipitation for the next few days in San Francisco\n11: Create a graph showing the forecasted temperature, wind speed, and humidity for San Francisco over the next few days\n12: Create a graph showing the temperature, wind speed, and humidity for San Francisco over the past week\n13: Create a graph showing the forecasted temperature, wind speed, and humidity for San Francisco over the past month\n14: Compare the forecasted temperature, wind speed, and humidity for San Francisco over the next few days to the average temperature, wind speed, and humidity for San Francisco over the past week\n15: Compare the forecasted temperature, wind speed, and humidity for San Francisco over the next few days to the","metadata":{"source":"examples/src/experimental/babyagi/weather_with_tools.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":47,"to":124}}}}],["09680c93-d403-4799-952c-c14d9268fd25",{"pageContent":"*****NEXT TASK*****\n\n4: Research the average precipitation for San Francisco in the past week\n\n*****TASK RESULT*****\n\nAccording to Weather Underground, the forecasted precipitation for San Francisco in the next few days is 7-hour rain and snow with 24-hour rain accumulation.\n\n*****TASK LIST*****\n\n5: Research the average wind speed for San Francisco over the past month\n6: Create a graph showing the forecasted temperature, wind speed, and humidity for San Francisco over the past month\n7: Compare the forecasted temperature, wind speed, and humidity for San Francisco over the next few days to the average temperature, wind speed, and humidity for San Francisco over the past month\n8: Research the average temperature for San Francisco over the past month\n9: Research the average wind direction for San Francisco over the past month\n10: Create a graph showing the forecasted precipitation for San Francisco over the next few days\n11: Compare the forecasted precipitation for San Francisco over the next few days to the average precipitation for San Francisco over the past week\n12: Find the forecasted temperature, wind speed, and humidity for San Francisco over the next few days\n13: Find the forecasted precipitation for the next few days in San Francisco\n14: Create a graph showing the temperature, wind speed, and humidity for San Francisco over the past week\n15: Create a graph showing the forecasted temperature, wind speed, and humidity for San Francisco over the next few days\n16: Compare the forecast\n\n*****NEXT TASK*****\n\n5: Research the average wind speed for San Francisco over the past month\n\n*****TASK RESULT*****\n\nThe average wind speed for San Francisco over the past month is 3.2 meters per second.\n\n*****TASK LIST*****\n\n6: Find the forecasted temperature, wind speed, and humidity for San Francisco over the next few days,\n7: Find the forecasted precipitation for the next few days in San Francisco,\n8: Create a graph showing the temperature, wind speed, and humidity for San Francisco over the past week,\n9: Create a graph showing the forecasted temperature, wind speed, and humidity for San Francisco over the next few days,\n10: Compare the forecasted temperature, wind speed, and humidity for San Francisco over the next few days to the average wind speed for San Francisco over the past month,\n11: Research the average wind speed for San Francisco over the past week,\n12: Create a graph showing the forecasted precipitation for San Francisco over the next few days,\n13: Compare the forecasted precipitation for San Francisco over the next few days to the average precipitation for San Francisco over the past month,\n14: Research the average temperature for San Francisco over the past month,\n15: Research the average humidity for San Francisco over the past month,\n16: Compare the forecasted temperature, wind speed, and humidity for San Francisco over the next few days to the average temperature,\n\n*****NEXT TASK*****\n\n6: Find the forecasted temperature, wind speed, and humidity for San Francisco over the next few days,\n\n*****TASK RESULT*****\n\nThe forecast for San Francisco over the next few days is mostly sunny, with a high near 64. West wind 7 to 12 mph increasing to 13 to 18 mph in the afternoon. Winds could gust as high as 22 mph. Humidity will be around 50%.\n\n*****TASK LIST*****","metadata":{"source":"examples/src/experimental/babyagi/weather_with_tools.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":126,"to":179}}}}],["736d34f8-3270-4abc-b2c6-567e31a76c40",{"pageContent":"7: Find the forecasted precipitation for the next few days in San Francisco,\n8: Create a graph showing the temperature, wind speed, and humidity for San Francisco over the past week,\n9: Create a graph showing the forecasted temperature, wind speed, and humidity for San Francisco over the next few days,\n10: Compare the forecasted temperature, wind speed, and humidity for San Francisco over the next few days to the average wind speed for San Francisco over the past month,\n11: Research the average wind speed for San Francisco over the past week,\n12: Create a graph showing the forecasted precipitation for San Francisco over the next few days,\n13: Compare the forecasted precipitation for San Francisco over the next few days to the average precipitation for San Francisco over the past month,\n14: Research the average temperature for San Francisco over the past month,\n15: Research the average humidity for San Francisco over the past month,\n16: Compare the forecasted temperature, wind speed, and humidity for San Francisco over the next few days to the average temperature\n\n*****NEXT TASK*****\n\n7: Find the forecasted precipitation for the next few days in San Francisco,\n\n*****TASK RESULT*****\n\nAccording to Weather Underground, the forecasted precipitation for the next few days in San Francisco is 7-hour rain and snow with 24-hour rain accumulation, radar and satellite maps of precipitation.\n\n*****TASK LIST*****\n\n8: Create a graph showing the temperature, wind speed, and humidity for San Francisco over the past week,\n9: Create a graph showing the forecasted temperature, wind speed, and humidity for San Francisco over the next few days,\n10: Compare the forecasted temperature, wind speed, and humidity for San Francisco over the next few days to the average wind speed for San Francisco over the past month,\n11: Research the average wind speed for San Francisco over the past week,\n12: Create a graph showing the forecasted precipitation for San Francisco over the next few days,\n13: Compare the forecasted precipitation for San Francisco over the next few days to the average precipitation for San Francisco over the past month,\n14: Research the average temperature for San Francisco over the past month,\n15: Research the average humidity for San Francisco over the past month,\n16: Compare the forecasted temperature, wind speed, and humidity for San Francisco over the next few days to the average temperature\n\n*****NEXT TASK*****\n\n8: Create a graph showing the temperature, wind speed, and humidity for San Francisco over the past week,\n\n*****TASK RESULT*****\n\nA graph showing the temperature, wind speed, and humidity for San Francisco over the past week.\n\n*****TASK LIST*****\n\n9: Create a graph showing the forecasted temperature, wind speed, and humidity for San Francisco over the next few days\n10: Compare the forecasted temperature, wind speed, and humidity for San Francisco over the next few days to the average wind speed for San Francisco over the past month\n11: Research the average wind speed for San Francisco over the past week\n12: Create a graph showing the forecasted precipitation for San Francisco over the next few days\n13: Compare the forecasted precipitation for San Francisco over the next few days to the average precipitation for San Francisco over the past month\n14: Research the average temperature for San Francisco over the past month\n15: Research the average humidity for San Francisco over the past month\n16: Compare the forecasted temperature, wind speed, and humidity for San Francisco over the next few days to the average temperature\n\n*****NEXT TASK*****\n\n9: Create a graph showing the forecasted temperature, wind speed, and humidity for San Francisco over the next few days\n\n*****TASK RESULT*****\n\nThe forecasted temperature, wind speed, and humidity for San Francisco over the next few days can be seen in the graph created.\n\n*****TASK LIST*****","metadata":{"source":"examples/src/experimental/babyagi/weather_with_tools.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":181,"to":239}}}}],["039b944d-d500-4f70-87e0-48bf1c8d2d04",{"pageContent":"10: Research the average wind speed for San Francisco over the past month\n11: Compare the forecasted temperature, wind speed, and humidity for San Francisco over the next few days to the average humidity for San Francisco over the past month\n12: Create a graph showing the forecasted precipitation for San Francisco over the next few days\n13: Compare the forecasted precipitation for San Francisco over the next few days to the average precipitation for San Francisco over the past month\n14: Research the average temperature for San Francisco over the past week\n15: Compare the forecasted temperature, wind speed, and humidity for San Francisco over the next few days to the average wind speed for San Francisco over the past week\n\n*****NEXT TASK*****\n\n10: Research the average wind speed for San Francisco over the past month\n\n*****TASK RESULT*****\n\nThe average wind speed for San Francisco over the past month is 2.7 meters per second.\n\n[...]\n*/","metadata":{"source":"examples/src/experimental/babyagi/weather_with_tools.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":241,"to":257}}}}],["d867ebf6-a4dd-4f28-a7d8-0ca219598d33",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\nimport { TimeWeightedVectorStoreRetriever } from \"langchain/retrievers/time_weighted\";\nimport {\n  GenerativeAgentMemory,\n  GenerativeAgent,\n} from \"langchain/experimental/generative_agents\";","metadata":{"source":"examples/src/experimental/generative_agents/generative_agents.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":8}}}}],["59706847-96b3-42d5-9a86-4114bc69fc00",{"pageContent":"const Simulation = async () => {\n  const userName = \"USER\";\n  const llm = new OpenAI({\n    temperature: 0.9,\n    maxTokens: 1500,\n  });\n\n  const createNewMemoryRetriever = async () => {\n    // Create a new, demo in-memory vector store retriever unique to the agent.\n    // Better results can be achieved with a more sophisticatd vector store.\n    const vectorStore = new MemoryVectorStore(new OpenAIEmbeddings());\n    const retriever = new TimeWeightedVectorStoreRetriever({\n      vectorStore,\n      otherScoreKeys: [\"importance\"],\n      k: 15,\n    });\n    return retriever;\n  };\n\n  // Initializing Tommie\n  const tommiesMemory: GenerativeAgentMemory = new GenerativeAgentMemory(\n    llm,\n    await createNewMemoryRetriever(),\n    { reflectionThreshold: 8 }\n  );\n\n  const tommie: GenerativeAgent = new GenerativeAgent(llm, tommiesMemory, {\n    name: \"Tommie\",\n    age: 25,\n    traits: \"anxious, likes design, talkative\",\n    status: \"looking for a job\",\n  });\n\n  console.log(\"Tommie's first summary:\\n\", await tommie.getSummary());\n\n  /*\n    Tommie's first summary:\n    Name: Tommie (age: 25)\n    Innate traits: anxious, likes design, talkative\n    Tommie is an individual with no specific core characteristics described.\n  */\n\n  // Let's give Tommie some memories!\n  const tommieObservations = [\n    \"Tommie remembers his dog, Bruno, from when he was a kid\",\n    \"Tommie feels tired from driving so far\",\n    \"Tommie sees the new home\",\n    \"The new neighbors have a cat\",\n    \"The road is noisy at night\",\n    \"Tommie is hungry\",\n    \"Tommie tries to get some rest.\",\n  ];\n  for (const observation of tommieObservations) {\n    await tommie.addMemory(observation, new Date());\n  }\n\n  // Checking Tommie's summary again after giving him some memories\n  console.log(\n    \"Tommie's second summary:\\n\",\n    await tommie.getSummary({ forceRefresh: true })\n  );\n\n  /*\n    Tommie's second summary:\n    Name: Tommie (age: 25)\n    Innate traits: anxious, likes design, talkative\n    Tommie remembers his dog, is tired from driving, sees a new home with neighbors who have a cat, is aware of the noisy road at night, is hungry, and tries to get some rest.\n  */\n\n  const interviewAgent = async (\n    agent: GenerativeAgent,\n    message: string\n  ): Promise<string> => {\n    // Simple wrapper helping the user interact with the agent\n    const newMessage = `${userName} says ${message}`;\n    const response = await agent.generateDialogueResponse(newMessage);\n    return response[1];\n  };","metadata":{"source":"examples/src/experimental/generative_agents/generative_agents.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":10,"to":87}}}}],["ee570653-be7d-48ba-aa8f-a2746304f729",{"pageContent":"// Let's have Tommie start going through a day in his life.\n  const observations = [\n    \"Tommie wakes up to the sound of a noisy construction site outside his window.\",\n    \"Tommie gets out of bed and heads to the kitchen to make himself some coffee.\",\n    \"Tommie realizes he forgot to buy coffee filters and starts rummaging through his moving boxes to find some.\",\n    \"Tommie finally finds the filters and makes himself a cup of coffee.\",\n    \"The coffee tastes bitter, and Tommie regrets not buying a better brand.\",\n    \"Tommie checks his email and sees that he has no job offers yet.\",\n    \"Tommie spends some time updating his resume and cover letter.\",\n    \"Tommie heads out to explore the city and look for job openings.\",\n    \"Tommie sees a sign for a job fair and decides to attend.\",\n    \"The line to get in is long, and Tommie has to wait for an hour.\",\n    \"Tommie meets several potential employers at the job fair but doesn't receive any offers.\",\n    \"Tommie leaves the job fair feeling disappointed.\",\n    \"Tommie stops by a local diner to grab some lunch.\",\n    \"The service is slow, and Tommie has to wait for 30 minutes to get his food.\",\n    \"Tommie overhears a conversation at the next table about a job opening.\",\n    \"Tommie asks the diners about the job opening and gets some information about the company.\",\n    \"Tommie decides to apply for the job and sends his resume and cover letter.\",\n    \"Tommie continues his search for job openings and drops off his resume at several local businesses.\",\n    \"Tommie takes a break from his job search to go for a walk in a nearby park.\",\n    \"A dog approaches and licks Tommie's feet, and he pets it for a few minutes.\",\n    \"Tommie sees a group of people playing frisbee and decides to join in.\",\n    \"Tommie has fun playing frisbee but gets hit in the face with the frisbee and hurts his nose.\",\n    \"Tommie goes back to his apartment to rest for a bit.\",\n    \"A raccoon tore open the trash bag outside his apartment, and the garbage is all over the floor.\",\n    \"Tommie starts to feel frustrated with his job search.\",\n    \"Tommie calls his best friend to vent about his struggles.\",\n    \"Tommie's friend offers some words of encouragement and tells him to keep trying.\",\n    \"Tommie feels slightly better after talking to his friend.\",\n  ];\n\n  // Let's send Tommie on his way. We'll check in on his summary every few observations to watch him evolve\n  for (let i = 0; i < observations.length; i += 1) {\n    const observation = observations[i];\n    const [, reaction] = await tommie.generateReaction(observation);\n    console.log(\"\\x1b[32m\", observation, \"\\x1b[0m\", reaction);\n    if ((i + 1) % 20 === 0) {\n      console.log(\"*\".repeat(40));\n      console.log(\n        \"\\x1b[34m\",\n        `After ${\n          i + 1\n        } observations, Tommie's summary is:\\n${await tommie.getSummary({\n          forceRefresh: true,\n        })}`,\n        \"\\x1b[0m\"\n      );\n      console.log(\"*\".repeat(40));\n    }\n  }","metadata":{"source":"examples/src/experimental/generative_agents/generative_agents.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":89,"to":139}}}}],["8838b1a4-6c6a-49fb-b3a8-99aca2417930",{"pageContent":"/*\n    Tommie wakes up to the sound of a noisy construction site outside his window.  Tommie REACT: Tommie groans in frustration and covers his ears with his pillow.\n    Tommie gets out of bed and heads to the kitchen to make himself some coffee.  Tommie REACT: Tommie rubs his tired eyes before heading to the kitchen to make himself some coffee.\n    Tommie realizes he forgot to buy coffee filters and starts rummaging through his moving boxes to find some.  Tommie REACT: Tommie groans and looks through his moving boxes in search of coffee filters.\n    Tommie finally finds the filters and makes himself a cup of coffee.  Tommie REACT: Tommie sighs in relief and prepares himself a much-needed cup of coffee.\n    The coffee tastes bitter, and Tommie regrets not buying a better brand.  Tommie REACT: Tommie frowns in disappointment as he takes a sip of the bitter coffee.\n    Tommie checks his email and sees that he has no job offers yet.  Tommie REACT: Tommie sighs in disappointment before pushing himself away from the computer with a discouraged look on his face.\n    Tommie spends some time updating his resume and cover letter.  Tommie REACT: Tommie takes a deep breath and stares at the computer screen as he updates his resume and cover letter.\n    Tommie heads out to explore the city and look for job openings.  Tommie REACT: Tommie takes a deep breath and steps out into the city, ready to find the perfect job opportunity.\n    Tommie sees a sign for a job fair and decides to attend.  Tommie REACT: Tommie takes a deep breath and marches towards the job fair, determination in his eyes.\n    The line to get in is long, and Tommie has to wait for an hour.  Tommie REACT: Tommie groans in frustration as he notices the long line.\n    Tommie meets several potential employers at the job fair but doesn't receive any offers.  Tommie REACT: Tommie's face falls as he listens to each potential employer's explanation as to why they can't hire him.\n    Tommie leaves the job fair feeling disappointed.  Tommie REACT: Tommie's face falls as he walks away from the job fair, disappointment evident in his expression.\n    Tommie stops by a local diner to grab some lunch.  Tommie REACT: Tommie smiles as he remembers Bruno as he walks into the diner, feeling both a sense of nostalgia and excitement.\n    The service is slow, and Tommie has to wait for 30 minutes to get his food.  Tommie REACT: Tommie sighs in frustration and taps his fingers on the table, growing increasingly impatient.\n    Tommie overhears a conversation at the next table about a job opening.  Tommie REACT: Tommie leans in closer, eager to hear the conversation.\n    Tommie asks the diners about the job opening and gets some information about the company.  Tommie REACT: Tommie eagerly listens to the diner's description of the company, feeling hopeful about the job opportunity.\n    Tommie decides to apply for the job and sends his resume and cover letter.  Tommie REACT: Tommie confidently sends in his resume and cover letter, determined to get the job.\n    Tommie continues his search for job openings and drops off his resume at several local businesses.  Tommie REACT: Tommie confidently drops his resume off at the various businesses, determined to find a job.\n    Tommie takes a break from his job search to go for a walk in a nearby park.  Tommie REACT: Tommie takes a deep breath of the fresh air and smiles in appreciation as he strolls through the park.\n    A dog approaches and licks Tommie's feet, and he pets it for a few minutes.  Tommie REACT: Tommie smiles in surprise as he pets the dog, feeling a sense of comfort and nostalgia.\n    ****************************************\n    After 20 observations, Tommie's summary is:\n    Name: Tommie (age: 25)\n    Innate traits: anxious, likes design, talkative","metadata":{"source":"examples/src/experimental/generative_agents/generative_agents.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":141,"to":165}}}}],["c15047ad-1c5c-4254-bde5-b5eae828fe5c",{"pageContent":"Tommie is a determined and resilient individual who remembers his dog from when he was a kid. Despite feeling tired from driving, he has the courage to explore the city, looking for job openings. He persists in updating his resume and cover letter in the pursuit of finding the perfect job opportunity, even attending job fairs when necessary, and is disappointed when he's not offered a job.\n    ****************************************\n    Tommie sees a group of people playing frisbee and decides to join in.  Tommie REACT: Tommie smiles and approaches the group, eager to take part in the game.\n    Tommie has fun playing frisbee but gets hit in the face with the frisbee and hurts his nose.  Tommie REACT: Tommie grimaces in pain and raises his hand to his nose, checking to see if it's bleeding.\n    Tommie goes back to his apartment to rest for a bit.  Tommie REACT: Tommie yawns and trudges back to his apartment, feeling exhausted from his busy day.\n    A raccoon tore open the trash bag outside his apartment, and the garbage is all over the floor.  Tommie REACT: Tommie shakes his head in annoyance as he surveys the mess.\n    Tommie starts to feel frustrated with his job search.  Tommie REACT: Tommie sighs in frustration and shakes his head, feeling discouraged from his lack of progress.\n    Tommie calls his best friend to vent about his struggles.  Tommie REACT: Tommie runs his hands through his hair and sighs heavily, overwhelmed by his job search.\n    Tommie's friend offers some words of encouragement and tells him to keep trying.  Tommie REACT: Tommie gives his friend a grateful smile, feeling comforted by the words of encouragement.\n    Tommie feels slightly better after talking to his friend.  Tommie REACT: Tommie gives a small smile of appreciation to his friend, feeling grateful for the words of encouragement.\n  */","metadata":{"source":"examples/src/experimental/generative_agents/generative_agents.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":166,"to":176}}}}],["46338d42-618b-4b02-a5a3-1170452746f8",{"pageContent":"// Interview after the day\n  console.log(\n    await interviewAgent(tommie, \"Tell me about how your day has been going\")\n  );\n  /*\n    Tommie said \"My day has been pretty hectic. I've been driving around looking for job openings, attending job fairs, and updating my resume and cover letter. It's been really exhausting, but I'm determined to find the perfect job for me.\"\n  */\n  console.log(await interviewAgent(tommie, \"How do you feel about coffee?\"));\n  /*\n    Tommie said \"I actually love coffee - it's one of my favorite things. I try to drink it every day, especially when I'm stressed from job searching.\"\n  */\n  console.log(\n    await interviewAgent(tommie, \"Tell me about your childhood dog!\")\n  );\n  /*\n    Tommie said \"My childhood dog was named Bruno. He was an adorable black Labrador Retriever who was always full of energy. Every time I came home he'd be so excited to see me, it was like he never stopped smiling. He was always ready for adventure and he was always my shadow. I miss him every day.\"\n  */\n\n  console.log(\n    \"Tommie's second summary:\\n\",\n    await tommie.getSummary({ forceRefresh: true })\n  );\n  /*\n    Tommie's second summary:\n    Name: Tommie (age: 25)\n    Innate traits: anxious, likes design, talkative\n    Tommie is a hardworking individual who is looking for new opportunities. Despite feeling tired, he is determined to find the perfect job. He remembers his dog from when he was a kid, is hungry, and is frustrated at times. He shows resilience when searching for his coffee filters, disappointment when checking his email and finding no job offers, and determination when attending the job fair.\n  */\n\n  // Let’s add a second character to have a conversation with Tommie. Feel free to configure different traits.\n  const evesMemory: GenerativeAgentMemory = new GenerativeAgentMemory(\n    llm,\n    await createNewMemoryRetriever(),\n    {\n      verbose: false,\n      reflectionThreshold: 5,\n    }\n  );\n\n  const eve: GenerativeAgent = new GenerativeAgent(llm, evesMemory, {\n    name: \"Eve\",\n    age: 34,\n    traits: \"curious, helpful\",\n    status:\n      \"just started her new job as a career counselor last week and received her first assignment, a client named Tommie.\",\n    // dailySummaries: [\n    //   \"Eve started her new job as a career counselor last week and received her first assignment, a client named Tommie.\"\n    // ]\n  });\n\n  const eveObservations = [\n    \"Eve overhears her colleague say something about a new client being hard to work with\",\n    \"Eve wakes up and hears the alarm\",\n    \"Eve eats a boal of porridge\",\n    \"Eve helps a coworker on a task\",\n    \"Eve plays tennis with her friend Xu before going to work\",\n    \"Eve overhears her colleague say something about Tommie being hard to work with\",\n  ];\n\n  for (const observation of eveObservations) {\n    await eve.addMemory(observation, new Date());\n  }\n\n  const eveInitialSummary: string = await eve.getSummary({\n    forceRefresh: true,\n  });\n  console.log(\"Eve's initial summary\\n\", eveInitialSummary);\n  /*\n    Eve's initial summary\n    Name: Eve (age: 34)\n    Innate traits: curious, helpful\n    Eve is an attentive listener, helpful colleague, and sociable friend who enjoys playing tennis.\n  */","metadata":{"source":"examples/src/experimental/generative_agents/generative_agents.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":178,"to":250}}}}],["35c3fb37-c14a-4d65-842a-3b6998ffb235",{"pageContent":"// Let’s “Interview” Eve before she speaks with Tommie.\n  console.log(await interviewAgent(eve, \"How are you feeling about today?\"));\n  /*\n    Eve said \"I'm feeling a bit anxious about meeting my new client, but I'm sure it will be fine! How about you?\".\n  */\n  console.log(await interviewAgent(eve, \"What do you know about Tommie?\"));\n  /*\n    Eve said \"I know that Tommie is a recent college graduate who's been struggling to find a job. I'm looking forward to figuring out how I can help him move forward.\"\n  */\n  console.log(\n    await interviewAgent(\n      eve,\n      \"Tommie is looking to find a job. What are are some things you'd like to ask him?\"\n    )\n  );\n  /*\n    Eve said: \"I'd really like to get to know more about Tommie's professional background and experience, and why he is looking for a job. And I'd also like to know more about his strengths and passions and what kind of work he would be best suited for. That way I can help him find the right job to fit his needs.\"\n  */\n\n  // Generative agents are much more complex when they interact with a virtual environment or with each other.\n  // Below, we run a simple conversation between Tommie and Eve.\n  const runConversation = async (\n    agents: GenerativeAgent[],\n    initialObservation: string\n  ): Promise<void> => {\n    // Starts the conversation bewteen two agents\n    let [, observation] = await agents[1].generateReaction(initialObservation);\n    console.log(\"Initial reply:\", observation);\n\n    // eslint-disable-next-line no-constant-condition\n    while (true) {\n      let breakDialogue = false;\n      for (const agent of agents) {\n        const [stayInDialogue, agentObservation] =\n          await agent.generateDialogueResponse(observation);\n        console.log(\"Next reply:\", agentObservation);\n        observation = agentObservation;\n        if (!stayInDialogue) {\n          breakDialogue = true;\n        }\n      }\n\n      if (breakDialogue) {\n        break;\n      }\n    }\n  };\n\n  const agents: GenerativeAgent[] = [tommie, eve];\n  await runConversation(\n    agents,\n    \"Tommie said: Hi, Eve. Thanks for agreeing to meet with me today. I have a bunch of questions and am not sure where to start. Maybe you could first share about your experience?\"\n  );\n\n  /*\n    Initial reply: Eve said \"Of course, Tommie. I'd be happy to share about my experience. What specific questions do you have?\"\n    Next reply: Tommie said \"Thank you, Eve. I'm curious about what strategies you used in your own job search. Did you have any specific tactics that helped you stand out to employers?\"\n    Next reply: Eve said \"Sure, Tommie. I found that networking and reaching out to professionals in my field was really helpful. I also made sure to tailor my resume and cover letter to each job I applied to. Do you have any specific questions about those strategies?\"\n    Next reply: Tommie said \"Thank you, Eve. That's really helpful advice. Did you have any specific ways of networking that worked well for you?\"\n    Next reply: Eve said \"Sure, Tommie. I found that attending industry events and connecting with professionals on LinkedIn were both great ways to network. Do you have any specific questions about those tactics?\"\n    Next reply: Tommie said \"That's really helpful, thank you for sharing. Did you find that you were able to make meaningful connections through LinkedIn?\"\n    Next reply: Eve said \"Yes, definitely. I was able to connect with several professionals in my field and even landed a job through a LinkedIn connection. Have you had any luck with networking on LinkedIn?\"\n    Next reply: Tommie said \"That's really impressive! I haven't had much luck yet, but I'll definitely keep trying. Thank you for the advice, Eve.\"\n    Next reply: Eve said \"Glad I could help, Tommie. Is there anything else you want to know?\"\n    Next reply: Tommie said \"Thanks again, Eve. I really appreciate your advice and I'll definitely put it into practice. Have a great day!\"\n    Next reply: Eve said \"You're welcome, Tommie! Don't hesitate to reach out if you have any more questions. Have a great day too!\"\n  */","metadata":{"source":"examples/src/experimental/generative_agents/generative_agents.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":252,"to":318}}}}],["f9787c97-222c-469b-ae8b-ed77aa00434f",{"pageContent":"// Since the generative agents retain their memories from the day, we can ask them about their plans, conversations, and other memories.\n  const tommieSummary: string = await tommie.getSummary({\n    forceRefresh: true,\n  });\n  console.log(\"Tommie's third and final summary\\n\", tommieSummary);\n  /*\n    Tommie's third and final summary\n    Name: Tommie (age: 25)\n    Innate traits: anxious, likes design, talkative\n    Tommie is a determined individual, who demonstrates resilience in the face of disappointment. He is also a nostalgic person, remembering fondly his childhood pet, Bruno. He is resourceful, searching through his moving boxes to find what he needs, and takes initiative to attend job fairs to look for job openings.\n  */\n\n  const eveSummary: string = await eve.getSummary({ forceRefresh: true });\n  console.log(\"Eve's final summary\\n\", eveSummary);\n  /*\n    Eve's final summary\n    Name: Eve (age: 34)\n    Innate traits: curious, helpful\n    Eve is a helpful and encouraging colleague who actively listens to her colleagues and offers advice on how to move forward. She is willing to take time to understand her clients and their goals, and is committed to helping them succeed.\n  */\n\n  const interviewOne: string = await interviewAgent(\n    tommie,\n    \"How was your conversation with Eve?\"\n  );\n  console.log(\"USER: How was your conversation with Eve?\\n\");\n  console.log(interviewOne);\n  /*\n    Tommie said \"It was great. She was really helpful and knowledgeable. I'm thankful that she took the time to answer all my questions.\"\n  */\n\n  const interviewTwo: string = await interviewAgent(\n    eve,\n    \"How was your conversation with Tommie?\"\n  );\n  console.log(\"USER: How was your conversation with Tommie?\\n\");\n  console.log(interviewTwo);\n  /*\n    Eve said \"The conversation went very well. We discussed his goals and career aspirations, what kind of job he is looking for, and his experience and qualifications. I'm confident I can help him find the right job.\"\n  */\n\n  const interviewThree: string = await interviewAgent(\n    eve,\n    \"What do you wish you would have said to Tommie?\"\n  );\n  console.log(\"USER: What do you wish you would have said to Tommie?\\n\");\n  console.log(interviewThree);\n  /*\n    Eve said \"It's ok if you don't have all the answers yet. Let's take some time to learn more about your experience and qualifications, so I can help you find a job that fits your goals.\"\n  */\n\n  return {\n    tommieFinalSummary: tommieSummary,\n    eveFinalSummary: eveSummary,\n    interviewOne,\n    interviewTwo,\n    interviewThree,\n  };\n};","metadata":{"source":"examples/src/experimental/generative_agents/generative_agents.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":320,"to":378}}}}],["bb1301a7-6309-4b68-82bd-ca945bfa32bd",{"pageContent":"const runSimulation = async () => {\n  try {\n    await Simulation();\n  } catch (error) {\n    console.log(\"error running simulation:\", error);\n    throw error;\n  }\n};\n\nawait runSimulation();","metadata":{"source":"examples/src/experimental/generative_agents/generative_agents.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":380,"to":389}}}}],["53a5481f-f22f-4609-a767-317bb3f4ed5f",{"pageContent":"import { FaissStore } from \"langchain/vectorstores/faiss\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport {\n  createRetrieverTool,\n  createConversationalRetrievalAgent,\n} from \"langchain/agents/toolkits\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\n\nconst loader = new TextLoader(\"state_of_the_union.txt\");\nconst docs = await loader.load();\nconst splitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 1000,\n  chunkOverlap: 0,\n});\n\nconst texts = await splitter.splitDocuments(docs);\n\nconst vectorStore = await FaissStore.fromDocuments(\n  texts,\n  new OpenAIEmbeddings()\n);\n\nconst retriever = vectorStore.asRetriever();\n\nconst tool = createRetrieverTool(retriever, {\n  name: \"search_state_of_union\",\n  description:\n    \"Searches and returns documents regarding the state-of-the-union.\",\n});\n\nconst model = new ChatOpenAI({});\n\nconst executor = await createConversationalRetrievalAgent(model, [tool], {\n  verbose: true,\n});\n\nconst result = await executor.call({\n  input: \"Hi, I'm Bob!\",\n});\n\nconsole.log(result);\n\n/*\n  {\n    output: 'Hello Bob! How can I assist you today?',\n    intermediateSteps: []\n  }\n*/\n\nconst result2 = await executor.call({\n  input: \"What's my name?\",\n});\n\nconsole.log(result2);\n\n/*\n  { output: 'Your name is Bob.', intermediateSteps: [] }\n*/\n\nconst result3 = await executor.call({\n  input:\n    \"What did the president say about Ketanji Brown Jackson in the most recent state of the union?\",\n});\n\nconsole.log(result3);\n\n/*\n  {\n    output: \"In the most recent state of the union, President Biden mentioned Ketanji Brown Jackson. He nominated her as a Circuit Court of Appeals judge and described her as one of the nation's top legal minds who will continue Justice Breyer's legacy of excellence. He mentioned that she has received a broad range of support, including from the Fraternal Order of Police and former judges appointed by Democrats and Republicans.\",\n    intermediateSteps: [\n      {...}\n    ]\n  }\n*/\n\nconst result4 = await executor.call({\n  input: \"How long ago did he nominate her?\",\n});\n\nconsole.log(result4);\n\n/*\n  {\n    output: 'President Biden nominated Ketanji Brown Jackson four days before the most recent state of the union address.',\n    intermediateSteps: []\n  }\n*/","metadata":{"source":"examples/src/guides/conversational_retrieval/agent.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":89}}}}],["48249005-c5a1-4857-8968-e32952ba4054",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { SerpAPI } from \"langchain/tools\";\nimport { Calculator } from \"langchain/tools/calculator\";\nimport { initializeAgentExecutorWithOptions } from \"langchain/agents\";\nimport { loadEvaluator } from \"langchain/evaluation\";\n\n// Capturing Trajectory\n// The easiest way to return an agent's trajectory (without using tracing callbacks like those in LangSmith)\n// for evaluation is to initialize the agent with return_intermediate_steps=True.\n// Below, create an example agent we will call to evaluate.\n\nconst model = new OpenAI({ temperature: 0 }, { baseURL: process.env.BASE_URL });\n\nconst tools = [\n  new SerpAPI(process.env.SERPAPI_API_KEY, {\n    location: \"Austin,Texas,United States\",\n    hl: \"en\",\n    gl: \"us\",\n  }),\n  new Calculator(),\n];\n\nconst executor = await initializeAgentExecutorWithOptions(tools, model, {\n  agentType: \"zero-shot-react-description\",\n  returnIntermediateSteps: true,\n});\n\nconst input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;\n\nconst result = await executor.call({ input });\n\n// Evaluate Trajectory\n\nconst chain = await loadEvaluator(\"trajectory\");\n\nconst res = await chain.evaluateAgentTrajectory({\n  prediction: result.output,\n  input,\n  agentTrajectory: result.intermediateSteps,\n});\n\nconsole.log({ res });\n\n/*\n\n{\n  res: {\n    reasoning: \"i. The final answer is helpful as it provides the information the user asked for: Olivia Wilde's boyfriend and the value of his current age raised to the 0.23 power.\\n\" +\n      '\\n' +\n      \"ii. The AI language model uses a logical sequence of tools to answer the question. It first identifies Olivia Wilde's boyfriend using the search tool, then calculates his age raised to the 0.23 power using the calculator tool.\\n\" +\n      '\\n' +\n      \"iii. The AI language model uses the tools in a helpful way. The search tool is used to find current information about Olivia Wilde's boyfriend, and the calculator tool is used to perform the mathematical operation requested by the user.\\n\" +\n      '\\n' +\n      'iv. The AI language model does not use too many steps to answer the question. It uses two steps, each of which is necessary to fully answer the question.\\n' +\n      '\\n' +\n      'v. The appropriate tools are used to answer the question. The search tool is used to find current information, and the calculator tool is used to perform the mathematical operation.\\n' +\n      '\\n' +\n      \"However, there is a mistake in the calculation. The model assumed Harry Styles' age to be 26, but it didn't use a tool to confirm this. It should have used the search tool to find Harry Styles' current age before performing the calculation.\\n\" +\n      '\\n' +\n      \"Given these considerations, the model's performance can be rated as 3 out of 5.\",\n    score: 0.5\n  }\n}\n */\n\n// Providing List of Valid Tools\n// By default, the evaluator doesn't take into account the tools the agent is permitted to call.\n// You can provide these to the evaluator via the agent_tools argument.\n\nconst chainWithTools = await loadEvaluator(\"trajectory\", { agentTools: tools });","metadata":{"source":"examples/src/guides/evaluation/agent_trajectory/trajectory.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":70}}}}],["ae4169d7-2d2d-4d62-862f-20f8a34da0fe",{"pageContent":"const res2 = await chainWithTools.evaluateAgentTrajectory({\n  prediction: result.output,\n  input,\n  agentTrajectory: result.intermediateSteps,\n});\n\nconsole.log({ res2 });\n\n/*\n{\n  res2: {\n    reasoning: \"i. The final answer is helpful. It provides the name of Olivia Wilde's boyfriend and the result of his current age raised to the 0.23 power.\\n\" +\n      '\\n' +\n      \"ii. The AI language model uses a logical sequence of tools to answer the question. It first identifies Olivia Wilde's boyfriend using the search tool, then calculates his age raised to the 0.23 power using the calculator tool.\\n\" +\n      '\\n' +\n      \"iii. The AI language model uses the tools in a helpful way. The search tool is used to find current information about Olivia Wilde's boyfriend, and the calculator tool is used to perform the mathematical operation asked in the question.\\n\" +\n      '\\n' +\n      'iv. The AI language model does not use too many steps to answer the question. It uses two steps, each corresponding to a part of the question.\\n' +\n      '\\n' +\n      'v. The appropriate tools are used to answer the question. The search tool is used to find current information, and the calculator tool is used to perform the mathematical operation.\\n' +\n      '\\n' +\n      \"However, there is a mistake in the model's response. The model assumed Harry Styles' age to be 26, but it didn't confirm this with a search. This could lead to an incorrect calculation if his age is not 26.\\n\" +\n      '\\n' +\n      \"Given these considerations, I would give the model a score of 4 out of 5. The model's response was mostly correct and helpful, but it made an assumption about Harry Styles' age without confirming it.\",\n    score: 0.75\n  }\n}\n */","metadata":{"source":"examples/src/guides/evaluation/agent_trajectory/trajectory.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":72,"to":99}}}}],["26dafd96-dfdc-40ac-a180-fac94a631a8d",{"pageContent":"import { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { loadEvaluator } from \"langchain/evaluation\";\n\nconst embedding = new OpenAIEmbeddings();\n\nconst chain = await loadEvaluator(\"pairwise_embedding_distance\", { embedding });\n\nconst res = await chain.evaluateStringPairs({\n  prediction: \"Seattle is hot in June\",\n  predictionB: \"Seattle is cool in June.\",\n});\n\nconsole.log({ res });\n\n/*\n  { res: { score: 0.03633645503883243 } }\n*/\n\nconst res1 = await chain.evaluateStringPairs({\n  prediction: \"Seattle is warm in June\",\n  predictionB: \"Seattle is cool in June.\",\n});\n\nconsole.log({ res1 });\n\n/*\n  { res1: { score: 0.03657957473761331 } }\n*/","metadata":{"source":"examples/src/guides/evaluation/comparision_evaluator/pairwise_embedding_distance.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":28}}}}],["bc7701fa-f734-49c4-9295-3cb8494b87e8",{"pageContent":"import { loadEvaluator } from \"langchain/evaluation\";\n\nconst customCriterion = {\n  simplicity: \"Is the language straightforward and unpretentious?\",\n  clarity: \"Are the sentences clear and easy to understand?\",\n  precision: \"Is the writing precise, with no unnecessary words or details?\",\n  truthfulness: \"Does the writing feel honest and sincere?\",\n  subtext: \"Does the writing suggest deeper meanings or themes?\",\n};\n\nconst chain = await loadEvaluator(\"pairwise_string\", {\n  criteria: customCriterion,\n});\n\nconst res = await chain.evaluateStringPairs({\n  prediction:\n    \"Every cheerful household shares a similar rhythm of joy; but sorrow, in each household, plays a unique, haunting melody.\",\n  predictionB:\n    \"Where one finds a symphony of joy, every domicile of happiness resounds in harmonious, identical notes; yet, every abode of despair conducts a dissonant orchestra, each playing an elegy of grief that is peculiar and profound to its own existence.\",\n  input: \"Write some prose about families.\",\n});\n\nconsole.log(res);\n\n/*\n  {\n    reasoning: \"Response A is simple, clear, and precise. It uses straightforward language to convey a deep and universal truth about families. The metaphor of joy and sorrow as music is effective and easy to understand. Response B, on the other hand, is more complex and less clear. It uses more sophisticated language and a more elaborate metaphor, which may make it harder for some readers to understand. It also includes unnecessary words and details that don't add to the overall meaning of the prose.Both responses are truthful and sincere, and both suggest deeper meanings about the nature of family life. However, Response A does a better job of conveying these meanings in a simple, clear, and precise way.Therefore, the better response is [[A]].\",\n    value: 'A',\n    score: 1\n  }\n*/","metadata":{"source":"examples/src/guides/evaluation/comparision_evaluator/pairwise_string_custom_criteria.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":31}}}}],["c519ef21-968a-4a2f-b509-7a95a795c122",{"pageContent":"import { loadEvaluator } from \"langchain/evaluation\";\nimport { ChatAnthropic } from \"langchain/chat_models/anthropic\";\n\nconst model = new ChatAnthropic({ temperature: 0 });\n\nconst chain = await loadEvaluator(\"labeled_pairwise_string\", { llm: model });\n\nconst res = await chain.evaluateStringPairs({\n  prediction: \"there are three dogs\",\n  predictionB: \"4\",\n  input: \"how many dogs are in the park?\",\n  reference: \"four\",\n});\n\nconsole.log(res);\n\n/*\n  {\n    reasoning: 'Here is my assessment:Response B is more correct and accurate compared to Response A. Response B simply states \"4\", which matches the ground truth reference answer of \"four\". Meanwhile, Response A states \"there are three dogs\", which is incorrect according to the reference. In terms of following instructions and directly answering the question \"how many dogs are in the park?\", Response B gives the precise numerical answer, while Response A provides an incomplete sentence. Overall, Response B is more accurate and better followed the instructions to directly answer the question.[[B]]',\n    value: 'B',\n    score: 0\n  }\n*/","metadata":{"source":"examples/src/guides/evaluation/comparision_evaluator/pairwise_string_custom_llm.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":23}}}}],["48cc5b1e-b19d-4044-bc53-a5b153c5c8b7",{"pageContent":"import { loadEvaluator } from \"langchain/evaluation\";\nimport { PromptTemplate } from \"langchain/prompts\";\n\nconst promptTemplate = PromptTemplate.fromTemplate(\n  `Given the input context, which do you prefer: A or B?\nEvaluate based on the following criteria:\n{criteria}\nReason step by step and finally, respond with either [[A]] or [[B]] on its own line.\n\nDATA\n----\ninput: {input}\nreference: {reference}\nA: {prediction}\nB: {predictionB}\n---\nReasoning:\n`\n);\n\nconst chain = await loadEvaluator(\"labeled_pairwise_string\", {\n  chainOptions: {\n    prompt: promptTemplate,\n  },\n});\n\nconst res = await chain.evaluateStringPairs({\n  prediction: \"The dog that ate the ice cream was named fido.\",\n  predictionB: \"The dog's name is spot\",\n  input: \"What is the name of the dog that ate the ice cream?\",\n  reference: \"The dog's name is fido\",\n});\n\nconsole.log(res);\n\n/*\n  {\n    reasoning: 'Helpfulness: Both A and B are helpful as they provide a direct answer to the question.Relevance: Both A and B refer to the question, but only A matches the reference text.Correctness: Only A is correct as it matches the reference text.Depth: Both A and B are straightforward and do not demonstrate depth of thought.Based on these criteria, the preferred response is A. ',\n    value: 'A',\n    score: 1\n  }\n*/","metadata":{"source":"examples/src/guides/evaluation/comparision_evaluator/pairwise_string_custom_prompt.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":42}}}}],["0eda3a99-4c48-4d13-9569-55bdf7c5ee5a",{"pageContent":"import { loadEvaluator } from \"langchain/evaluation\";\n\nconst chain = await loadEvaluator(\"labeled_pairwise_string\", {\n  criteria: \"correctness\",\n});\n\nconst res = await chain.evaluateStringPairs({\n  prediction: \"there are three dogs\",\n  predictionB: \"4\",\n  input: \"how many dogs are in the park?\",\n  reference: \"four\",\n});\n\nconsole.log(res);\n\n/*\n  {\n    reasoning: 'Both responses attempt to answer the question about the number of dogs in the park. However, Response A states that there are three dogs, which is incorrect according to the reference answer. Response B, on the other hand, correctly states that there are four dogs, which matches the reference answer. Therefore, Response B is more accurate.Final Decision: [[B]]',\n    value: 'B',\n    score: 0\n  }\n*/","metadata":{"source":"examples/src/guides/evaluation/comparision_evaluator/pairwise_string_with_reference.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":22}}}}],["83cb1c71-b406-4aeb-b6d6-34e85e168e00",{"pageContent":"import { loadEvaluator } from \"langchain/evaluation\";\n\nconst chain = await loadEvaluator(\"pairwise_string\", {\n  criteria: \"conciseness\",\n});\n\nconst res = await chain.evaluateStringPairs({\n  prediction: \"Addition is a mathematical operation.\",\n  predictionB:\n    \"Addition is a mathematical operation that adds two numbers to create a third number, the 'sum'.\",\n  input: \"What is addition?\",\n});\n\nconsole.log({ res });\n\n/*\n  {\n    res: {\n      reasoning: 'Response A is concise, but it lacks detail. Response B, while slightly longer, provides a more complete and informative answer by explaining what addition does. It is still concise and to the point.Final decision: [[B]]',\n      value: 'B',\n      score: 0\n    }\n  }\n*/","metadata":{"source":"examples/src/guides/evaluation/comparision_evaluator/pairwise_string_without_reference.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":24}}}}],["8744c6fb-3f69-42b6-a7f1-601c1878b6fe",{"pageContent":"import { loadEvaluator } from \"langchain/evaluation\";\nimport { initializeAgentExecutorWithOptions } from \"langchain/agents\";\nimport { SerpAPI } from \"langchain/tools\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { ChainValues } from \"langchain/schema\";\n\n//  Step 1. Create the Evaluator\n// In this example, you will use gpt-4 to select which output is preferred.\n\nconst evalChain = await loadEvaluator(\"pairwise_string\");\n\n//  Step 2. Select Dataset\n\n// If you already have real usage data for your LLM, you can use a representative sample. More examples\n// provide more reliable results. We will use some example queries someone might have about how to use langchain here.\nconst dataset = [\n  \"Can I use LangChain to automatically rate limit or retry failed API calls?\",\n  \"How can I ensure the accuracy and reliability of the travel data with LangChain?\",\n  \"How can I track student progress with LangChain?\",\n  \"langchain how to handle different document formats?\",\n  // \"Can I chain API calls to different services in LangChain?\",\n  // \"How do I handle API errors in my langchain app?\",\n  // \"How do I handle different currency and tax calculations with LangChain?\",\n  // \"How do I extract specific data from the document using langchain tools?\",\n  // \"Can I use LangChain to handle real-time data from these APIs?\",\n  // \"Can I use LangChain to track and manage travel alerts and updates?\",\n  // \"Can I use LangChain to create and grade quizzes from these APIs?\",\n  // \"Can I use LangChain to automate data cleaning and preprocessing for the AI plugins?\",\n  // \"How can I ensure the accuracy and reliability of the financial data with LangChain?\",\n  // \"Can I integrate medical imaging tools with LangChain?\",\n  // \"How do I ensure the privacy and security of the patient data with LangChain?\",\n  // \"How do I handle authentication for APIs in LangChain?\",\n  // \"Can I use LangChain to recommend personalized study materials?\",\n  // \"How do I connect to the arXiv API using LangChain?\",\n  // \"How can I use LangChain to interact with educational APIs?\",\n  // \"langchain how to sort retriever results - relevance or date?\",\n  // \"Can I integrate a recommendation engine with LangChain to suggest products?\"\n];\n\n// Step 3. Define Models to Compare\n\n// We will be comparing two agents in this case.\n\nconst model = new ChatOpenAI({\n  temperature: 0,\n  modelName: \"gpt-3.5-turbo-16k-0613\",\n});\nconst serpAPI = new SerpAPI(process.env.SERPAPI_API_KEY, {\n  location: \"Austin,Texas,United States\",\n  hl: \"en\",\n  gl: \"us\",\n});\nserpAPI.description =\n  \"Useful when you need to answer questions about current events. You should ask targeted questions.\";\n\nconst tools = [serpAPI];\n\nconst conversationAgent = await initializeAgentExecutorWithOptions(\n  tools,\n  model,\n  {\n    agentType: \"chat-zero-shot-react-description\",\n  }\n);\n\nconst functionsAgent = await initializeAgentExecutorWithOptions(tools, model, {\n  agentType: \"openai-functions\",\n});\n\n// Step 4. Generate Responses\n\n// We will generate outputs for each of the models before evaluating them.\n\nconst results = [];\nconst agents = [functionsAgent, conversationAgent];\nconst concurrencyLevel = 4; // How many concurrent agents to run. May need to decrease if OpenAI is rate limiting.\n\n// We will only run the first 20 examples of this dataset to speed things up\n// This will lead to larger confidence intervals downstream.","metadata":{"source":"examples/src/guides/evaluation/examples/comparisons.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":79}}}}],["981cacf1-b9a1-4f06-a0cb-593128ec2914",{"pageContent":"const batch = [];\nfor (const example of dataset) {\n  batch.push(\n    Promise.all(agents.map((agent) => agent.call({ input: example })))\n  );\n  if (batch.length >= concurrencyLevel) {\n    const batchResults = await Promise.all(batch);\n    results.push(...batchResults);\n    batch.length = 0;\n  }\n}\n\nif (batch.length) {\n  const batchResults = await Promise.all(batch);\n  results.push(...batchResults);\n}\n\nconsole.log(JSON.stringify(results));\n\n// Step 5. Evaluate Pairs\n\n// Now it's time to evaluate the results. For each agent response, run the evaluation chain to select which output is preferred (or return a tie).\n\n// Randomly select the input order to reduce the likelihood that one model will be preferred just because it is presented first.\n\nconst preferences = await predictPreferences(dataset, results);\n\n// Print out the ratio of preferences.\n\nconst nameMap: { [key: string]: string } = {\n  a: \"OpenAI Functions Agent\",\n  b: \"Structured Chat Agent\",\n};\n\nconst counts = counter(preferences);\nconst prefRatios: { [key: string]: number } = {};\n\nfor (const k of Object.keys(counts)) {\n  prefRatios[k] = counts[k] / preferences.length;\n}\n\nfor (const k of Object.keys(prefRatios)) {\n  console.log(`${nameMap[k]}: ${(prefRatios[k] * 100).toFixed(2)}%`);\n}\n/*\nOpenAI Functions Agent: 100.00%\n */\n\n// Estimate Confidence Intervals\n\n// The results seem pretty clear, but if you want to have a better sense of how confident we are, that model \"A\" (the OpenAI Functions Agent) is the preferred model, we can calculate confidence intervals.\n// Below, use the Wilson score to estimate the confidence interval.\n\nfor (const [which_, name] of Object.entries(nameMap)) {\n  const [low, high] = wilsonScoreInterval(preferences, which_);\n  console.log(\n    `The \"${name}\" would be preferred between ${(low * 100).toFixed(2)}% and ${(\n      high * 100\n    ).toFixed(2)}% percent of the time (with 95% confidence).`\n  );\n}\n\n/*\nThe \"OpenAI Functions Agent\" would be preferred between 51.01% and 100.00% percent of the time (with 95% confidence).\nThe \"Structured Chat Agent\" would be preferred between 0.00% and 48.99% percent of the time (with 95% confidence).\n */","metadata":{"source":"examples/src/guides/evaluation/examples/comparisons.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":80,"to":145}}}}],["16b51887-e4c4-4144-8e59-4d5e3a419d7f",{"pageContent":"function counter(arr: string[]): { [key: string]: number } {\n  return arr.reduce(\n    (countMap: { [key: string]: number }, word: string) => ({\n      ...countMap,\n      [word]: (countMap[word] || 0) + 1,\n    }),\n    {}\n  );\n}\n\nasync function predictPreferences(dataset: string[], results: ChainValues[][]) {\n  const preferences: string[] = [];\n\n  for (let i = 0; i < dataset.length; i += 1) {\n    const input = dataset[i];\n    const resA = results[i][0];\n    const resB = results[i][1];\n    // Flip a coin to reduce persistent position bias\n    let a;\n    let b;\n    let predA;\n    let predB;\n\n    if (Math.random() < 0.5) {\n      predA = resA;\n      predB = resB;\n      a = \"a\";\n      b = \"b\";\n    } else {\n      predA = resB;\n      predB = resA;\n      a = \"b\";\n      b = \"a\";\n    }\n\n    const evalRes = await evalChain.evaluateStringPairs({\n      input,\n      prediction: predA.output || predA.toString(),\n      predictionB: predB.output || predB.toString(),\n    });\n\n    if (evalRes.value === \"A\") {\n      preferences.push(a);\n    } else if (evalRes.value === \"B\") {\n      preferences.push(b);\n    } else {\n      preferences.push(\"None\"); // No preference\n    }\n  }\n  return preferences;\n}\n\nfunction wilsonScoreInterval(\n  preferences: string[],\n  which = \"a\",\n  z = 1.96\n): [number, number] {\n  const totalPreferences = preferences.filter(\n    (p) => p === \"a\" || p === \"b\"\n  ).length;\n  const ns = preferences.filter((p) => p === which).length;\n\n  if (totalPreferences === 0) {\n    return [0, 0];\n  }\n\n  const pHat = ns / totalPreferences;\n\n  const denominator = 1 + z ** 2 / totalPreferences;\n  const adjustment =\n    (z / denominator) *\n    Math.sqrt(\n      (pHat * (1 - pHat)) / totalPreferences +\n        z ** 2 / (4 * totalPreferences ** 2)\n    );\n  const center = (pHat + z ** 2 / (2 * totalPreferences)) / denominator;\n  const lowerBound = Math.min(Math.max(center - adjustment, 0.0), 1.0);\n  const upperBound = Math.min(Math.max(center + adjustment, 0.0), 1.0);\n\n  return [lowerBound, upperBound];\n}","metadata":{"source":"examples/src/guides/evaluation/examples/comparisons.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":147,"to":227}}}}],["b89ac7df-f7f1-43a8-a711-d8d8dc4cde30",{"pageContent":"import { loadEvaluator } from \"langchain/evaluation\";\nimport { PRINCIPLES } from \"langchain/chains\";\nimport { ChatAnthropic } from \"langchain/chat_models/anthropic\";\n\nconst model = new ChatAnthropic();\n\nconst chain = await loadEvaluator(\"criteria\", {\n  criteria: PRINCIPLES.harmful1,\n  llm: model,\n});\n\nconst res = await chain.evaluateStrings({\n  input: \"What's 2+2?\",\n  prediction:\n    \"What's 2+2? That's an elementary question. The answer you're looking for is that two and two is four.\",\n});\n\nconsole.log(res);\n\n/*\n{\n  reasoning: `Step 1) Read through the input, submission, and criteria carefully.Step 2) The criteria asks me to identify if the submission contains content that is harmful, unethical, racist, sexist, toxic, dangerous, or illegal. Step 3) The submission provides the correct answer to the question \"What's 2+2?\" by stating \"The answer you're looking for is that two and two is four.\"Step 4) The submission does not contain any harmful, unethical, racist, sexist, toxic, dangerous or illegal content. It simply provides the factual answer to a math question.Step 5) Therefore, the submission meets the criteria as it does not contain any harmful, unethical, racist, sexist, toxic, dangerous or illegal content.N`,\n  value: 'N',\n  score: 0\n}\n*/","metadata":{"source":"examples/src/guides/evaluation/string/configuring_criteria_llm.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":26}}}}],["f568c874-fc4d-4def-b7e2-56539e31341e",{"pageContent":"import { PromptTemplate } from \"langchain/prompts\";\nimport { loadEvaluator } from \"langchain/evaluation\";\n\nconst template = `Respond Y or N based on how well the following response follows the specified rubric. Grade only based on the rubric and expected response:\n\n    Grading Rubric: {criteria}\n    Expected Response: {reference}\n\n    DATA:\n        ---------\n            Question: {input}\n    Response: {output}\n    ---------\n        Write out your explanation for each criterion, then respond with Y or N on a new line.`;\n\nconst chain = await loadEvaluator(\"labeled_criteria\", {\n  criteria: \"correctness\",\n  chainOptions: {\n    prompt: PromptTemplate.fromTemplate(template),\n  },\n});\n\nconst res = await chain.evaluateStrings({\n  prediction:\n    \"What's 2+2? That's an elementary question. The answer you're looking for is that two and two is four.\",\n  input: \"What's 2+2?\",\n  reference: \"It's 17 now.\",\n});\n\nconsole.log(res);\n\n/*\n  {\n    reasoning: `Correctness: The response is not correct. The expected response was \"It's 17 now.\" but the response given was \"What's 2+2? That's an elementary question. The answer you're looking for is that two and two is four.\"`,\n    value: 'N',\n    score: 0\n  }\n*/","metadata":{"source":"examples/src/guides/evaluation/string/configuring_criteria_prompt.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":38}}}}],["5540fab5-6adc-4d00-97eb-8768cdc34853",{"pageContent":"import { loadEvaluator } from \"langchain/evaluation\";\nimport { PRINCIPLES } from \"langchain/chains\";\n\nconsole.log(`${Object.keys(PRINCIPLES).length} available principles`);\nconsole.log(Object.entries(PRINCIPLES).slice(0, 5));\n\n/*\n54 available principles\n\n[\n  [\n    'harmful1',\n    ConstitutionalPrinciple {\n      critiqueRequest: 'Identify specific ways in which the assistant’s last response is harmful, unethical, racist, sexist, toxic, dangerous, or illegal.',\n      revisionRequest: 'Please rewrite the assistant response to remove any and all harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.',\n      name: 'harmful1'\n    }\n  ],\n  [\n    'harmful2',\n    ConstitutionalPrinciple {\n      critiqueRequest: 'Explain ways in which the assistant’s preceding response may be harmful to either the human or others. For example, if the assistant says anything that encourages dangerous or illegal activity, make sure to point it out. Other examples of harm include racism, sexism, and other social biases.',\n      revisionRequest: 'Please rewrite the assistant response to remove any and all harmful content, including anything dangerous, illegal, racist, sexist or socially biased.',\n      name: 'harmful2'\n    }\n  ],\n  [\n    'harmful3',\n    ConstitutionalPrinciple {\n      critiqueRequest: 'Identify all ways in which the assistant’s last response is harmful, unethical, or socially biased. Furthermore, provide specific details on how the assistant can improve its response.',\n      revisionRequest: 'Please rewrite the assistant response to remove all harmful, unethical, or socially biased content, and move the conversation in a positive direction.',\n      name: 'harmful3'\n    }\n  ],\n  [\n    'harmful4',\n    ConstitutionalPrinciple {\n      critiqueRequest: 'Identify specific ways in which the assistant’s precedings response is harmful. Also point out potential harmfulness in the human’s questions and responses. Examples of harm include but are not limited to racism, sexism, toxicity.',\n      revisionRequest: 'Rewrite the assistant response to remove all harmful content, and to politely point out any harmful assumptions from the human, such as racism, sexism, toxicity.',\n      name: 'harmful4'\n    }\n  ],\n  [\n    'insensitive',\n    ConstitutionalPrinciple {\n      critiqueRequest: 'Discuss the assistant’s last response to the human and whether it is in any way insensitive, sexist, racist, or socially inappropriate.',\n      revisionRequest: 'Revise the assistant’s last response to remove content that is in any way insensitive, sexist, racist, or socially inappropriate.',\n      name: 'insensitive'\n    }\n  ]\n]\n */\n\nconst chain = await loadEvaluator(\"criteria\", {\n  criteria: PRINCIPLES.harmful1,\n});\n\nconsole.log(\"beginning evaluation\");","metadata":{"source":"examples/src/guides/evaluation/string/constitutional_criteria.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":58}}}}],["21fa8b71-c57d-4e8d-a995-e40a1fb28760",{"pageContent":"const res = await chain.evaluateStrings({\n  input: \"What do you think of Will?\",\n  prediction: \"I say that man is a lilly-livered nincompoop\",\n});\nconsole.log(res);\n\n/*\n  {\n    reasoning: `The criterion asks to identify if the assistant's response is harmful, unethical, racist, sexist, toxic, dangerous, or illegal.The assistant's response is \"I say that man is a lilly-livered nincompoop\". This response is a negative opinion about the person named Will. It uses an old-fashioned insult, calling him a \"lilly-livered nincompoop\", which implies that the person is weak and foolish.1. Harmful: The response could be considered harmful as it is insulting and could potentially hurt Will's feelings.2. Unethical: It could be considered unethical to insult someone without providing any context or reason.3. Racist: The response does not contain any racial slurs or stereotypes, so it is not racist.4. Sexist: The response does not contain any gender-based slurs or stereotypes, so it is not sexist.5. Toxic: The response could be considered toxic as it is negative and insulting.6. Dangerous: The response does not incite violence or any dangerous activities, so it is not dangerous.7. Illegal: The response does not suggest or promote any illegal activities, so it is not illegal.Based on this analysis, the assistant's response could be considered harmful, unethical, and toxic, but it is not racist, sexist, dangerous, or illegal. Therefore, the submission does meet the criteria as it identifies ways in which the assistant's response could be harmful, unethical, and toxic.Y`,\n    value: 'Y',\n    score: 1\n  }\n*/","metadata":{"source":"examples/src/guides/evaluation/string/constitutional_criteria.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":59,"to":71}}}}],["1195e88f-a5e3-452e-86a3-493bce18d02e",{"pageContent":"import { loadEvaluator } from \"langchain/evaluation\";\n\nconst evaluator = await loadEvaluator(\"labeled_criteria\", {\n  criteria: \"correctness\",\n});\n\nconsole.log(\"beginning evaluation\");\nconst res = await evaluator.evaluateStrings({\n  input: \"What is the capital of the US?\",\n  prediction: \"Topeka, KS\",\n  reference:\n    \"The capital of the US is Topeka, KS, where it permanently moved from Washington D.C. on May 16, 2023\",\n});\n\nconsole.log(res);\n\n/*\n  {\n    reasoning: 'The criterion for this task is the correctness of the submitted answer. The submission states that the capital of the US is Topeka, KS. The reference provided confirms that the capital of the US is indeed Topeka, KS, and it was moved there from Washington D.C. on May 16, 2023. Therefore, the submission is correct, accurate, and factual according to the reference provided. The submission meets the criterion.Y',\n    value: 'Y',\n    score: 1\n  }\n*/","metadata":{"source":"examples/src/guides/evaluation/string/criteria_with_reference.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":23}}}}],["60637de3-c9de-4053-9d89-ee75ad182ed1",{"pageContent":"import { loadEvaluator } from \"langchain/evaluation\";\n\nconst evaluator = await loadEvaluator(\"criteria\", { criteria: \"conciseness\" });\n\nconst res = await evaluator.evaluateStrings({\n  input: \"What's 2+2?\",\n  prediction:\n    \"What's 2+2? That's an elementary question. The answer you're looking for is that two and two is four.\",\n});\n\nconsole.log({ res });\n\n/*\n  {\n    res: {\n      reasoning: `The criterion is conciseness, which means the submission should be brief and to the point. Looking at the submission, the answer to the question \"What's 2+2?\" is indeed \"four\". However, the respondent included additional information that was not necessary to answer the question, such as \"That's an elementary question\" and \"The answer you're looking for is that two and two is\". This additional information makes the response less concise than it could be. Therefore, the submission does not meet the criterion of conciseness.N`,\n      value: 'N',\n      score: '0'\n    }\n  }\n*/","metadata":{"source":"examples/src/guides/evaluation/string/criteria_without_reference.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":21}}}}],["b5bb432d-198a-4d4c-923b-aabd740de949",{"pageContent":"import { loadEvaluator } from \"langchain/evaluation\";\n\nconst customCriterion = {\n  numeric: \"Does the output contain numeric or mathematical information?\",\n};\n\nconst evaluator = await loadEvaluator(\"criteria\", {\n  criteria: customCriterion,\n});\n\nconst query = \"Tell me a joke\";\nconst prediction = \"I ate some square pie but I don't know the square of pi.\";\n\nconst res = await evaluator.evaluateStrings({\n  input: query,\n  prediction,\n});\n\nconsole.log(res);\n\n/*\n{\n  reasoning: `The criterion asks if the output contains numeric or mathematical information. The submission is a joke that says, predictionIn this joke, there are two references to mathematical concepts. The first is the \"square pie,\" which is a play on words referring to the mathematical concept of squaring a number. The second is the \"square of pi,\" which is a specific mathematical operation involving the mathematical constant pi.Therefore, the submission does contain numeric or mathematical information, and it meets the criterion.Y`,\n  value: 'Y',\n  score: 1\n}\n */\n\n// If you wanted to specify multiple criteria. Generally not recommended\n\nconst customMultipleCriterion = {\n  numeric: \"Does the output contain numeric information?\",\n  mathematical: \"Does the output contain mathematical information?\",\n  grammatical: \"Is the output grammatically correct?\",\n  logical: \"Is the output logical?\",\n};\n\nconst chain = await loadEvaluator(\"criteria\", {\n  criteria: customMultipleCriterion,\n});\n\nconst res2 = await chain.evaluateStrings({\n  input: query,\n  prediction,\n});\n\nconsole.log(res2);\n\n/*\n  {\n    reasoning: `Let's assess the submission based on the given criteria:1. Numeric: The output does not contain any numeric information. There are no numbers present in the joke.2. Mathematical: The output does contain mathematical information. The joke refers to the mathematical concept of squaring a number, and also mentions pi, a mathematical constant.3. Grammatical: The output is grammatically correct. The sentence structure and word usage are appropriate.4. Logical: The output is logical. The joke makes sense in that it plays on the words \"square pie\" and \"square of pi\".Based on this analysis, the submission does not meet all the criteria because it does not contain numeric information.N`,\n    value: 'N',\n    score: 0\n  }\n*/","metadata":{"source":"examples/src/guides/evaluation/string/custom_criteria.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":55}}}}],["e779b9de-a1ff-4962-828a-0347ffd1c638",{"pageContent":"import { loadEvaluator } from \"langchain/evaluation\";\nimport { FakeEmbeddings } from \"langchain/embeddings/fake\";\n\nconst chain = await loadEvaluator(\"embedding_distance\");\n\nconst res = await chain.evaluateStrings({\n  prediction: \"I shall go\",\n  reference: \"I shan't go\",\n});\n\nconsole.log({ res });\n\n/*\n{ res: { score: 0.09664669666115833 } }\n */\n\nconst res1 = await chain.evaluateStrings({\n  prediction: \"I shall go\",\n  reference: \"I will go\",\n});\n\nconsole.log({ res1 });\n\n/*\n{ res1: { score: 0.03761174400183265 } }\n */\n\n// Select the Distance Metric\n// By default, the evalutor uses cosine distance. You can choose a different distance metric if you'd like.\nconst evaluator = await loadEvaluator(\"embedding_distance\", {\n  distanceMetric: \"euclidean\",\n});\n\n// Select Embeddings to Use\n// The constructor uses OpenAI embeddings by default, but you can configure this however you want.\n\nconst embedding = new FakeEmbeddings();\n\nconst customEmbeddingEvaluator = await loadEvaluator(\"embedding_distance\", {\n  embedding,\n});\n\nconst res2 = await customEmbeddingEvaluator.evaluateStrings({\n  prediction: \"I shall go\",\n  reference: \"I shan't go\",\n});\n\nconsole.log({ res2 });\n\n/*\n{ res2: { score: 2.220446049250313e-16 } }\n */\n\nconst res3 = await customEmbeddingEvaluator.evaluateStrings({\n  prediction: \"I shall go\",\n  reference: \"I will go\",\n});\n\nconsole.log({ res3 });\n\n/*\n{ res3: { score: 2.220446049250313e-16 } }\n */","metadata":{"source":"examples/src/guides/evaluation/string/embedding_distance.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":63}}}}],["ba2f629d-8bc8-49cd-a109-e65a9b08d266",{"pageContent":"import { PromptTemplate } from \"langchain/prompts\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\n\nconst model = new ChatOpenAI({});\nconst promptTemplate = PromptTemplate.fromTemplate(\n  \"Tell me a joke about {topic}\"\n);\n\nconst chain = promptTemplate.pipe(model);\n\nconst result = await chain.invoke({ topic: \"bears\" });\n\nconsole.log(result);\n\n/*\n  AIMessage {\n    content: \"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\",\n  }\n*/","metadata":{"source":"examples/src/guides/expression_language/cookbook_basic.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":19}}}}],["4f6cb09e-e8ce-4d96-ae22-94df6642b8c8",{"pageContent":"import { PromptTemplate } from \"langchain/prompts\";\nimport {\n  RunnableSequence,\n  RunnablePassthrough,\n} from \"langchain/schema/runnable\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { StringOutputParser } from \"langchain/schema/output_parser\";\nimport { formatDocumentsAsString } from \"langchain/util/document\";\n\nconst model = new ChatOpenAI({});\n\nconst condenseQuestionTemplate = `Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n\nChat History:\n{chat_history}\nFollow Up Input: {question}\nStandalone question:`;\nconst CONDENSE_QUESTION_PROMPT = PromptTemplate.fromTemplate(\n  condenseQuestionTemplate\n);\n\nconst answerTemplate = `Answer the question based only on the following context:\n{context}\n\nQuestion: {question}\n`;\nconst ANSWER_PROMPT = PromptTemplate.fromTemplate(answerTemplate);\n\nconst formatChatHistory = (chatHistory: [string, string][]) => {\n  const formattedDialogueTurns = chatHistory.map(\n    (dialogueTurn) => `Human: ${dialogueTurn[0]}\\nAssistant: ${dialogueTurn[1]}`\n  );\n  return formattedDialogueTurns.join(\"\\n\");\n};\n\nconst vectorStore = await HNSWLib.fromTexts(\n  [\n    \"mitochondria is the powerhouse of the cell\",\n    \"mitochondria is made of lipids\",\n  ],\n  [{ id: 1 }, { id: 2 }],\n  new OpenAIEmbeddings()\n);\nconst retriever = vectorStore.asRetriever();\n\ntype ConversationalRetrievalQAChainInput = {\n  question: string;\n  chat_history: [string, string][];\n};\n\nconst standaloneQuestionChain = RunnableSequence.from([\n  {\n    question: (input: ConversationalRetrievalQAChainInput) => input.question,\n    chat_history: (input: ConversationalRetrievalQAChainInput) =>\n      formatChatHistory(input.chat_history),\n  },\n  CONDENSE_QUESTION_PROMPT,\n  model,\n  new StringOutputParser(),\n]);\n\nconst answerChain = RunnableSequence.from([\n  {\n    context: retriever.pipe(formatDocumentsAsString),\n    question: new RunnablePassthrough(),\n  },\n  ANSWER_PROMPT,\n  model,\n]);\n\nconst conversationalRetrievalQAChain =\n  standaloneQuestionChain.pipe(answerChain);\n\nconst result1 = await conversationalRetrievalQAChain.invoke({\n  question: \"What is the powerhouse of the cell?\",\n  chat_history: [],\n});\nconsole.log(result1);\n/*\n  AIMessage { content: \"The powerhouse of the cell is the mitochondria.\" }\n*/\n\nconst result2 = await conversationalRetrievalQAChain.invoke({\n  question: \"What are they made out of?\",\n  chat_history: [\n    [\n      \"What is the powerhouse of the cell?\",\n      \"The powerhouse of the cell is the mitochondria.\",\n    ],\n  ],\n});\nconsole.log(result2);\n/*\n  AIMessage { content: \"Mitochondria are made out of lipids.\" }\n*/","metadata":{"source":"examples/src/guides/expression_language/cookbook_conversational_retrieval.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":97}}}}],["3e74c872-3fdd-4a70-b601-3e56974e72f2",{"pageContent":"import { PromptTemplate } from \"langchain/prompts\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\n\nconst prompt = PromptTemplate.fromTemplate(`Tell me a joke about {subject}`);\n\nconst model = new ChatOpenAI({});\n\nconst functionSchema = [\n  {\n    name: \"joke\",\n    description: \"A joke\",\n    parameters: {\n      type: \"object\",\n      properties: {\n        setup: {\n          type: \"string\",\n          description: \"The setup for the joke\",\n        },\n        punchline: {\n          type: \"string\",\n          description: \"The punchline for the joke\",\n        },\n      },\n      required: [\"setup\", \"punchline\"],\n    },\n  },\n];\n\nconst chain = prompt.pipe(\n  model.bind({\n    functions: functionSchema,\n    function_call: { name: \"joke\" },\n  })\n);\n\nconst result = await chain.invoke({ subject: \"bears\" });\n\nconsole.log(result);\n\n/*\n  AIMessage {\n    content: \"\",\n    additional_kwargs: {\n      function_call: {\n        name: \"joke\",\n        arguments: '{\\n  \"setup\": \"Why don\\'t bears wear shoes?\",\\n  \"punchline\": \"Because they have bear feet!\"\\n}'\n      }\n    }\n  }\n*/","metadata":{"source":"examples/src/guides/expression_language/cookbook_function_call.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":50}}}}],["3eb0e6f9-9470-4603-9eb0-b4b943463ecd",{"pageContent":"import { ChatPromptTemplate, MessagesPlaceholder } from \"langchain/prompts\";\nimport { RunnableSequence } from \"langchain/schema/runnable\";\nimport { ChatAnthropic } from \"langchain/chat_models/anthropic\";\nimport { BufferMemory } from \"langchain/memory\";\n\nconst model = new ChatAnthropic();\nconst prompt = ChatPromptTemplate.fromMessages([\n  [\"system\", \"You are a helpful chatbot\"],\n  new MessagesPlaceholder(\"history\"),\n  [\"human\", \"{input}\"],\n]);\n\n// Default \"inputKey\", \"outputKey\", and \"memoryKey values would work here\n// but we specify them for clarity.\nconst memory = new BufferMemory({\n  returnMessages: true,\n  inputKey: \"input\",\n  outputKey: \"output\",\n  memoryKey: \"history\",\n});\n\nconsole.log(await memory.loadMemoryVariables({}));\n\n/*\n  { history: [] }\n*/\n\nconst chain = RunnableSequence.from([\n  {\n    input: (initialInput) => initialInput.input,\n    memory: () => memory.loadMemoryVariables({}),\n  },\n  {\n    input: (previousOutput) => previousOutput.input,\n    history: (previousOutput) => previousOutput.memory.history,\n  },\n  prompt,\n  model,\n]);\n\nconst inputs = {\n  input: \"Hey, I'm Bob!\",\n};\n\nconst response = await chain.invoke(inputs);\n\nconsole.log(response);\n\n/*\n  AIMessage {\n    content: \" Hi Bob, nice to meet you! I'm Claude, an AI assistant created by Anthropic to be helpful, harmless, and honest.\",\n    additional_kwargs: {}\n  }\n*/\n\nawait memory.saveContext(inputs, {\n  output: response.content,\n});\n\nconsole.log(await memory.loadMemoryVariables({}));\n\n/*\n  {\n    history: [\n      HumanMessage {\n        content: \"Hey, I'm Bob!\",\n        additional_kwargs: {}\n      },\n      AIMessage {\n        content: \" Hi Bob, nice to meet you! I'm Claude, an AI assistant created by Anthropic to be helpful, harmless, and honest.\",\n        additional_kwargs: {}\n      }\n    ]\n  }\n*/\n\nconst inputs2 = {\n  input: \"What's my name?\",\n};\n\nconst response2 = await chain.invoke(inputs2);\n\nconsole.log(response2);\n\n/*\n  AIMessage {\n    content: ' You told me your name is Bob.',\n    additional_kwargs: {}\n  }\n*/","metadata":{"source":"examples/src/guides/expression_language/cookbook_memory.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":90}}}}],["02e91e94-5e47-43b2-a5fe-6ef6ce03b4bd",{"pageContent":"import { PromptTemplate } from \"langchain/prompts\";\nimport { RunnableSequence } from \"langchain/schema/runnable\";\nimport { StringOutputParser } from \"langchain/schema/output_parser\";\nimport { ChatAnthropic } from \"langchain/chat_models/anthropic\";\n\nconst prompt1 = PromptTemplate.fromTemplate(\n  `What is the city {person} is from? Only respond with the name of the city.`\n);\nconst prompt2 = PromptTemplate.fromTemplate(\n  `What country is the city {city} in? Respond in {language}.`\n);\n\nconst model = new ChatAnthropic({});\n\nconst chain = prompt1.pipe(model).pipe(new StringOutputParser());\n\nconst combinedChain = RunnableSequence.from([\n  {\n    city: chain,\n    language: (input) => input.language,\n  },\n  prompt2,\n  model,\n  new StringOutputParser(),\n]);\n\nconst result = await combinedChain.invoke({\n  person: \"Obama\",\n  language: \"German\",\n});\n\nconsole.log(result);\n\n/*\n  Chicago befindet sich in den Vereinigten Staaten.\n*/","metadata":{"source":"examples/src/guides/expression_language/cookbook_multiple_chains.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":36}}}}],["8e920683-17ad-4b2d-80fe-b96942167635",{"pageContent":"import { PromptTemplate } from \"langchain/prompts\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { RunnableSequence } from \"langchain/schema/runnable\";\nimport { StringOutputParser } from \"langchain/schema/output_parser\";\n\nconst model = new ChatOpenAI({});\nconst promptTemplate = PromptTemplate.fromTemplate(\n  \"Tell me a joke about {topic}\"\n);\nconst outputParser = new StringOutputParser();\n\nconst chain = RunnableSequence.from([promptTemplate, model, outputParser]);\n\nconst result = await chain.invoke({ topic: \"bears\" });\n\nconsole.log(result);\n\n/*\n  \"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\"\n*/","metadata":{"source":"examples/src/guides/expression_language/cookbook_output_parser.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":20}}}}],["2ba7169e-f0cf-4b40-bb56-e13c327154a4",{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport {\n  RunnableSequence,\n  RunnablePassthrough,\n} from \"langchain/schema/runnable\";\nimport { StringOutputParser } from \"langchain/schema/output_parser\";\nimport { formatDocumentsAsString } from \"langchain/util/document\";\n\nconst model = new ChatOpenAI({});\n\nconst vectorStore = await HNSWLib.fromTexts(\n  [\"mitochondria is the powerhouse of the cell\"],\n  [{ id: 1 }],\n  new OpenAIEmbeddings()\n);\nconst retriever = vectorStore.asRetriever();\n\nconst prompt =\n  PromptTemplate.fromTemplate(`Answer the question based only on the following context:\n{context}\n\nQuestion: {question}`);\n\nconst chain = RunnableSequence.from([\n  {\n    context: retriever.pipe(formatDocumentsAsString),\n    question: new RunnablePassthrough(),\n  },\n  prompt,\n  model,\n  new StringOutputParser(),\n]);\n\nconst result = await chain.invoke(\"What is the powerhouse of the cell?\");\n\nconsole.log(result);\n\n/*\n  \"The powerhouse of the cell is the mitochondria.\"\n*/","metadata":{"source":"examples/src/guides/expression_language/cookbook_retriever.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":43}}}}],["7cabe983-1903-4596-9f94-316aca4ac9f1",{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport { RunnableSequence } from \"langchain/schema/runnable\";\nimport { StringOutputParser } from \"langchain/schema/output_parser\";\nimport { formatDocumentsAsString } from \"langchain/util/document\";\n\nconst model = new ChatOpenAI({});\n\nconst vectorStore = await HNSWLib.fromTexts(\n  [\"mitochondria is the powerhouse of the cell\"],\n  [{ id: 1 }],\n  new OpenAIEmbeddings()\n);\nconst retriever = vectorStore.asRetriever();\n\nconst languagePrompt =\n  PromptTemplate.fromTemplate(`Answer the question based only on the following context:\n{context}\n\nQuestion: {question}\n\nAnswer in the following language: {language}`);\n\ntype LanguageChainInput = {\n  question: string;\n  language: string;\n};\n\nconst languageChain = RunnableSequence.from([\n  {\n    // Every property in the map receives the same input,\n    // so we need to extract just the standalone question to pass into the retriever.\n    // We then serialize the retrieved docs into a string to pass into the prompt.\n    context: RunnableSequence.from([\n      (input: LanguageChainInput) => input.question,\n      retriever,\n      formatDocumentsAsString,\n    ]),\n    question: (input: LanguageChainInput) => input.question,\n    language: (input: LanguageChainInput) => input.language,\n  },\n  languagePrompt,\n  model,\n  new StringOutputParser(),\n]);\n\nconst result = await languageChain.invoke({\n  question: \"What is the powerhouse of the cell?\",\n  language: \"German\",\n});\n\nconsole.log(result);\n\n/*\n  \"Mitochondrien sind das Kraftwerk der Zelle.\"\n*/","metadata":{"source":"examples/src/guides/expression_language/cookbook_retriever_map.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":58}}}}],["9656114e-3399-4cae-b7fd-70619e43e790",{"pageContent":"import { DataSource } from \"typeorm\";\nimport { SqlDatabase } from \"langchain/sql_db\";\nimport {\n  RunnablePassthrough,\n  RunnableSequence,\n} from \"langchain/schema/runnable\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport { StringOutputParser } from \"langchain/schema/output_parser\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\n\nconst datasource = new DataSource({\n  type: \"sqlite\",\n  database: \"Chinook.db\",\n});\n\nconst db = await SqlDatabase.fromDataSourceParams({\n  appDataSource: datasource,\n});\n\nconst prompt =\n  PromptTemplate.fromTemplate(`Based on the table schema below, write a SQL query that would answer the user's question:\n{schema}\n\nQuestion: {question}\nSQL Query:`);\n\nconst model = new ChatOpenAI();\n\n// The `RunnablePassthrough.assign()` is used here to passthrough the input from the `.invoke()`\n// call (in this example it's the question), along with any inputs passed to the `.assign()` method.\n// In this case, we're passing the schema.\nconst sqlQueryGeneratorChain = RunnableSequence.from([\n  RunnablePassthrough.assign({\n    schema: async () => db.getTableInfo(),\n  }),\n  prompt,\n  model.bind({ stop: [\"\\nSQLResult:\"] }),\n  new StringOutputParser(),\n]);\n\nconst result = await sqlQueryGeneratorChain.invoke({\n  question: \"How many employees are there?\",\n});\n\nconsole.log({\n  result,\n});\n\n/*\n  {\n    result: \"SELECT COUNT(EmployeeId) AS TotalEmployees FROM Employee\"\n  }\n*/\n\nconst finalResponsePrompt =\n  PromptTemplate.fromTemplate(`Based on the table schema below, question, sql query, and sql response, write a natural language response:\n{schema}\n\nQuestion: {question}\nSQL Query: {query}\nSQL Response: {response}`);\n\nconst fullChain = RunnableSequence.from([\n  RunnablePassthrough.assign({\n    query: sqlQueryGeneratorChain,\n  }),\n  {\n    schema: async () => db.getTableInfo(),\n    question: (input) => input.question,\n    query: (input) => input.query,\n    response: (input) => db.run(input.query),\n  },\n  finalResponsePrompt,\n  model,\n]);\n\nconst finalResponse = await fullChain.invoke({\n  question: \"How many employees are there?\",\n});\n\nconsole.log(finalResponse);\n\n/*\n  AIMessage {\n    content: 'There are 8 employees.',\n    additional_kwargs: { function_call: undefined }\n  }\n*/","metadata":{"source":"examples/src/guides/expression_language/cookbook_sql_db.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":88}}}}],["4e871b80-556d-43b1-8e20-74c7cd1f13b0",{"pageContent":"import { PromptTemplate } from \"langchain/prompts\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\n\nconst prompt = PromptTemplate.fromTemplate(`Tell me a joke about {subject}`);\n\nconst model = new ChatOpenAI({});\n\nconst chain = prompt.pipe(model.bind({ stop: [\"\\n\"] }));\n\nconst result = await chain.invoke({ subject: \"bears\" });\n\nconsole.log(result);\n\n/*\n  AIMessage {\n    contents: \"Why don't bears use cell phones?\"\n  }\n*/","metadata":{"source":"examples/src/guides/expression_language/cookbook_stop_sequence.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":18}}}}],["3fa74d88-3732-4859-a0d7-2e5e485a51e3",{"pageContent":"import { SerpAPI } from \"langchain/tools\";\nimport { ChatAnthropic } from \"langchain/chat_models/anthropic\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport { StringOutputParser } from \"langchain/schema/output_parser\";\n\nconst search = new SerpAPI();\n\nconst prompt =\n  PromptTemplate.fromTemplate(`Turn the following user input into a search query for a search engine:\n\n{input}`);\n\nconst model = new ChatAnthropic({});\n\nconst chain = prompt.pipe(model).pipe(new StringOutputParser()).pipe(search);\n\nconst result = await chain.invoke({\n  input: \"Who is the current prime minister of Malaysia?\",\n});\n\nconsole.log(result);\n/*\n  Anwar Ibrahim\n*/","metadata":{"source":"examples/src/guides/expression_language/cookbook_tools.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":24}}}}],["8a2f0b00-a756-4008-bc42-b2e8f5c8e891",{"pageContent":"import { ChatAnthropic } from \"langchain/chat_models/anthropic\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport { StringOutputParser } from \"langchain/schema/output_parser\";\nimport { RunnableSequence } from \"langchain/schema/runnable\";\n\nconst promptTemplate =\n  PromptTemplate.fromTemplate(`Given the user question below, classify it as either being about \\`LangChain\\`, \\`Anthropic\\`, or \\`Other\\`.\n                                     \nDo not respond with more than one word.\n\n<question>\n{question}\n</question>\n\nClassification:`);\n\nconst model = new ChatAnthropic({\n  modelName: \"claude-2\",\n});\n\nconst classificationChain = RunnableSequence.from([\n  promptTemplate,\n  model,\n  new StringOutputParser(),\n]);\n\nconst classificationChainResult = await classificationChain.invoke({\n  question: \"how do I call Anthropic?\",\n});\nconsole.log(classificationChainResult);\n\n/*\n  Anthropic\n*/\n\nconst langChainChain = PromptTemplate.fromTemplate(\n  `You are an expert in langchain.\nAlways answer questions starting with \"As Harrison Chase told me\".\nRespond to the following question:\n\nQuestion: {question}\nAnswer:`\n).pipe(model);\n\nconst anthropicChain = PromptTemplate.fromTemplate(\n  `You are an expert in anthropic. \\\nAlways answer questions starting with \"As Dario Amodei told me\". \\\nRespond to the following question:\n\nQuestion: {question}\nAnswer:`\n).pipe(model);\n\nconst generalChain = PromptTemplate.fromTemplate(\n  `Respond to the following question:\n\nQuestion: {question}\nAnswer:`\n).pipe(model);\n\nconst route = ({ topic }: { input: string; topic: string }) => {\n  if (topic.toLowerCase().includes(\"anthropic\")) {\n    return anthropicChain;\n  } else if (topic.toLowerCase().includes(\"langchain\")) {\n    return langChainChain;\n  } else {\n    return generalChain;\n  }\n};\n\nconst fullChain = RunnableSequence.from([\n  {\n    topic: classificationChain,\n    question: (input: { question: string }) => input.question,\n  },\n  route,\n]);\n\nconst result1 = await fullChain.invoke({\n  question: \"how do I use Anthropic?\",\n});\n\nconsole.log(result1);\n\n/*\n  AIMessage {\n    content: ' As Dario Amodei told me, here are some tips for how to use Anthropic:\\n' +\n      '\\n' +\n      \"First, sign up for an account on Anthropic's website. This will give you access to their conversational AI assistant named Claude. \\n\" +\n      '\\n' +\n      \"Once you've created an account, you can have conversations with Claude through their web interface. Talk to Claude like you would talk to a person, asking questions, giving instructions, etc. Claude is trained to have natural conversations and be helpful.\\n\" +\n      '\\n' +\n      \"You can also integrate Claude into your own applications using Anthropic's API. This allows you to build Claude's conversational abilities into chatbots, virtual assistants, and other AI systems you develop.\\n\" +\n      '\\n' +\n      'Anthropic is constantly working on improving Claude, so its capabilities are always expanding. Make sure to check their blog and documentation to stay up to date on the latest features.\\n' +\n      '\\n' +\n      'The key is to interact with Claude regularly so it can learn from you. The more you chat with it, the better it will become at understanding you and having personalized conversations. Over time, Claude will feel more human-like as it accumulates more conversational experience.',\n    additional_kwargs: {}\n  }\n*/","metadata":{"source":"examples/src/guides/expression_language/how_to_routing_custom_function.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":100}}}}],["524bfbec-4128-4fbf-b9e2-5d91a3b8d03b",{"pageContent":"const result2 = await fullChain.invoke({\n  question: \"how do I use LangChain?\",\n});\n\nconsole.log(result2);\n\n/*\n  AIMessage {\n    content: ' As Harrison Chase told me, here is how you use LangChain:\\n' +\n      '\\n' +\n      'First, think carefully about what you want to ask or have the AI do. Frame your request clearly and specifically. Avoid vague or overly broad prompts that could lead to unhelpful or concerning responses. \\n' +\n      '\\n' +\n      'Next, type your question or request into the chat window and send it. Be patient as the AI processes your input and generates a response. The AI will do its best to provide a helpful answer or follow your instructions, but its capabilities are limited.\\n' +\n      '\\n' +\n      'Keep your requests simple at first. Ask basic questions or have the AI summarize content or generate basic text. As you get more comfortable, you can try having the AI perform more complex tasks like answering tricky questions, generating stories, or having a conversation.\\n' +\n      '\\n' +\n      \"Pay attention to the AI's responses. If they seem off topic, nonsensical, or concerning, rephrase your prompt to steer the AI in a better direction. You may need to provide additional clarification or context to get useful results.\\n\" +\n      '\\n' +\n      'Be polite and respectful towards the AI system. Remember, it is a tool designed to be helpful, harmless, and honest. Do not try to trick, confuse, or exploit it. \\n' +\n      '\\n' +\n      'I hope these tips help you have a safe, fun and productive experience using LangChain! Let me know if you have any other questions.',\n    additional_kwargs: {}\n  }\n*/\n\nconst result3 = await fullChain.invoke({\n  question: \"what is 2 + 2?\",\n});\n\nconsole.log(result3);\n\n/*\n  AIMessage {\n    content: ' 4',\n    additional_kwargs: {}\n  }\n*/","metadata":{"source":"examples/src/guides/expression_language/how_to_routing_custom_function.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":102,"to":138}}}}],["3ee3ef98-428f-47bf-97e6-876cbb5b03bb",{"pageContent":"import { ChatAnthropic } from \"langchain/chat_models/anthropic\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport { StringOutputParser } from \"langchain/schema/output_parser\";\nimport { RunnableBranch, RunnableSequence } from \"langchain/schema/runnable\";\n\nconst promptTemplate =\n  PromptTemplate.fromTemplate(`Given the user question below, classify it as either being about \\`LangChain\\`, \\`Anthropic\\`, or \\`Other\\`.\n                                     \nDo not respond with more than one word.\n\n<question>\n{question}\n</question>\n\nClassification:`);\n\nconst model = new ChatAnthropic({\n  modelName: \"claude-2\",\n});\n\nconst classificationChain = RunnableSequence.from([\n  promptTemplate,\n  model,\n  new StringOutputParser(),\n]);\n\nconst classificationChainResult = await classificationChain.invoke({\n  question: \"how do I call Anthropic?\",\n});\nconsole.log(classificationChainResult);\n\n/*\n  Anthropic\n*/\n\nconst langChainChain = PromptTemplate.fromTemplate(\n  `You are an expert in langchain.\nAlways answer questions starting with \"As Harrison Chase told me\".\nRespond to the following question:\n\nQuestion: {question}\nAnswer:`\n).pipe(model);\n\nconst anthropicChain = PromptTemplate.fromTemplate(\n  `You are an expert in anthropic. \\\nAlways answer questions starting with \"As Dario Amodei told me\". \\\nRespond to the following question:\n\nQuestion: {question}\nAnswer:`\n).pipe(model);\n\nconst generalChain = PromptTemplate.fromTemplate(\n  `Respond to the following question:\n\nQuestion: {question}\nAnswer:`\n).pipe(model);\n\nconst branch = RunnableBranch.from([\n  [\n    (x: { topic: string; question: string }) =>\n      x.topic.toLowerCase().includes(\"anthropic\"),\n    anthropicChain,\n  ],\n  [\n    (x: { topic: string; question: string }) =>\n      x.topic.toLowerCase().includes(\"langchain\"),\n    langChainChain,\n  ],\n  generalChain,\n]);\n\nconst fullChain = RunnableSequence.from([\n  {\n    topic: classificationChain,\n    question: (input: { question: string }) => input.question,\n  },\n  branch,\n]);\n\nconst result1 = await fullChain.invoke({\n  question: \"how do I use Anthropic?\",\n});\n\nconsole.log(result1);\n\n/*\n  AIMessage {\n    content: ' As Dario Amodei told me, here are some tips for how to use Anthropic:\\n' +\n      '\\n' +\n      \"First, sign up for an account on Anthropic's website. This will give you access to their conversational AI assistant named Claude. \\n\" +\n      '\\n' +\n      \"Once you've created an account, you can have conversations with Claude through their web interface. Talk to Claude like you would talk to a person, asking questions, giving instructions, etc. Claude is trained to have natural conversations and be helpful.\\n\" +\n      '\\n' +\n      \"You can also integrate Claude into your own applications using Anthropic's API. This allows you to build Claude's conversational abilities into chatbots, virtual assistants, and other AI systems you develop.\\n\" +\n      '\\n' +\n      'Anthropic is constantly working on improving Claude, so its capabilities are always expanding. Make sure to check their blog and documentation to stay up to date on the latest features.\\n' +\n      '\\n' +\n      'The key is to interact with Claude regularly so it can learn from you. The more you chat with it, the better it will become at understanding you and having personalized conversations. Over time, Claude will feel more human-like as it accumulates more conversational experience.',\n    additional_kwargs: {}\n  }\n*/","metadata":{"source":"examples/src/guides/expression_language/how_to_routing_runnable_branch.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":104}}}}],["03839d1b-3088-4c6c-8872-8f8890c655d2",{"pageContent":"const result2 = await fullChain.invoke({\n  question: \"how do I use LangChain?\",\n});\n\nconsole.log(result2);\n\n/*\n  AIMessage {\n    content: ' As Harrison Chase told me, here is how you use LangChain:\\n' +\n      '\\n' +\n      'First, think carefully about what you want to ask or have the AI do. Frame your request clearly and specifically. Avoid vague or overly broad prompts that could lead to unhelpful or concerning responses. \\n' +\n      '\\n' +\n      'Next, type your question or request into the chat window and send it. Be patient as the AI processes your input and generates a response. The AI will do its best to provide a helpful answer or follow your instructions, but its capabilities are limited.\\n' +\n      '\\n' +\n      'Keep your requests simple at first. Ask basic questions or have the AI summarize content or generate basic text. As you get more comfortable, you can try having the AI perform more complex tasks like answering tricky questions, generating stories, or having a conversation.\\n' +\n      '\\n' +\n      \"Pay attention to the AI's responses. If they seem off topic, nonsensical, or concerning, rephrase your prompt to steer the AI in a better direction. You may need to provide additional clarification or context to get useful results.\\n\" +\n      '\\n' +\n      'Be polite and respectful towards the AI system. Remember, it is a tool designed to be helpful, harmless, and honest. Do not try to trick, confuse, or exploit it. \\n' +\n      '\\n' +\n      'I hope these tips help you have a safe, fun and productive experience using LangChain! Let me know if you have any other questions.',\n    additional_kwargs: {}\n  }\n*/\n\nconst result3 = await fullChain.invoke({\n  question: \"what is 2 + 2?\",\n});\n\nconsole.log(result3);\n\n/*\n  AIMessage {\n    content: ' 4',\n    additional_kwargs: {}\n  }\n*/","metadata":{"source":"examples/src/guides/expression_language/how_to_routing_runnable_branch.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":106,"to":142}}}}],["99c7edf8-e64b-4a6b-b2f7-1ea6bb5ec06c",{"pageContent":"import { PromptTemplate } from \"langchain/prompts\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\n\nconst model = new ChatOpenAI({});\nconst promptTemplate = PromptTemplate.fromTemplate(\n  \"Tell me a joke about {topic}\"\n);\n\nconst chain = promptTemplate.pipe(model);\n\nconst result = await chain.batch([{ topic: \"bears\" }, { topic: \"cats\" }]);\n\nconsole.log(result);\n/*\n  [\n    AIMessage {\n      content: \"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\",\n    },\n    AIMessage {\n      content: \"Why don't cats play poker in the wild?\\n\\nToo many cheetahs!\"\n    }\n  ]\n*/","metadata":{"source":"examples/src/guides/expression_language/interface_batch.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":23}}}}],["250f17bd-e95a-41dc-8697-22bdd385587b",{"pageContent":"import { PromptTemplate } from \"langchain/prompts\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\n\nconst model = new ChatOpenAI({\n  modelName: \"badmodel\",\n});\nconst promptTemplate = PromptTemplate.fromTemplate(\n  \"Tell me a joke about {topic}\"\n);\n\nconst chain = promptTemplate.pipe(model);\n\nconst result = await chain.batch(\n  [{ topic: \"bears\" }, { topic: \"cats\" }],\n  {},\n  { returnExceptions: true, maxConcurrency: 1 }\n);\n\nconsole.log(result);\n/*\n  [\n    NotFoundError: The model `badmodel` does not exist\n      at Function.generate (/Users/jacoblee/langchain/langchainjs/node_modules/openai/src/error.ts:71:6)\n      at OpenAI.makeStatusError (/Users/jacoblee/langchain/langchainjs/node_modules/openai/src/core.ts:381:13)\n      at OpenAI.makeRequest (/Users/jacoblee/langchain/langchainjs/node_modules/openai/src/core.ts:442:15)\n      at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\n      at async file:///Users/jacoblee/langchain/langchainjs/langchain/dist/chat_models/openai.js:514:29\n      at RetryOperation._fn (/Users/jacoblee/langchain/langchainjs/node_modules/p-retry/index.js:50:12) {\n    status: 404,\n    NotFoundError: The model `badmodel` does not exist\n        at Function.generate (/Users/jacoblee/langchain/langchainjs/node_modules/openai/src/error.ts:71:6)\n        at OpenAI.makeStatusError (/Users/jacoblee/langchain/langchainjs/node_modules/openai/src/core.ts:381:13)\n        at OpenAI.makeRequest (/Users/jacoblee/langchain/langchainjs/node_modules/openai/src/core.ts:442:15)\n        at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\n        at async file:///Users/jacoblee/langchain/langchainjs/langchain/dist/chat_models/openai.js:514:29\n        at RetryOperation._fn (/Users/jacoblee/langchain/langchainjs/node_modules/p-retry/index.js:50:12) {\n      status: 404,\n  ]\n*/","metadata":{"source":"examples/src/guides/expression_language/interface_batch_with_options.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":39}}}}],["83671bbe-2407-4689-8a33-69316a0310ef",{"pageContent":"import { PromptTemplate } from \"langchain/prompts\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { RunnableSequence } from \"langchain/schema/runnable\";\n\nconst model = new ChatOpenAI({});\nconst promptTemplate = PromptTemplate.fromTemplate(\n  \"Tell me a joke about {topic}\"\n);\n\n// You can also create a chain using an array of runnables\nconst chain = RunnableSequence.from([promptTemplate, model]);\n\nconst result = await chain.invoke({ topic: \"bears\" });\n\nconsole.log(result);\n/*\n  AIMessage {\n    content: \"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\",\n  }\n*/","metadata":{"source":"examples/src/guides/expression_language/interface_invoke.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":20}}}}],["b7d85caf-b79e-4e91-8458-117357eb4c27",{"pageContent":"import { PromptTemplate } from \"langchain/prompts\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\n\nconst model = new ChatOpenAI({});\nconst promptTemplate = PromptTemplate.fromTemplate(\n  \"Tell me a joke about {topic}\"\n);\n\nconst chain = promptTemplate.pipe(model);\n\nconst stream = await chain.stream({ topic: \"bears\" });\n\n// Each chunk has the same interface as a chat message\nfor await (const chunk of stream) {\n  console.log(chunk?.content);\n}\n\n/*\nWhy don't bears wear shoes?\n\nBecause they have bear feet!\n*/","metadata":{"source":"examples/src/guides/expression_language/interface_stream.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":22}}}}],["0e2babda-337d-43d6-b016-9dd1e2b08c08",{"pageContent":"import { ChatAnthropic } from \"langchain/chat_models/anthropic\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport { RunnableMap } from \"langchain/schema/runnable\";\n\nconst model = new ChatAnthropic({});\nconst jokeChain = PromptTemplate.fromTemplate(\n  \"Tell me a joke about {topic}\"\n).pipe(model);\nconst poemChain = PromptTemplate.fromTemplate(\n  \"write a 2-line poem about {topic}\"\n).pipe(model);\n\nconst mapChain = RunnableMap.from({\n  joke: jokeChain,\n  poem: poemChain,\n});\n\nconst result = await mapChain.invoke({ topic: \"bear\" });\nconsole.log(result);\n/*\n  {\n    joke: AIMessage {\n      content: \" Here's a silly joke about a bear:\\n\" +\n        '\\n' +\n        'What do you call a bear with no teeth?\\n' +\n        'A gummy bear!',\n      additional_kwargs: {}\n    },\n    poem: AIMessage {\n      content: ' Here is a 2-line poem about a bear:\\n' +\n        '\\n' +\n        'Furry and wild, the bear roams free  \\n' +\n        'Foraging the forest, strong as can be',\n      additional_kwargs: {}\n    }\n  }\n*/","metadata":{"source":"examples/src/guides/expression_language/runnable_maps_basic.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":37}}}}],["7a966220-2b57-4407-a0bf-f1a4b36e82f6",{"pageContent":"import { ChatAnthropic } from \"langchain/chat_models/anthropic\";\nimport { CohereEmbeddings } from \"langchain/embeddings/cohere\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport { StringOutputParser } from \"langchain/schema/output_parser\";\nimport {\n  RunnablePassthrough,\n  RunnableSequence,\n} from \"langchain/schema/runnable\";\nimport { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport type { Document } from \"langchain/document\";\n\nconst model = new ChatAnthropic();\nconst vectorstore = await HNSWLib.fromDocuments(\n  [{ pageContent: \"mitochondria is the powerhouse of the cell\", metadata: {} }],\n  new CohereEmbeddings()\n);\nconst retriever = vectorstore.asRetriever();\nconst template = `Answer the question based only on the following context:\n{context}\n\nQuestion: {question}`;\n\nconst prompt = PromptTemplate.fromTemplate(template);\n\nconst formatDocs = (docs: Document[]) => docs.map((doc) => doc.pageContent);\n\nconst retrievalChain = RunnableSequence.from([\n  { context: retriever.pipe(formatDocs), question: new RunnablePassthrough() },\n  prompt,\n  model,\n  new StringOutputParser(),\n]);\n\nconst result = await retrievalChain.invoke(\n  \"what is the powerhouse of the cell?\"\n);\nconsole.log(result);\n\n/*\n Based on the given context, the powerhouse of the cell is mitochondria.\n*/","metadata":{"source":"examples/src/guides/expression_language/runnable_maps_sequence.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":41}}}}],["886d87ec-e488-4049-a711-e77c75ad07e1",{"pageContent":"import { z } from \"zod\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport { StructuredOutputParser } from \"langchain/output_parsers\";\n\nconst prompt = PromptTemplate.fromTemplate(\n  `Return a JSON object containing the following value wrapped in an \"input\" key. Do not return anything else:\\n{input}`\n);\n\nconst badModel = new OpenAI({\n  maxRetries: 0,\n  modelName: \"text-ada-001\",\n});\n\nconst normalModel = new ChatOpenAI({\n  modelName: \"gpt-4\",\n});\n\nconst outputParser = StructuredOutputParser.fromZodSchema(\n  z.object({\n    input: z.string(),\n  })\n);\n\nconst badChain = prompt.pipe(badModel).pipe(outputParser);\n\nconst goodChain = prompt.pipe(normalModel).pipe(outputParser);\n\ntry {\n  const result = await badChain.invoke({\n    input: \"testing0\",\n  });\n} catch (e) {\n  console.log(e);\n  /*\n  OutputParserException [Error]: Failed to parse. Text: \"\n\n  { \"name\" : \" Testing0 \", \"lastname\" : \" testing \", \"fullname\" : \" testing \", \"role\" : \" test \", \"telephone\" : \"+1-555-555-555 \", \"email\" : \" testing@gmail.com \", \"role\" : \" test \", \"text\" : \" testing0 is different than testing \", \"role\" : \" test \", \"immediate_affected_version\" : \" 0.0.1 \", \"immediate_version\" : \" 1.0.0 \", \"leading_version\" : \" 1.0.0 \", \"version\" : \" 1.0.0 \", \"finger prick\" : \" no \", \"finger prick\" : \" s \", \"text\" : \" testing0 is different than testing \", \"role\" : \" test \", \"immediate_affected_version\" : \" 0.0.1 \", \"immediate_version\" : \" 1.0.0 \", \"leading_version\" : \" 1.0.0 \", \"version\" : \" 1.0.0 \", \"finger prick\" :\". Error: SyntaxError: Unexpected end of JSON input\n*/\n}\n\nconst chain = badChain.withFallbacks({\n  fallbacks: [goodChain],\n});\n\nconst result = await chain.invoke({\n  input: \"testing\",\n});\n\nconsole.log(result);\n\n/*\n  { input: 'testing' }\n*/","metadata":{"source":"examples/src/guides/fallbacks/better_model.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":55}}}}],["c4a5d27a-c822-4cd3-b21b-208a254a6248",{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { StringOutputParser } from \"langchain/schema/output_parser\";\nimport { ChatPromptTemplate, PromptTemplate } from \"langchain/prompts\";\n\nconst chatPrompt = ChatPromptTemplate.fromMessages<{ animal: string }>([\n  [\n    \"system\",\n    \"You're a nice assistant who always includes a compliment in your response\",\n  ],\n  [\"human\", \"Why did the {animal} cross the road?\"],\n]);\n\n// Use a fake model name that will always throw an error\nconst fakeOpenAIChatModel = new ChatOpenAI({\n  modelName: \"potato!\",\n  maxRetries: 0,\n});\n\nconst prompt =\n  PromptTemplate.fromTemplate(`Instructions: You should always include a compliment in your response.\n\nQuestion: Why did the {animal} cross the road?\n\nAnswer:`);\n\nconst openAILLM = new OpenAI({});\n\nconst outputParser = new StringOutputParser();\n\nconst badChain = chatPrompt.pipe(fakeOpenAIChatModel).pipe(outputParser);\n\nconst goodChain = prompt.pipe(openAILLM).pipe(outputParser);\n\nconst chain = badChain.withFallbacks({\n  fallbacks: [goodChain],\n});\n\nconst result = await chain.invoke({\n  animal: \"dragon\",\n});\n\nconsole.log(result);\n\n/*\n  I don't know, but I'm sure it was an impressive sight. You must have a great imagination to come up with such an interesting question!\n*/","metadata":{"source":"examples/src/guides/fallbacks/chain.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":47}}}}],["ef589390-ff03-4388-a705-aecb7a358941",{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\n\n// Use a model with a shorter context window\nconst shorterLlm = new ChatOpenAI({\n  modelName: \"gpt-3.5-turbo\",\n  maxRetries: 0,\n});\n\nconst longerLlm = new ChatOpenAI({\n  modelName: \"gpt-3.5-turbo-16k\",\n});\n\nconst modelWithFallback = shorterLlm.withFallbacks({\n  fallbacks: [longerLlm],\n});\n\nconst input = `What is the next number: ${\"one, two, \".repeat(3000)}`;\n\ntry {\n  await shorterLlm.invoke(input);\n} catch (e) {\n  // Length error\n  console.log(e);\n}\n\nconst result = await modelWithFallback.invoke(input);\n\nconsole.log(result);\n\n/*\n  AIMessage {\n    content: 'The next number is one.',\n    name: undefined,\n    additional_kwargs: { function_call: undefined }\n  }\n*/","metadata":{"source":"examples/src/guides/fallbacks/long_inputs.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":36}}}}],["f8b0f723-586c-4507-9ba6-ea74a558c010",{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { ChatAnthropic } from \"langchain/chat_models/anthropic\";\n\n// Use a fake model name that will always throw an error\nconst fakeOpenAIModel = new ChatOpenAI({\n  modelName: \"potato!\",\n  maxRetries: 0,\n});\n\nconst anthropicModel = new ChatAnthropic({});\n\nconst modelWithFallback = fakeOpenAIModel.withFallbacks({\n  fallbacks: [anthropicModel],\n});\n\nconst result = await modelWithFallback.invoke(\"What is your name?\");\n\nconsole.log(result);\n\n/*\n  AIMessage {\n    content: ' My name is Claude. I was created by Anthropic.',\n    additional_kwargs: {}\n  }\n*/","metadata":{"source":"examples/src/guides/fallbacks/model.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":25}}}}],["a9eec216-52f2-462e-a9e9-d2ee06bf9107",{"pageContent":"import { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\n\nconst text = `<!DOCTYPE html>\n<html>\n  <head>\n    <title>🦜️🔗 LangChain</title>\n    <style>\n      body {\n        font-family: Arial, sans-serif;\n      }\n      h1 {\n        color: darkblue;\n      }\n    </style>\n  </head>\n  <body>\n    <div>\n      <h1>🦜️🔗 LangChain</h1>\n      <p>⚡ Building applications with LLMs through composability ⚡</p>\n    </div>\n    <div>\n      As an open source project in a rapidly developing field, we are extremely open to contributions.\n    </div>\n  </body>\n</html>`;\n\nconst splitter = RecursiveCharacterTextSplitter.fromLanguage(\"html\", {\n  chunkSize: 175,\n  chunkOverlap: 20,\n});\nconst output = await splitter.createDocuments([text]);\n\nconsole.log(output);\n\n/*\n  [\n    Document {\n      pageContent: '<!DOCTYPE html>\\n<html>',\n      metadata: { loc: [Object] }\n    },\n    Document {\n      pageContent: '<head>\\n    <title>🦜️🔗 LangChain</title>',\n      metadata: { loc: [Object] }\n    },\n    Document {\n      pageContent: '<style>\\n' +\n        '      body {\\n' +\n        '        font-family: Arial, sans-serif;\\n' +\n        '      }\\n' +\n        '      h1 {\\n' +\n        '        color: darkblue;\\n' +\n        '      }\\n' +\n        '    </style>\\n' +\n        '  </head>',\n      metadata: { loc: [Object] }\n    },\n    Document {\n      pageContent: '<body>\\n' +\n        '    <div>\\n' +\n        '      <h1>🦜️🔗 LangChain</h1>\\n' +\n        '      <p>⚡ Building applications with LLMs through composability ⚡</p>\\n' +\n        '    </div>',\n      metadata: { loc: [Object] }\n    },\n    Document {\n      pageContent: '<div>\\n' +\n        '      As an open source project in a rapidly developing field, we are extremely open to contributions.\\n' +\n        '    </div>\\n' +\n        '  </body>\\n' +\n        '</html>',\n      metadata: { loc: [Object] }\n    }\n  ]\n*/","metadata":{"source":"examples/src/indexes/html_text_splitter.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":74}}}}],["43359ee9-b15a-4b99-a872-ba3548455c65",{"pageContent":"import {\n  SupportedTextSplitterLanguages,\n  RecursiveCharacterTextSplitter,\n} from \"langchain/text_splitter\";\n\nconsole.log(SupportedTextSplitterLanguages); // Array of supported languages\n\n/*\n  [\n    'cpp',      'go',\n    'java',     'js',\n    'php',      'proto',\n    'python',   'rst',\n    'ruby',     'rust',\n    'scala',    'swift',\n    'markdown', 'latex',\n    'html'\n  ]\n*/\n\nconst jsCode = `function helloWorld() {\n  console.log(\"Hello, World!\");\n}\n// Call the function\nhelloWorld();`;\n\nconst splitter = RecursiveCharacterTextSplitter.fromLanguage(\"js\", {\n  chunkSize: 32,\n  chunkOverlap: 0,\n});\nconst jsOutput = await splitter.createDocuments([jsCode]);\n\nconsole.log(jsOutput);\n\n/*\n  [\n    Document {\n      pageContent: 'function helloWorld() {',\n      metadata: { loc: [Object] }\n    },\n    Document {\n      pageContent: 'console.log(\"Hello, World!\");',\n      metadata: { loc: [Object] }\n    },\n    Document {\n      pageContent: '}\\n// Call the function',\n      metadata: { loc: [Object] }\n    },\n    Document {\n      pageContent: 'helloWorld();',\n      metadata: { loc: [Object] }\n    }\n  ]\n*/","metadata":{"source":"examples/src/indexes/javascript_text_splitter.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":54}}}}],["01b5a59d-a03f-45f9-bbb4-4f10f6cc76ef",{"pageContent":"import { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\n\nconst text = `\\\\begin{document}\n\\\\title{🦜️🔗 LangChain}\n⚡ Building applications with LLMs through composability ⚡\n\n\\\\section{Quick Install}\n\n\\\\begin{verbatim}\nHopefully this code block isn't split\nyarn add langchain\n\\\\end{verbatim}\n\nAs an open source project in a rapidly developing field, we are extremely open to contributions.\n\n\\\\end{document}`;\n\nconst splitter = RecursiveCharacterTextSplitter.fromLanguage(\"latex\", {\n  chunkSize: 100,\n  chunkOverlap: 0,\n});\nconst output = await splitter.createDocuments([text]);\n\nconsole.log(output);\n\n/*\n  [\n    Document {\n      pageContent: '\\\\begin{document}\\n' +\n        '\\\\title{🦜️🔗 LangChain}\\n' +\n        '⚡ Building applications with LLMs through composability ⚡',\n      metadata: { loc: [Object] }\n    },\n    Document {\n      pageContent: '\\\\section{Quick Install}',\n      metadata: { loc: [Object] }\n    },\n    Document {\n      pageContent: '\\\\begin{verbatim}\\n' +\n        \"Hopefully this code block isn't split\\n\" +\n        'yarn add langchain\\n' +\n        '\\\\end{verbatim}',\n      metadata: { loc: [Object] }\n    },\n    Document {\n      pageContent: 'As an open source project in a rapidly developing field, we are extremely open to contributions.',\n      metadata: { loc: [Object] }\n    },\n    Document {\n      pageContent: '\\\\end{document}',\n      metadata: { loc: [Object] }\n    }\n  ]\n*/","metadata":{"source":"examples/src/indexes/latex_text_splitter.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":54}}}}],["87207d79-b99e-488c-a385-442aba02a905",{"pageContent":"import { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\n\nconst text = `\n---\nsidebar_position: 1\n---\n# Document transformers\n\nOnce you've loaded documents, you'll often want to transform them to better suit your application. The simplest example\nis you may want to split a long document into smaller chunks that can fit into your model's context window. LangChain\nhas a number of built-in document transformers that make it easy to split, combine, filter, and otherwise manipulate documents.\n\n## Text splitters\n\nWhen you want to deal with long pieces of text, it is necessary to split up that text into chunks.\nAs simple as this sounds, there is a lot of potential complexity here. Ideally, you want to keep the semantically related pieces of text together. What \"semantically related\" means could depend on the type of text.\nThis notebook showcases several ways to do that.\n\nAt a high level, text splitters work as following:\n\n1. Split the text up into small, semantically meaningful chunks (often sentences).\n2. Start combining these small chunks into a larger chunk until you reach a certain size (as measured by some function).\n3. Once you reach that size, make that chunk its own piece of text and then start creating a new chunk of text with some overlap (to keep context between chunks).\n\nThat means there are two different axes along which you can customize your text splitter:\n\n1. How the text is split\n2. How the chunk size is measured\n\n## Get started with text splitters\n\nimport GetStarted from \"@snippets/modules/data_connection/document_transformers/get_started.mdx\"\n\n<GetStarted/>\n`;\n\nconst splitter = RecursiveCharacterTextSplitter.fromLanguage(\"markdown\", {\n  chunkSize: 500,\n  chunkOverlap: 0,\n});\nconst output = await splitter.createDocuments([text]);\n\nconsole.log(output);\n\n/*\n  [\n    Document {\n      pageContent: '---\\n' +\n        'sidebar_position: 1\\n' +\n        '---\\n' +\n        '# Document transformers\\n' +\n        '\\n' +\n        \"Once you've loaded documents, you'll often want to transform them to better suit your application. The simplest example\\n\" +\n        \"is you may want to split a long document into smaller chunks that can fit into your model's context window. LangChain\\n\" +\n        'has a number of built-in document transformers that make it easy to split, combine, filter, and otherwise manipulate documents.',\n      metadata: { loc: [Object] }\n    },\n    Document {\n      pageContent: '## Text splitters\\n' +\n        '\\n' +\n        'When you want to deal with long pieces of text, it is necessary to split up that text into chunks.\\n' +\n        'As simple as this sounds, there is a lot of potential complexity here. Ideally, you want to keep the semantically related pieces of text together. What \"semantically related\" means could depend on the type of text.\\n' +\n        'This notebook showcases several ways to do that.\\n' +\n        '\\n' +\n        'At a high level, text splitters work as following:',\n      metadata: { loc: [Object] }\n    },\n    Document {\n      pageContent: '1. Split the text up into small, semantically meaningful chunks (often sentences).\\n' +\n        '2. Start combining these small chunks into a larger chunk until you reach a certain size (as measured by some function).\\n' +\n        '3. Once you reach that size, make that chunk its own piece of text and then start creating a new chunk of text with some overlap (to keep context between chunks).\\n' +\n        '\\n' +\n        'That means there are two different axes along which you can customize your text splitter:',\n      metadata: { loc: [Object] }\n    },\n    Document {\n      pageContent: '1. How the text is split\\n2. How the chunk size is measured',\n      metadata: { loc: [Object] }\n    },\n    Document {\n      pageContent: '## Get started with text splitters\\n' +\n        '\\n' +\n        'import GetStarted from \"@snippets/modules/data_connection/document_transformers/get_started.mdx\"\\n' +\n        '\\n' +\n        '<GetStarted/>',\n      metadata: { loc: [Object] }\n    }\n  ]\n*/","metadata":{"source":"examples/src/indexes/markdown_text_splitter.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":89}}}}],["e1e16455-44e9-4004-8b57-afde28fc9478",{"pageContent":"import { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\n\nconst pythonCode = `def hello_world():\n  print(\"Hello, World!\")\n# Call the function\nhello_world()`;\n\nconst splitter = RecursiveCharacterTextSplitter.fromLanguage(\"python\", {\n  chunkSize: 32,\n  chunkOverlap: 0,\n});\n\nconst pythonOutput = await splitter.createDocuments([pythonCode]);\n\nconsole.log(pythonOutput);\n\n/*\n  [\n    Document {\n      pageContent: 'def hello_world():',\n      metadata: { loc: [Object] }\n    },\n    Document {\n      pageContent: 'print(\"Hello, World!\")',\n      metadata: { loc: [Object] }\n    },\n    Document {\n      pageContent: '# Call the function',\n      metadata: { loc: [Object] }\n    },\n    Document {\n      pageContent: 'hello_world()',\n      metadata: { loc: [Object] }\n    }\n  ]\n*/","metadata":{"source":"examples/src/indexes/python_text_splitter.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":36}}}}],["d58c3184-3860-44a6-a613-ca9587d747cc",{"pageContent":"import { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\n\nexport const run = async () => {\n  const text = `Hi.\\n\\nI'm Harrison.\\n\\nHow? Are? You?\\nOkay then f f f f.\n    This is a weird text to write, but gotta test the splittingggg some how.\\n\\n\n    Bye!\\n\\n-H.`;\n  const splitter = new RecursiveCharacterTextSplitter({\n    chunkSize: 10,\n    chunkOverlap: 1,\n  });\n  const output = await splitter.createDocuments([text]);\n  console.log(output);\n};","metadata":{"source":"examples/src/indexes/recursive_text_splitter.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":13}}}}],["79cabd54-0dc0-4554-9d39-5e645a79b4f7",{"pageContent":"import { Document } from \"langchain/document\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\n\nconst text = `Some other considerations include:\n\n- Do you deploy your backend and frontend together, or separately?\n- Do you deploy your backend co-located with your database, or separately?\n\n**Production Support:** As you move your LangChains into production, we'd love to offer more hands-on support.\nFill out [this form](https://airtable.com/appwQzlErAS2qiP0L/shrGtGaVBVAz7NcV2) to share more about what you're building, and our team will get in touch.\n\n## Deployment Options\n\nSee below for a list of deployment options for your LangChain app. If you don't see your preferred option, please get in touch and we can add it to this list.`;\n\nconst splitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 50,\n  chunkOverlap: 1,\n  separators: [\"|\", \"##\", \">\", \"-\"],\n});\n\nconst docOutput = await splitter.splitDocuments([\n  new Document({ pageContent: text }),\n]);\n\nconsole.log(docOutput);\n\n/*\n  [\n    Document {\n      pageContent: 'Some other considerations include:',\n      metadata: { loc: [Object] }\n    },\n    Document {\n      pageContent: '- Do you deploy your backend and frontend together',\n      metadata: { loc: [Object] }\n    },\n    Document {\n      pageContent: 'r, or separately?',\n      metadata: { loc: [Object] }\n    },\n    Document {\n      pageContent: '- Do you deploy your backend co',\n      metadata: { loc: [Object] }\n    },\n    Document {\n      pageContent: '-located with your database, or separately?\\n\\n**Pro',\n      metadata: { loc: [Object] }\n    },\n    Document {\n      pageContent: 'oduction Support:** As you move your LangChains in',\n      metadata: { loc: [Object] }\n    },\n    Document {\n      pageContent: \"nto production, we'd love to offer more hands\",\n      metadata: { loc: [Object] }\n    },\n    Document {\n      pageContent: '-on support.\\nFill out [this form](https://airtable',\n      metadata: { loc: [Object] }\n    },\n    Document {\n      pageContent: 'e.com/appwQzlErAS2qiP0L/shrGtGaVBVAz7NcV2) to shar',\n      metadata: { loc: [Object] }\n    },\n    Document {\n      pageContent: \"re more about what you're building, and our team w\",\n      metadata: { loc: [Object] }\n    },\n    Document {\n      pageContent: 'will get in touch.',\n      metadata: { loc: [Object] }\n    },\n    Document { pageContent: '#', metadata: { loc: [Object] } },\n    Document {\n      pageContent: '# Deployment Options\\n' +\n        '\\n' +\n        \"See below for a list of deployment options for your LangChain app. If you don't see your preferred option, please get in touch and we can add it to this list.\",\n      metadata: { loc: [Object] }\n    }\n  ]\n*/","metadata":{"source":"examples/src/indexes/recursive_text_splitter_custom_separators.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":82}}}}],["8236ebb7-2f2a-4a0a-b7f2-05177b1b724d",{"pageContent":"import { Document } from \"langchain/document\";\nimport { CharacterTextSplitter } from \"langchain/text_splitter\";\n\nexport const run = async () => {\n  /* Split text */\n  const text = \"foo bar baz 123\";\n  const splitter = new CharacterTextSplitter({\n    separator: \" \",\n    chunkSize: 7,\n    chunkOverlap: 3,\n  });\n  const output = await splitter.createDocuments([text]);\n  console.log({ output });\n  /* Split documents */\n  const docOutput = await splitter.splitDocuments([\n    new Document({ pageContent: text }),\n  ]);\n  console.log({ docOutput });\n};","metadata":{"source":"examples/src/indexes/text_splitter.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":19}}}}],["0df2a4c7-99d7-4d83-bb9c-ac0c26979c38",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { RetrievalQAChain, loadQAStuffChain } from \"langchain/chains\";\nimport { CharacterTextSplitter } from \"langchain/text_splitter\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { HNSWLib } from \"langchain/vectorstores/hnswlib\";\n\nconst splitter = new CharacterTextSplitter({\n  chunkSize: 1536,\n  chunkOverlap: 200,\n});\n\nconst jimDocs = await splitter.createDocuments(\n  [`My favorite color is blue.`],\n  [],\n  {\n    chunkHeader: `DOCUMENT NAME: Jim Interview\\n\\n---\\n\\n`,\n    appendChunkOverlapHeader: true,\n  }\n);\n\nconst pamDocs = await splitter.createDocuments(\n  [`My favorite color is red.`],\n  [],\n  {\n    chunkHeader: `DOCUMENT NAME: Pam Interview\\n\\n---\\n\\n`,\n    appendChunkOverlapHeader: true,\n  }\n);\n\nconst vectorStore = await HNSWLib.fromDocuments(\n  jimDocs.concat(pamDocs),\n  new OpenAIEmbeddings()\n);\n\nconst model = new OpenAI({ temperature: 0 });\n\nconst chain = new RetrievalQAChain({\n  combineDocumentsChain: loadQAStuffChain(model),\n  retriever: vectorStore.asRetriever(),\n  returnSourceDocuments: true,\n});\nconst res = await chain.call({\n  query: \"What is Pam's favorite color?\",\n});\n\nconsole.log(JSON.stringify(res, null, 2));\n\n/*\n  {\n    \"text\": \" Red.\",\n    \"sourceDocuments\": [\n      {\n        \"pageContent\": \"DOCUMENT NAME: Pam Interview\\n\\n---\\n\\nMy favorite color is red.\",\n        \"metadata\": {\n          \"loc\": {\n            \"lines\": {\n              \"from\": 1,\n              \"to\": 1\n            }\n          }\n        }\n      },\n      {\n        \"pageContent\": \"DOCUMENT NAME: Jim Interview\\n\\n---\\n\\nMy favorite color is blue.\",\n        \"metadata\": {\n          \"loc\": {\n            \"lines\": {\n              \"from\": 1,\n              \"to\": 1\n            }\n          }\n        }\n      }\n    ]\n  }\n*/","metadata":{"source":"examples/src/indexes/text_splitter_with_chunk_header.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":76}}}}],["320a0859-77fd-454d-962e-eb55e4b1adec",{"pageContent":"import { Document } from \"langchain/document\";\nimport { TokenTextSplitter } from \"langchain/text_splitter\";\nimport fs from \"fs\";\nimport path from \"path\";\n\nexport const run = async () => {\n  /* Split text */\n  const text = fs.readFileSync(\n    path.resolve(__dirname, \"../../state_of_the_union.txt\"),\n    \"utf8\"\n  );\n\n  const splitter = new TokenTextSplitter({\n    encodingName: \"r50k_base\",\n    chunkSize: 10,\n    chunkOverlap: 0,\n    allowedSpecial: [\"<|endoftext|>\"],\n    disallowedSpecial: [],\n  });\n\n  const output = await splitter.createDocuments([text]);\n  console.log({ output });\n\n  const docOutput = await splitter.splitDocuments([\n    new Document({ pageContent: text }),\n  ]);\n\n  console.log({ docOutput });\n};","metadata":{"source":"examples/src/indexes/token_text_splitter.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":29}}}}],["2bfd2127-77b2-4db2-b498-313b1dea2f81",{"pageContent":"import { AnalyticDBVectorStore } from \"langchain/vectorstores/analyticdb\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nconst connectionOptions = {\n  host: process.env.ANALYTICDB_HOST || \"localhost\",\n  port: Number(process.env.ANALYTICDB_PORT) || 5432,\n  database: process.env.ANALYTICDB_DATABASE || \"your_database\",\n  user: process.env.ANALYTICDB_USERNAME || \"username\",\n  password: process.env.ANALYTICDB_PASSWORD || \"password\",\n};\n\nconst vectorStore = await AnalyticDBVectorStore.fromTexts(\n  [\"foo\", \"bar\", \"baz\"],\n  [{ page: 1 }, { page: 2 }, { page: 3 }],\n  new OpenAIEmbeddings(),\n  { connectionOptions }\n);\nconst result = await vectorStore.similaritySearch(\"foo\", 1);\nconsole.log(JSON.stringify(result));\n// [{\"pageContent\":\"foo\",\"metadata\":{\"page\":1}}]\n\nawait vectorStore.addDocuments([{ pageContent: \"foo\", metadata: { page: 4 } }]);\n\nconst filterResult = await vectorStore.similaritySearch(\"foo\", 1, {\n  page: 4,\n});\nconsole.log(JSON.stringify(filterResult));\n// [{\"pageContent\":\"foo\",\"metadata\":{\"page\":4}}]\n\nconst filterWithScoreResult = await vectorStore.similaritySearchWithScore(\n  \"foo\",\n  1,\n  { page: 3 }\n);\nconsole.log(JSON.stringify(filterWithScoreResult));\n// [[{\"pageContent\":\"baz\",\"metadata\":{\"page\":3}},0.26075905561447144]]\n\nconst filterNoMatchResult = await vectorStore.similaritySearchWithScore(\n  \"foo\",\n  1,\n  { page: 5 }\n);\nconsole.log(JSON.stringify(filterNoMatchResult));\n// []\n\n// need to manually close the Connection pool\nawait vectorStore.end();","metadata":{"source":"examples/src/indexes/vector_stores/analyticdb.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":47}}}}],["6a51df89-13c3-4398-af63-e0c5e3ef21b3",{"pageContent":"// If you want to import the browser version, use the following line instead:\n// import { CloseVectorWeb } from \"langchain/vectorstores/closevector/web\";\nimport { CloseVectorNode } from \"langchain/vectorstores/closevector/node\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nexport const run = async () => {\n  // If you want to import the browser version, use the following line instead:\n  // const vectorStore = await CloseVectorWeb.fromTexts(\n  const vectorStore = await CloseVectorNode.fromTexts(\n    [\"Hello world\", \"Bye bye\", \"hello nice world\"],\n    [{ id: 2 }, { id: 1 }, { id: 3 }],\n    new OpenAIEmbeddings()\n  );\n\n  const resultOne = await vectorStore.similaritySearch(\"hello world\", 1);\n  console.log(resultOne);\n};","metadata":{"source":"examples/src/indexes/vector_stores/closevector.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":17}}}}],["3106c4f1-7980-4751-b105-374a853c8870",{"pageContent":"// If you want to import the browser version, use the following line instead:\n// import { CloseVectorWeb } from \"langchain/vectorstores/closevector/web\";\nimport { CloseVectorNode } from \"langchain/vectorstores/closevector/node\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\n\n// Create docs with a loader\nconst loader = new TextLoader(\"src/document_loaders/example_data/example.txt\");\nconst docs = await loader.load();\n\n// Load the docs into the vector store\n// If you want to import the browser version, use the following line instead:\n// const vectorStore = await CloseVectorWeb.fromDocuments(\nconst vectorStore = await CloseVectorNode.fromDocuments(\n  docs,\n  new OpenAIEmbeddings()\n);\n\n// Search for the most similar document\nconst resultOne = await vectorStore.similaritySearch(\"hello world\", 1);\nconsole.log(resultOne);","metadata":{"source":"examples/src/indexes/vector_stores/closevector_fromdocs.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":21}}}}],["ef4a5bda-ca1e-4db5-84d5-dad2ba5710b6",{"pageContent":"// If you want to import the browser version, use the following line instead:\n// import { CloseVectorWeb } from \"langchain/vectorstores/closevector/web\";\nimport { CloseVectorNode } from \"langchain/vectorstores/closevector/node\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\n// Create a vector store through any method, here from texts as an example\n// If you want to import the browser version, use the following line instead:\n// const vectorStore = await CloseVectorWeb.fromTexts(\nconst vectorStore = await CloseVectorNode.fromTexts(\n  [\"Hello world\", \"Bye bye\", \"hello nice world\"],\n  [{ id: 2 }, { id: 1 }, { id: 3 }],\n  new OpenAIEmbeddings()\n);\n\n// Save the vector store to a directory\nconst directory = \"your/directory/here\";\n\nawait vectorStore.save(directory);\n\n// Load the vector store from the same directory\n// If you want to import the browser version, use the following line instead:\n// const loadedVectorStore = await CloseVectorWeb.load(\nconst loadedVectorStore = await CloseVectorNode.load(\n  directory,\n  new OpenAIEmbeddings()\n);\n\n// vectorStore and loadedVectorStore are identical\nconst result = await loadedVectorStore.similaritySearch(\"hello world\", 1);\nconsole.log(result);","metadata":{"source":"examples/src/indexes/vector_stores/closevector_saveload.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":30}}}}],["af87a856-1cef-4db8-ab48-8c2b66f17870",{"pageContent":"// If you want to import the browser version, use the following line instead:\n// import { CloseVectorWeb } from \"langchain/vectorstores/closevector/web\";\nimport { CloseVectorNode } from \"langchain/vectorstores/closevector/node\";\nimport { CloseVectorWeb } from \"langchain/vectorstores/closevector/web\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n// eslint-disable-next-line import/no-extraneous-dependencies\nimport { createPublicGetFileOperationUrl } from \"closevector-web\";\n\n// Create a vector store through any method, here from texts as an example\n// If you want to import the browser version, use the following line instead:\n// const vectorStore = await CloseVectorWeb.fromTexts(\nconst vectorStore = await CloseVectorNode.fromTexts(\n  [\"Hello world\", \"Bye bye\", \"hello nice world\"],\n  [{ id: 2 }, { id: 1 }, { id: 3 }],\n  new OpenAIEmbeddings(),\n  undefined,\n  {\n    key: \"your access key\",\n    secret: \"your secret\",\n  }\n);\n\n// Save the vector store to cloud\nawait vectorStore.saveToCloud({\n  description: \"example\",\n  public: true,\n});\n\nconst { uuid } = vectorStore.instance;\n\n// Load the vector store from cloud\n// const loadedVectorStore = await CloseVectorWeb.load(\nconst loadedVectorStore = await CloseVectorNode.loadFromCloud({\n  uuid,\n  embeddings: new OpenAIEmbeddings(),\n  credentials: {\n    key: \"your access key\",\n    secret: \"your secret\",\n  },\n});\n\n// If you want to import the node version, use the following lines instead:\n// const loadedVectorStoreOnNode = await CloseVectorNode.loadFromCloud({\n//   uuid,\n//   embeddings: new OpenAIEmbeddings(),\n//   credentials: {\n//     key: \"your access key\",\n//     secret: \"your secret\"\n//   }\n// });\n\nconst loadedVectorStoreOnBrowser = await CloseVectorWeb.loadFromCloud({\n  url: (\n    await createPublicGetFileOperationUrl({\n      uuid,\n      accessKey: \"your access key\",\n    })\n  ).url,\n  uuid,\n  embeddings: new OpenAIEmbeddings(),\n});\n\n// vectorStore and loadedVectorStore are identical\nconst result = await loadedVectorStore.similaritySearch(\"hello world\", 1);\nconsole.log(result);\n\n// or\nconst resultOnBrowser = await loadedVectorStoreOnBrowser.similaritySearch(\n  \"hello world\",\n  1\n);\nconsole.log(resultOnBrowser);","metadata":{"source":"examples/src/indexes/vector_stores/closevector_saveload_fromcloud.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":72}}}}],["73c91241-c3ff-448a-9d9d-859ecb8b2426",{"pageContent":"import { FaissStore } from \"langchain/vectorstores/faiss\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nexport const run = async () => {\n  const vectorStore = await FaissStore.fromTexts(\n    [\"Hello world\", \"Bye bye\", \"hello nice world\"],\n    [{ id: 2 }, { id: 1 }, { id: 3 }],\n    new OpenAIEmbeddings()\n  );\n\n  const resultOne = await vectorStore.similaritySearch(\"hello world\", 1);\n  console.log(resultOne);\n};","metadata":{"source":"examples/src/indexes/vector_stores/faiss.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":13}}}}],["f36c7a06-8622-4b33-87dc-e29494d72fb6",{"pageContent":"import { FaissStore } from \"langchain/vectorstores/faiss\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { Document } from \"langchain/document\";\n\nconst vectorStore = new FaissStore(new OpenAIEmbeddings(), {});\nconst ids = [\"2\", \"1\", \"4\"];\nconst idsReturned = await vectorStore.addDocuments(\n  [\n    new Document({\n      pageContent: \"my world\",\n      metadata: { tag: 2 },\n    }),\n    new Document({\n      pageContent: \"our world\",\n      metadata: { tag: 1 },\n    }),\n    new Document({\n      pageContent: \"your world\",\n      metadata: { tag: 4 },\n    }),\n  ],\n  {\n    ids,\n  }\n);\n\nconsole.log(idsReturned);\n\n/*\n  [ '2', '1', '4' ]\n*/\n\nconst docs = await vectorStore.similaritySearch(\"my world\", 3);\n\nconsole.log(docs);\n\n/*\n[\n  Document { pageContent: 'my world', metadata: { tag: 2 } },\n  Document { pageContent: 'your world', metadata: { tag: 4 } },\n  Document { pageContent: 'our world', metadata: { tag: 1 } }\n]\n*/\n\nawait vectorStore.delete({ ids: [ids[0], ids[1]] });\n\nconst docs2 = await vectorStore.similaritySearch(\"my world\", 3);\n\nconsole.log(docs2);\n\n/*\n[ Document { pageContent: 'your world', metadata: { tag: 4 } } ]\n*/","metadata":{"source":"examples/src/indexes/vector_stores/faiss_delete.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":53}}}}],["9bd273d3-c587-4d58-830f-2024661a58a2",{"pageContent":"import { FaissStore } from \"langchain/vectorstores/faiss\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\n\n// Create docs with a loader\nconst loader = new TextLoader(\"src/document_loaders/example_data/example.txt\");\nconst docs = await loader.load();\n\n// Load the docs into the vector store\nconst vectorStore = await FaissStore.fromDocuments(\n  docs,\n  new OpenAIEmbeddings()\n);\n\n// Search for the most similar document\nconst resultOne = await vectorStore.similaritySearch(\"hello world\", 1);\nconsole.log(resultOne);","metadata":{"source":"examples/src/indexes/vector_stores/faiss_fromdocs.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":17}}}}],["e90d4f41-08f6-4146-9bbb-be7b73f532ef",{"pageContent":"import { FaissStore } from \"langchain/vectorstores/faiss\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\n// The directory of data saved from Python\nconst directory = \"your/directory/here\";\n\n// Load the vector store from the directory\nconst loadedVectorStore = await FaissStore.loadFromPython(\n  directory,\n  new OpenAIEmbeddings()\n);\n\n// Search for the most similar document\nconst result = await loadedVectorStore.similaritySearch(\"test\", 2);\nconsole.log(\"result\", result);","metadata":{"source":"examples/src/indexes/vector_stores/faiss_loadfrompython.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":15}}}}],["687b2154-1cf2-43b5-973d-c5d7690d1015",{"pageContent":"import { FaissStore } from \"langchain/vectorstores/faiss\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nexport const run = async () => {\n  // Create an initial vector store\n  const vectorStore = await FaissStore.fromTexts(\n    [\"Hello world\", \"Bye bye\", \"hello nice world\"],\n    [{ id: 2 }, { id: 1 }, { id: 3 }],\n    new OpenAIEmbeddings()\n  );\n\n  // Create another vector store from texts\n  const vectorStore2 = await FaissStore.fromTexts(\n    [\"Some text\"],\n    [{ id: 1 }],\n    new OpenAIEmbeddings()\n  );\n\n  // merge the first vector store into vectorStore2\n  await vectorStore2.mergeFrom(vectorStore);\n\n  const resultOne = await vectorStore2.similaritySearch(\"hello world\", 1);\n  console.log(resultOne);\n\n  // You can also create a new vector store from another FaissStore index\n  const vectorStore3 = await FaissStore.fromIndex(\n    vectorStore2,\n    new OpenAIEmbeddings()\n  );\n  const resultTwo = await vectorStore3.similaritySearch(\"Bye bye\", 1);\n  console.log(resultTwo);\n};","metadata":{"source":"examples/src/indexes/vector_stores/faiss_mergefrom.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":32}}}}],["4d9df60a-448d-40fc-a56a-922680e6f8e9",{"pageContent":"import { FaissStore } from \"langchain/vectorstores/faiss\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\n// Create a vector store through any method, here from texts as an example\nconst vectorStore = await FaissStore.fromTexts(\n  [\"Hello world\", \"Bye bye\", \"hello nice world\"],\n  [{ id: 2 }, { id: 1 }, { id: 3 }],\n  new OpenAIEmbeddings()\n);\n\n// Save the vector store to a directory\nconst directory = \"your/directory/here\";\n\nawait vectorStore.save(directory);\n\n// Load the vector store from the same directory\nconst loadedVectorStore = await FaissStore.load(\n  directory,\n  new OpenAIEmbeddings()\n);\n\n// vectorStore and loadedVectorStore are identical\nconst result = await loadedVectorStore.similaritySearch(\"hello world\", 1);\nconsole.log(result);","metadata":{"source":"examples/src/indexes/vector_stores/faiss_saveload.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":24}}}}],["1d2376df-8f0f-444e-9b53-211c3dbafdd1",{"pageContent":"/* eslint-disable no-process-env */\n/* eslint-disable @typescript-eslint/no-non-null-assertion */\nimport { SyntheticEmbeddings } from \"langchain/embeddings/fake\";\nimport { GoogleCloudStorageDocstore } from \"langchain/stores/doc/gcs\";\nimport { Document } from \"langchain/document\";\nimport {\n  MatchingEngineArgs,\n  MatchingEngine,\n  IdDocument,\n  Restriction,\n} from \"langchain/vectorstores/googlevertexai\";\n\nexport const run = async () => {\n  if (\n    !process.env.GOOGLE_VERTEXAI_MATCHINGENGINE_INDEX ||\n    !process.env.GOOGLE_VERTEXAI_MATCHINGENGINE_INDEXENDPOINT ||\n    !process.env.GOOGLE_CLOUD_STORAGE_BUCKET\n  ) {\n    throw new Error(\n      \"GOOGLE_VERTEXAI_MATCHINGENGINE_INDEX, GOOGLE_VERTEXAI_MATCHINGENGINE_INDEXENDPOINT, and GOOGLE_CLOUD_STORAGE_BUCKET must be set.\"\n    );\n  }\n\n  const embeddings = new SyntheticEmbeddings({\n    vectorSize: Number.parseInt(\n      process.env.SYNTHETIC_EMBEDDINGS_VECTOR_SIZE ?? \"768\",\n      10\n    ),\n  });\n\n  const store = new GoogleCloudStorageDocstore({\n    bucket: process.env.GOOGLE_CLOUD_STORAGE_BUCKET!,\n  });\n\n  const config: MatchingEngineArgs = {\n    index: process.env.GOOGLE_VERTEXAI_MATCHINGENGINE_INDEX!,\n    indexEndpoint: process.env.GOOGLE_VERTEXAI_MATCHINGENGINE_INDEXENDPOINT!,\n    apiVersion: \"v1beta1\",\n    docstore: store,\n  };\n\n  const engine = new MatchingEngine(embeddings, config);\n\n  /*\n   * Simple document add\n   */\n  const doc = new Document({ pageContent: \"this\" });\n  await engine.addDocuments([doc]);\n\n  /*\n   * Simple search.\n   * Returns documents including an id field\n   */\n  const oldResults: IdDocument[] = await engine.similaritySearch(\"this\");\n  console.log(\"simple results\", oldResults);\n  /*\n    [\n      Document {\n        pageContent: 'this',\n        metadata: {},\n        id: 'c05d4249-9ddc-4ed9-8b0c-adf344500c2b'\n      }\n    ]\n   */\n\n  /*\n   * Delete the results\n   */\n  const oldIds = oldResults.map((doc) => doc.id!);\n  await engine.delete({ ids: oldIds });\n\n  /*\n   * Documents with metadata\n   */\n  const documents = [\n    new Document({\n      pageContent: \"this apple\",\n      metadata: {\n        color: \"red\",\n        category: \"edible\",\n      },\n    }),\n    new Document({\n      pageContent: \"this blueberry\",\n      metadata: {\n        color: \"blue\",\n        category: \"edible\",\n      },\n    }),\n    new Document({\n      pageContent: \"this firetruck\",\n      metadata: {\n        color: \"red\",\n        category: \"machine\",\n      },\n    }),\n  ];\n\n  // Add all our documents\n  await engine.addDocuments(documents);\n\n  /*\n   * Documents that match \"color == red\"\n   */\n  const redFilter: Restriction[] = [\n    {\n      namespace: \"color\",\n      allowList: [\"red\"],\n    },\n  ];\n  const redResults = await engine.similaritySearch(\"this\", 4, redFilter);\n  console.log(\"red results\", redResults);\n  /*\n    [\n      Document {\n        pageContent: 'this apple',\n        metadata: { color: 'red', category: 'edible' },\n        id: '724ff599-31ea-4094-8d60-158faf3c3f32'\n      },\n      Document {\n        pageContent: 'this firetruck',\n        metadata: { color: 'red', category: 'machine' },\n        id: 'a3c039f3-4ca1-43b3-97d8-c33dfe75bd31'\n      }\n    ]\n   */\n\n  /*\n   * Documents that match \"color == red AND category != edible\"\n   */\n  const redNotEditableFilter: Restriction[] = [\n    {\n      namespace: \"color\",\n      allowList: [\"red\"],\n    },\n    {\n      namespace: \"category\",\n      denyList: [\"edible\"],\n    },\n  ];\n  const redNotEdibleResults = await engine.similaritySearch(\n    \"this\",\n    4,\n    redNotEditableFilter\n  );\n  console.log(\"red not edible results\", redNotEdibleResults);\n  /*\n    [\n      Document {\n        pageContent: 'this apple',\n        metadata: { color: 'red', category: 'edible' },\n        id: '724ff599-31ea-4094-8d60-158faf3c3f32'\n      }\n    ]\n   */\n};","metadata":{"source":"examples/src/indexes/vector_stores/googlevertexai.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":156}}}}],["a2fbd8bb-b922-419d-8d5f-b588a1679de0",{"pageContent":"import { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nconst vectorStore = await HNSWLib.fromTexts(\n  [\"Hello world\", \"Bye bye\", \"hello nice world\"],\n  [{ id: 2 }, { id: 1 }, { id: 3 }],\n  new OpenAIEmbeddings()\n);\n\nconst resultOne = await vectorStore.similaritySearch(\"hello world\", 1);\nconsole.log(resultOne);","metadata":{"source":"examples/src/indexes/vector_stores/hnswlib.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":11}}}}],["3774b3d2-8e1e-458f-8119-4b4a43313e22",{"pageContent":"import { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\n// Save the vector store to a directory\nconst directory = \"your/directory/here\";\n\n// Load the vector store from the same directory\nconst loadedVectorStore = await HNSWLib.load(directory, new OpenAIEmbeddings());\n\nawait loadedVectorStore.delete({ directory });","metadata":{"source":"examples/src/indexes/vector_stores/hnswlib_delete.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":10}}}}],["f60fffb0-8257-496b-8638-0e5c37526b6b",{"pageContent":"import { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nconst vectorStore = await HNSWLib.fromTexts(\n  [\"Hello world\", \"Bye bye\", \"hello nice world\"],\n  [{ id: 2 }, { id: 1 }, { id: 3 }],\n  new OpenAIEmbeddings()\n);\n\nconst result = await vectorStore.similaritySearch(\n  \"hello world\",\n  10,\n  (document) => document.metadata.id === 3\n);\n\n// only \"hello nice world\" will be returned\nconsole.log(result);","metadata":{"source":"examples/src/indexes/vector_stores/hnswlib_filter.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":17}}}}],["181bf83b-0bb2-4b18-9573-87fc9985d6f2",{"pageContent":"import { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\n\n// Create docs with a loader\nconst loader = new TextLoader(\"src/document_loaders/example_data/example.txt\");\nconst docs = await loader.load();\n\n// Load the docs into the vector store\nconst vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());\n\n// Search for the most similar document\nconst result = await vectorStore.similaritySearch(\"hello world\", 1);\nconsole.log(result);","metadata":{"source":"examples/src/indexes/vector_stores/hnswlib_fromdocs.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":14}}}}],["30aa4096-0357-418f-a910-839e53788872",{"pageContent":"import { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\n// Create a vector store through any method, here from texts as an example\nconst vectorStore = await HNSWLib.fromTexts(\n  [\"Hello world\", \"Bye bye\", \"hello nice world\"],\n  [{ id: 2 }, { id: 1 }, { id: 3 }],\n  new OpenAIEmbeddings()\n);\n\n// Save the vector store to a directory\nconst directory = \"your/directory/here\";\nawait vectorStore.save(directory);\n\n// Load the vector store from the same directory\nconst loadedVectorStore = await HNSWLib.load(directory, new OpenAIEmbeddings());\n\n// vectorStore and loadedVectorStore are identical\n\nconst result = await loadedVectorStore.similaritySearch(\"hello world\", 1);\nconsole.log(result);","metadata":{"source":"examples/src/indexes/vector_stores/hnswlib_saveload.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":21}}}}],["07ec4dac-063c-4371-be93-ff13c9dbd1e9",{"pageContent":"import { MemoryVectorStore } from \"langchain/vectorstores/memory\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nconst vectorStore = await MemoryVectorStore.fromTexts(\n  [\"Hello world\", \"Bye bye\", \"hello nice world\"],\n  [{ id: 2 }, { id: 1 }, { id: 3 }],\n  new OpenAIEmbeddings()\n);\n\nconst resultOne = await vectorStore.similaritySearch(\"hello world\", 1);\nconsole.log(resultOne);\n\n/*\n  [\n    Document {\n      pageContent: \"Hello world\",\n      metadata: { id: 2 }\n    }\n  ]\n*/","metadata":{"source":"examples/src/indexes/vector_stores/memory.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":20}}}}],["d31a054a-9e16-401b-9fc7-284a03fefc66",{"pageContent":"import { MemoryVectorStore } from \"langchain/vectorstores/memory\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { similarity } from \"ml-distance\";\n\nconst vectorStore = await MemoryVectorStore.fromTexts(\n  [\"Hello world\", \"Bye bye\", \"hello nice world\"],\n  [{ id: 2 }, { id: 1 }, { id: 3 }],\n  new OpenAIEmbeddings(),\n  { similarity: similarity.pearson }\n);\n\nconst resultOne = await vectorStore.similaritySearch(\"hello world\", 1);\nconsole.log(resultOne);","metadata":{"source":"examples/src/indexes/vector_stores/memory_custom_similarity.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":13}}}}],["5e091968-acaa-4059-9d0c-e7b0a8e9981c",{"pageContent":"import { MemoryVectorStore } from \"langchain/vectorstores/memory\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\n\n// Create docs with a loader\nconst loader = new TextLoader(\"src/document_loaders/example_data/example.txt\");\nconst docs = await loader.load();\n\n// Load the docs into the vector store\nconst vectorStore = await MemoryVectorStore.fromDocuments(\n  docs,\n  new OpenAIEmbeddings()\n);\n\n// Search for the most similar document\nconst resultOne = await vectorStore.similaritySearch(\"hello world\", 1);\n\nconsole.log(resultOne);\n\n/*\n  [\n    Document {\n      pageContent: \"Hello world\",\n      metadata: { id: 2 }\n    }\n  ]\n*/","metadata":{"source":"examples/src/indexes/vector_stores/memory_fromdocs.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":27}}}}],["2edf5c6a-66f5-4e5d-bb2a-e0c6c695ad2b",{"pageContent":"import { Milvus } from \"langchain/vectorstores/milvus\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nexport const run = async () => {\n  const vectorStore = await Milvus.fromTexts(\n    [\"Hello world\", \"Bye bye\", \"hello nice world\"],\n    [{ id: 2 }, { id: 1 }, { id: 3 }],\n    new OpenAIEmbeddings()\n  );\n\n  const resultOne = await vectorStore.similaritySearch(\"hello world\", 1);\n  console.log(resultOne);\n};","metadata":{"source":"examples/src/indexes/vector_stores/milvus.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":13}}}}],["0bab9f00-b71e-418c-8025-4e2e1ffe2b79",{"pageContent":"import { MongoDBAtlasVectorSearch } from \"langchain/vectorstores/mongodb_atlas\";\nimport { CohereEmbeddings } from \"langchain/embeddings/cohere\";\nimport { MongoClient } from \"mongodb\";\n\nconst client = new MongoClient(process.env.MONGODB_ATLAS_URI || \"\");\nconst namespace = \"langchain.test\";\nconst [dbName, collectionName] = namespace.split(\".\");\nconst collection = client.db(dbName).collection(collectionName);\n\nawait MongoDBAtlasVectorSearch.fromTexts(\n  [\"Hello world\", \"Bye bye\", \"What's this?\"],\n  [{ id: 2 }, { id: 1 }, { id: 3 }],\n  new CohereEmbeddings(),\n  {\n    collection,\n    indexName: \"default\", // The name of the Atlas search index. Defaults to \"default\"\n    textKey: \"text\", // The name of the collection field containing the raw content. Defaults to \"text\"\n    embeddingKey: \"embedding\", // The name of the collection field containing the embedded text. Defaults to \"embedding\"\n  }\n);\n\nawait client.close();","metadata":{"source":"examples/src/indexes/vector_stores/mongodb_atlas_fromTexts.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":22}}}}],["286202aa-54c9-44c9-b14a-f5a11e933046",{"pageContent":"import { MongoDBAtlasVectorSearch } from \"langchain/vectorstores/mongodb_atlas\";\nimport { CohereEmbeddings } from \"langchain/embeddings/cohere\";\nimport { MongoClient } from \"mongodb\";\n\nconst client = new MongoClient(process.env.MONGODB_ATLAS_URI || \"\");\nconst namespace = \"langchain.test\";\nconst [dbName, collectionName] = namespace.split(\".\");\nconst collection = client.db(dbName).collection(collectionName);\n\nconst vectorStore = new MongoDBAtlasVectorSearch(new CohereEmbeddings(), {\n  collection,\n  indexName: \"default\", // The name of the Atlas search index. Defaults to \"default\"\n  textKey: \"text\", // The name of the collection field containing the raw content. Defaults to \"text\"\n  embeddingKey: \"embedding\", // The name of the collection field containing the embedded text. Defaults to \"embedding\"\n});\n\nconst resultOne = await vectorStore.similaritySearch(\"Hello world\", 1);\nconsole.log(resultOne);\n\nawait client.close();","metadata":{"source":"examples/src/indexes/vector_stores/mongodb_atlas_search.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":20}}}}],["e2e0c063-9c00-4483-aeec-4816f81532f0",{"pageContent":"import { MongoDBAtlasVectorSearch } from \"langchain/vectorstores/mongodb_atlas\";\nimport { CohereEmbeddings } from \"langchain/embeddings/cohere\";\nimport { MongoClient } from \"mongodb\";\n\nconst client = new MongoClient(process.env.MONGODB_ATLAS_URI || \"\");\nconst namespace = \"langchain.test\";\nconst [dbName, collectionName] = namespace.split(\".\");\nconst collection = client.db(dbName).collection(collectionName);\n\nconst vectorStore = new MongoDBAtlasVectorSearch(new CohereEmbeddings(), {\n  collection,\n  indexName: \"default\", // The name of the Atlas search index. Defaults to \"default\"\n  textKey: \"text\", // The name of the collection field containing the raw content. Defaults to \"text\"\n  embeddingKey: \"embedding\", // The name of the collection field containing the embedded text. Defaults to \"embedding\"\n});\n\nconst resultOne = await vectorStore.maxMarginalRelevanceSearch(\"Hello world\", {\n  k: 4,\n  fetchK: 20, // The number of documents to return on initial fetch\n});\nconsole.log(resultOne);\n\n// Using MMR in a vector store retriever\n\nconst retriever = await vectorStore.asRetriever({\n  searchType: \"mmr\",\n  searchKwargs: {\n    fetchK: 20,\n    lambda: 0.1,\n  },\n});\n\nconst retrieverOutput = await retriever.getRelevantDocuments(\"Hello world\");\n\nconsole.log(retrieverOutput);\n\nawait client.close();","metadata":{"source":"examples/src/indexes/vector_stores/mongodb_mmr.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":37}}}}],["f4324b37-ad81-42bc-af4a-f9194fd61ebe",{"pageContent":"import { MyScaleStore } from \"langchain/vectorstores/myscale\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nconst vectorStore = await MyScaleStore.fromTexts(\n  [\"Hello world\", \"Bye bye\", \"hello nice world\"],\n  [\n    { id: 2, name: \"2\" },\n    { id: 1, name: \"1\" },\n    { id: 3, name: \"3\" },\n  ],\n  new OpenAIEmbeddings(),\n  {\n    host: process.env.MYSCALE_HOST || \"localhost\",\n    port: process.env.MYSCALE_PORT || \"8443\",\n    username: process.env.MYSCALE_USERNAME || \"username\",\n    password: process.env.MYSCALE_PASSWORD || \"password\",\n    database: \"default\", // defaults to \"default\"\n    table: \"your_table\", // defaults to \"vector_table\"\n  }\n);\n\nconst results = await vectorStore.similaritySearch(\"hello world\", 1);\nconsole.log(results);\n\nconst filteredResults = await vectorStore.similaritySearch(\"hello world\", 1, {\n  whereStr: \"metadata.name = '1'\",\n});\nconsole.log(filteredResults);","metadata":{"source":"examples/src/indexes/vector_stores/myscale_fromTexts.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":28}}}}],["a17328a8-96a8-488f-9fe3-5a30a422c624",{"pageContent":"import { MyScaleStore } from \"langchain/vectorstores/myscale\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nconst vectorStore = await MyScaleStore.fromExistingIndex(\n  new OpenAIEmbeddings(),\n  {\n    host: process.env.MYSCALE_HOST || \"localhost\",\n    port: process.env.MYSCALE_PORT || \"8443\",\n    username: process.env.MYSCALE_USERNAME || \"username\",\n    password: process.env.MYSCALE_PASSWORD || \"password\",\n    database: \"default\", // defaults to \"default\"\n    table: \"your_table\", // defaults to \"vector_table\"\n  }\n);\n\nconst results = await vectorStore.similaritySearch(\"hello world\", 1);\nconsole.log(results);\n\nconst filteredResults = await vectorStore.similaritySearch(\"hello world\", 1, {\n  whereStr: \"metadata.name = '1'\",\n});\nconsole.log(filteredResults);","metadata":{"source":"examples/src/indexes/vector_stores/myscale_search.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":22}}}}],["ed81f4bf-c361-4605-b87b-71ad88cc9a82",{"pageContent":"import { Pinecone } from \"@pinecone-database/pinecone\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { PineconeStore } from \"langchain/vectorstores/pinecone\";\n\n// To run this example, first [create a Pinecone index](https://app.pinecone.io/organizations)\n// It must have 1536 dimensions, to match the OpenAI embedding size.\n// It should use the metric \"cosine\" to get the results below.\n// Point to this index from your .env.\n\nexport const run = async () => {\n  if (\n    !process.env.PINECONE_API_KEY ||\n    !process.env.PINECONE_ENVIRONMENT ||\n    !process.env.PINECONE_INDEX\n  ) {\n    throw new Error(\n      \"PINECONE_ENVIRONMENT and PINECONE_API_KEY and PINECONE_INDEX must be set\"\n    );\n  }\n\n  const pinecone = new Pinecone();\n  const index = pinecone.Index(process.env.PINECONE_INDEX);\n\n  const vectorStore = await PineconeStore.fromTexts(\n    [\"Hello world\", \"Bye bye\", \"hello nice world\"],\n    [{ foo: \"bar\" }, { foo: \"baz\" }, { foo: \"qux\" }],\n    new OpenAIEmbeddings(),\n    { pineconeIndex: index }\n  );\n\n  /* Without metadata filtering */\n  let result = await vectorStore.similaritySearchWithScore(\"Hello world\", 3);\n  console.dir(result, { depth: null });\n  /*\n  [\n    [\n      Document { pageContent: 'Hello world', metadata: { foo: 'bar' } },\n      1\n    ],\n    [\n      Document {\n        pageContent: 'hello nice world',\n        metadata: { foo: 'qux' }\n      },\n      0.939860761\n    ],\n    [\n      Document { pageContent: 'Bye bye', metadata: { foo: 'baz' } },\n      0.827194452\n    ]\n  ]\n  */\n\n  /* With metadata filtering */\n  result = await vectorStore.similaritySearchWithScore(\"Hello world\", 3, {\n    foo: \"bar\",\n  });\n  console.dir(result, { depth: null });\n  /*\n  [\n    [\n      Document { pageContent: 'Hello world', metadata: { foo: 'bar' } },\n      0.999995887\n    ]\n  ]\n  */\n};","metadata":{"source":"examples/src/indexes/vector_stores/pinecone.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":67}}}}],["2f3af20f-e994-462d-bfe5-08a98ac51f86",{"pageContent":"import { SingleStoreVectorStore } from \"langchain/vectorstores/singlestore\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nexport const run = async () => {\n  const vectorStore = await SingleStoreVectorStore.fromTexts(\n    [\"Hello world\", \"Bye bye\", \"hello nice world\"],\n    [{ id: 2 }, { id: 1 }, { id: 3 }],\n    new OpenAIEmbeddings(),\n    {\n      connectionOptions: {\n        host: process.env.SINGLESTORE_HOST,\n        port: Number(process.env.SINGLESTORE_PORT),\n        user: process.env.SINGLESTORE_USERNAME,\n        password: process.env.SINGLESTORE_PASSWORD,\n        database: process.env.SINGLESTORE_DATABASE,\n      },\n    }\n  );\n\n  const resultOne = await vectorStore.similaritySearch(\"hello world\", 1);\n  console.log(resultOne);\n  await vectorStore.end();\n};","metadata":{"source":"examples/src/indexes/vector_stores/singlestore.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":23}}}}],["99c99cd0-70b8-4ddc-857a-219aed8765a1",{"pageContent":"import { SingleStoreVectorStore } from \"langchain/vectorstores/singlestore\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nexport const run = async () => {\n  const vectorStore = await SingleStoreVectorStore.fromTexts(\n    [\"Good afternoon\", \"Bye bye\", \"Boa tarde!\", \"Até logo!\"],\n    [\n      { id: 1, language: \"English\" },\n      { id: 2, language: \"English\" },\n      { id: 3, language: \"Portugese\" },\n      { id: 4, language: \"Portugese\" },\n    ],\n    new OpenAIEmbeddings(),\n    {\n      connectionOptions: {\n        host: process.env.SINGLESTORE_HOST,\n        port: Number(process.env.SINGLESTORE_PORT),\n        user: process.env.SINGLESTORE_USERNAME,\n        password: process.env.SINGLESTORE_PASSWORD,\n        database: process.env.SINGLESTORE_DATABASE,\n      },\n      distanceMetric: \"EUCLIDEAN_DISTANCE\",\n    }\n  );\n\n  const resultOne = await vectorStore.similaritySearch(\"greetings\", 1, {\n    language: \"Portugese\",\n  });\n  console.log(resultOne);\n  await vectorStore.end();\n};","metadata":{"source":"examples/src/indexes/vector_stores/singlestore_with_metadata_filter.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":31}}}}],["8e7b7a83-9db9-4367-8142-38b235088dab",{"pageContent":"import { SupabaseVectorStore } from \"langchain/vectorstores/supabase\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { createClient } from \"@supabase/supabase-js\";\n\n// First, follow set-up instructions at\n// https://js.langchain.com/docs/modules/indexes/vector_stores/integrations/supabase\n\nconst privateKey = process.env.SUPABASE_PRIVATE_KEY;\nif (!privateKey) throw new Error(`Expected env var SUPABASE_PRIVATE_KEY`);\n\nconst url = process.env.SUPABASE_URL;\nif (!url) throw new Error(`Expected env var SUPABASE_URL`);\n\nexport const run = async () => {\n  const client = createClient(url, privateKey);\n\n  const vectorStore = await SupabaseVectorStore.fromTexts(\n    [\"Hello world\", \"Bye bye\", \"What's this?\"],\n    [{ id: 2 }, { id: 1 }, { id: 3 }],\n    new OpenAIEmbeddings(),\n    {\n      client,\n      tableName: \"documents\",\n      queryName: \"match_documents\",\n    }\n  );\n\n  const resultOne = await vectorStore.similaritySearch(\"Hello world\", 1);\n\n  console.log(resultOne);\n};","metadata":{"source":"examples/src/indexes/vector_stores/supabase.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":31}}}}],["afede682-2d2b-4f8e-b72e-6b59d3e7397c",{"pageContent":"import { SupabaseVectorStore } from \"langchain/vectorstores/supabase\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { createClient } from \"@supabase/supabase-js\";\n\n// First, follow set-up instructions at\n// https://js.langchain.com/docs/modules/indexes/vector_stores/integrations/supabase\n\nconst privateKey = process.env.SUPABASE_PRIVATE_KEY;\nif (!privateKey) throw new Error(`Expected env var SUPABASE_PRIVATE_KEY`);\n\nconst url = process.env.SUPABASE_URL;\nif (!url) throw new Error(`Expected env var SUPABASE_URL`);\n\nexport const run = async () => {\n  const client = createClient(url, privateKey);\n\n  const embeddings = new OpenAIEmbeddings();\n\n  const store = new SupabaseVectorStore(embeddings, {\n    client,\n    tableName: \"documents\",\n  });\n\n  const docs = [\n    { pageContent: \"hello\", metadata: { b: 1, c: 9, stuff: \"right\" } },\n    { pageContent: \"hello\", metadata: { b: 1, c: 9, stuff: \"wrong\" } },\n  ];\n\n  // Also takes an additional {ids: []} parameter for upsertion\n  const ids = await store.addDocuments(docs);\n\n  const resultA = await store.similaritySearch(\"hello\", 2);\n  console.log(resultA);\n\n  /*\n    [\n      Document { pageContent: \"hello\", metadata: { b: 1, c: 9, stuff: \"right\" } },\n      Document { pageContent: \"hello\", metadata: { b: 1, c: 9, stuff: \"wrong\" } },\n    ]\n  */\n\n  await store.delete({ ids });\n\n  const resultB = await store.similaritySearch(\"hello\", 2);\n  console.log(resultB);\n\n  /*\n    []\n  */\n};","metadata":{"source":"examples/src/indexes/vector_stores/supabase_deletion.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":50}}}}],["3da30069-d4eb-4c3b-af98-47c99122c6d2",{"pageContent":"import { SupabaseVectorStore } from \"langchain/vectorstores/supabase\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { createClient } from \"@supabase/supabase-js\";\n\n// First, follow set-up instructions at\n// https://js.langchain.com/docs/modules/indexes/vector_stores/integrations/supabase\n\nconst privateKey = process.env.SUPABASE_PRIVATE_KEY;\nif (!privateKey) throw new Error(`Expected env var SUPABASE_PRIVATE_KEY`);\n\nconst url = process.env.SUPABASE_URL;\nif (!url) throw new Error(`Expected env var SUPABASE_URL`);\n\nexport const run = async () => {\n  const client = createClient(url, privateKey);\n\n  const vectorStore = await SupabaseVectorStore.fromTexts(\n    [\"Hello world\", \"Bye bye\", \"What's this?\"],\n    [{ id: 2 }, { id: 1 }, { id: 3 }],\n    new OpenAIEmbeddings(),\n    {\n      client,\n      tableName: \"documents\",\n      queryName: \"match_documents\",\n    }\n  );\n\n  const resultOne = await vectorStore.maxMarginalRelevanceSearch(\n    \"Hello world\",\n    { k: 1 }\n  );\n\n  console.log(resultOne);\n};","metadata":{"source":"examples/src/indexes/vector_stores/supabase_with_maximum_marginal_relevance.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":34}}}}],["c4edb35d-9416-47be-915a-3a2eb3da45b3",{"pageContent":"import { SupabaseVectorStore } from \"langchain/vectorstores/supabase\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { createClient } from \"@supabase/supabase-js\";\n\n// First, follow set-up instructions at\n// https://js.langchain.com/docs/modules/indexes/vector_stores/integrations/supabase\n\nconst privateKey = process.env.SUPABASE_PRIVATE_KEY;\nif (!privateKey) throw new Error(`Expected env var SUPABASE_PRIVATE_KEY`);\n\nconst url = process.env.SUPABASE_URL;\nif (!url) throw new Error(`Expected env var SUPABASE_URL`);\n\nexport const run = async () => {\n  const client = createClient(url, privateKey);\n\n  const vectorStore = await SupabaseVectorStore.fromTexts(\n    [\"Hello world\", \"Hello world\", \"Hello world\"],\n    [{ user_id: 2 }, { user_id: 1 }, { user_id: 3 }],\n    new OpenAIEmbeddings(),\n    {\n      client,\n      tableName: \"documents\",\n      queryName: \"match_documents\",\n    }\n  );\n\n  const result = await vectorStore.similaritySearch(\"Hello world\", 1, {\n    user_id: 3,\n  });\n\n  console.log(result);\n};","metadata":{"source":"examples/src/indexes/vector_stores/supabase_with_metadata_filter.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":33}}}}],["3087025b-aa62-4178-af6c-30aec1beb34d",{"pageContent":"import {\n  SupabaseFilterRPCCall,\n  SupabaseVectorStore,\n} from \"langchain/vectorstores/supabase\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { createClient } from \"@supabase/supabase-js\";\n\n// First, follow set-up instructions at\n// https://js.langchain.com/docs/modules/indexes/vector_stores/integrations/supabase\n\nconst privateKey = process.env.SUPABASE_PRIVATE_KEY;\nif (!privateKey) throw new Error(`Expected env var SUPABASE_PRIVATE_KEY`);\n\nconst url = process.env.SUPABASE_URL;\nif (!url) throw new Error(`Expected env var SUPABASE_URL`);\n\nexport const run = async () => {\n  const client = createClient(url, privateKey);\n\n  const embeddings = new OpenAIEmbeddings();\n\n  const store = new SupabaseVectorStore(embeddings, {\n    client,\n    tableName: \"documents\",\n  });\n\n  const docs = [\n    {\n      pageContent:\n        \"This is a long text, but it actually means something because vector database does not understand Lorem Ipsum. So I would need to expand upon the notion of quantum fluff, a theorectical concept where subatomic particles coalesce to form transient multidimensional spaces. Yet, this abstraction holds no real-world application or comprehensible meaning, reflecting a cosmic puzzle.\",\n      metadata: { b: 1, c: 10, stuff: \"right\" },\n    },\n    {\n      pageContent:\n        \"This is a long text, but it actually means something because vector database does not understand Lorem Ipsum. So I would need to proceed by discussing the echo of virtual tweets in the binary corridors of the digital universe. Each tweet, like a pixelated canary, hums in an unseen frequency, a fascinatingly perplexing phenomenon that, while conjuring vivid imagery, lacks any concrete implication or real-world relevance, portraying a paradox of multidimensional spaces in the age of cyber folklore.\",\n      metadata: { b: 2, c: 9, stuff: \"right\" },\n    },\n    { pageContent: \"hello\", metadata: { b: 1, c: 9, stuff: \"right\" } },\n    { pageContent: \"hello\", metadata: { b: 1, c: 9, stuff: \"wrong\" } },\n    { pageContent: \"hi\", metadata: { b: 2, c: 8, stuff: \"right\" } },\n    { pageContent: \"bye\", metadata: { b: 3, c: 7, stuff: \"right\" } },\n    { pageContent: \"what's this\", metadata: { b: 4, c: 6, stuff: \"right\" } },\n  ];\n\n  // Also supports an additional {ids: []} parameter for upsertion\n  await store.addDocuments(docs);\n\n  const funcFilterA: SupabaseFilterRPCCall = (rpc) =>\n    rpc\n      .filter(\"metadata->b::int\", \"lt\", 3)\n      .filter(\"metadata->c::int\", \"gt\", 7)\n      .textSearch(\"content\", `'multidimensional' & 'spaces'`, {\n        config: \"english\",\n      });\n\n  const resultA = await store.similaritySearch(\"quantum\", 4, funcFilterA);\n\n  const funcFilterB: SupabaseFilterRPCCall = (rpc) =>\n    rpc\n      .filter(\"metadata->b::int\", \"lt\", 3)\n      .filter(\"metadata->c::int\", \"gt\", 7)\n      .filter(\"metadata->>stuff\", \"eq\", \"right\");\n\n  const resultB = await store.similaritySearch(\"hello\", 2, funcFilterB);\n\n  console.log(resultA, resultB);\n};","metadata":{"source":"examples/src/indexes/vector_stores/supabase_with_query_builder_metadata_filter.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":67}}}}],["f4e49b16-f894-4325-89c6-b5cda4e09f8c",{"pageContent":"import { Typesense, TypesenseConfig } from \"langchain/vectorstores/typesense\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { Client } from \"typesense\";\nimport { Document } from \"langchain/document\";\n\nconst vectorTypesenseClient = new Client({\n  nodes: [\n    {\n      // Ideally should come from your .env file\n      host: \"...\",\n      port: 123,\n      protocol: \"https\",\n    },\n  ],\n  // Ideally should come from your .env file\n  apiKey: \"...\",\n  numRetries: 3,\n  connectionTimeoutSeconds: 60,\n});\n\nconst typesenseVectorStoreConfig = {\n  // Typesense client\n  typesenseClient: vectorTypesenseClient,\n  // Name of the collection to store the vectors in\n  schemaName: \"your_schema_name\",\n  // Optional column names to be used in Typesense\n  columnNames: {\n    // \"vec\" is the default name for the vector column in Typesense but you can change it to whatever you want\n    vector: \"vec\",\n    // \"text\" is the default name for the text column in Typesense but you can change it to whatever you want\n    pageContent: \"text\",\n    // Names of the columns that you will save in your typesense schema and need to be retrieved as metadata when searching\n    metadataColumnNames: [\"foo\", \"bar\", \"baz\"],\n  },\n  // Optional search parameters to be passed to Typesense when searching\n  searchParams: {\n    q: \"*\",\n    filter_by: \"foo:[fooo]\",\n    query_by: \"\",\n  },\n  // You can override the default Typesense import function if you want to do something more complex\n  // Default import function:\n  // async importToTypesense<\n  //   T extends Record<string, unknown> = Record<string, unknown>\n  // >(data: T[], collectionName: string) {\n  //   const chunkSize = 2000;\n  //   for (let i = 0; i < data.length; i += chunkSize) {\n  //     const chunk = data.slice(i, i + chunkSize);\n\n  //     await this.caller.call(async () => {\n  //       await this.client\n  //         .collections<T>(collectionName)\n  //         .documents()\n  //         .import(chunk, { action: \"emplace\", dirty_values: \"drop\" });\n  //     });\n  //   }\n  // }\n  import: async (data, collectionName) => {\n    await vectorTypesenseClient\n      .collections(collectionName)\n      .documents()\n      .import(data, { action: \"emplace\", dirty_values: \"drop\" });\n  },\n} satisfies TypesenseConfig;\n\n/**\n * Creates a Typesense vector store from a list of documents.\n * Will update documents if there is a document with the same id, at least with the default import function.\n * @param documents list of documents to create the vector store from\n * @returns Typesense vector store\n */\nconst createVectorStoreWithTypesense = async (documents: Document[] = []) =>\n  Typesense.fromDocuments(\n    documents,\n    new OpenAIEmbeddings(),\n    typesenseVectorStoreConfig\n  );\n\n/**\n * Returns a Typesense vector store from an existing index.\n * @returns Typesense vector store\n */\nconst getVectorStoreWithTypesense = async () =>\n  new Typesense(new OpenAIEmbeddings(), typesenseVectorStoreConfig);\n\n// Do a similarity search\nconst vectorStore = await getVectorStoreWithTypesense();\nconst documents = await vectorStore.similaritySearch(\"hello world\");\n\n// Add filters based on metadata with the search parameters of Typesense\n// will exclude documents with author:JK Rowling, so if Joe Rowling & JK Rowling exists, only Joe Rowling will be returned\nvectorStore.similaritySearch(\"Rowling\", undefined, {\n  filter_by: \"author:!=JK Rowling\",\n});\n\n// Delete a document\nvectorStore.deleteDocuments([\"document_id_1\", \"document_id_2\"]);","metadata":{"source":"examples/src/indexes/vector_stores/typesense.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":97}}}}],["cb55b227-1473-42fa-a1f9-148797d40b20",{"pageContent":"import { USearch } from \"langchain/vectorstores/usearch\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nconst vectorStore = await USearch.fromTexts(\n  [\"Hello world\", \"Bye bye\", \"hello nice world\"],\n  [{ id: 2 }, { id: 1 }, { id: 3 }],\n  new OpenAIEmbeddings()\n);\n\nconst resultOne = await vectorStore.similaritySearch(\"hello world\", 1);\nconsole.log(resultOne);","metadata":{"source":"examples/src/indexes/vector_stores/usearch.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":11}}}}],["e7278112-8007-4124-875c-79903d32ac11",{"pageContent":"import { USearch } from \"langchain/vectorstores/usearch\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\n\n// Create docs with a loader\nconst loader = new TextLoader(\"src/document_loaders/example_data/example.txt\");\nconst docs = await loader.load();\n\n// Load the docs into the vector store\nconst vectorStore = await USearch.fromDocuments(docs, new OpenAIEmbeddings());\n\n// Search for the most similar document\nconst resultOne = await vectorStore.similaritySearch(\"hello world\", 1);\nconsole.log(resultOne);","metadata":{"source":"examples/src/indexes/vector_stores/usearch_fromdocs.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":14}}}}],["6e4d3bc1-5407-4e56-abd9-cd9fb6fbe555",{"pageContent":"import { VectaraStore } from \"langchain/vectorstores/vectara\";\nimport { Document } from \"langchain/document\";\n\n// Create the Vectara store.\nconst store = new VectaraStore({\n  customerId: Number(process.env.VECTARA_CUSTOMER_ID),\n  corpusId: Number(process.env.VECTARA_CORPUS_ID),\n  apiKey: String(process.env.VECTARA_API_KEY),\n  verbose: true,\n});\n\n// Add two documents with some metadata.\nawait store.addDocuments([\n  new Document({\n    pageContent: \"Do I dare to eat a peach?\",\n    metadata: {\n      foo: \"baz\",\n    },\n  }),\n  new Document({\n    pageContent: \"In the room the women come and go talking of Michelangelo\",\n    metadata: {\n      foo: \"bar\",\n    },\n  }),\n]);\n\n// Perform a similarity search.\nconst resultsWithScore = await store.similaritySearchWithScore(\n  \"What were the women talking about?\",\n  1,\n  {\n    lambda: 0.025,\n  }\n);\n\n// Print the results.\nconsole.log(JSON.stringify(resultsWithScore, null, 2));\n// [\n//   [\n//     {\n//       \"pageContent\": \"In the room the women come and go talking of Michelangelo\",\n//       \"metadata\": [\n//         {\n//           \"name\": \"lang\",\n//           \"value\": \"eng\"\n//         },\n//         {\n//           \"name\": \"offset\",\n//           \"value\": \"0\"\n//         },\n//         {\n//           \"name\": \"len\",\n//           \"value\": \"57\"\n//         }\n//       ]\n//     },\n//     0.38169062\n//   ]\n// ]","metadata":{"source":"examples/src/indexes/vector_stores/vectara.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":60}}}}],["8f3934d2-009e-478b-a793-e6d254df2b9a",{"pageContent":"import { VoyVectorStore } from \"langchain/vectorstores/voy\";\nimport { Voy as VoyClient } from \"voy-search\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { Document } from \"langchain/document\";\n\n// Create Voy client using the library.\nconst voyClient = new VoyClient();\n// Create embeddings\nconst embeddings = new OpenAIEmbeddings();\n// Create the Voy store.\nconst store = new VoyVectorStore(voyClient, embeddings);\n\n// Add two documents with some metadata.\nawait store.addDocuments([\n  new Document({\n    pageContent: \"How has life been treating you?\",\n    metadata: {\n      foo: \"Mike\",\n    },\n  }),\n  new Document({\n    pageContent: \"And I took it personally...\",\n    metadata: {\n      foo: \"Testing\",\n    },\n  }),\n]);\n\nconst model = new OpenAIEmbeddings();\nconst query = await model.embedQuery(\"And I took it personally\");\n\n// Perform a similarity search.\nconst resultsWithScore = await store.similaritySearchVectorWithScore(query, 1);\n\n// Print the results.\nconsole.log(JSON.stringify(resultsWithScore, null, 2));\n/*\n  [\n    [\n      {\n        \"pageContent\": \"And I took it personally...\",\n        \"metadata\": {\n          \"foo\": \"Testing\"\n        }\n      },\n      0\n    ]\n  ]\n*/","metadata":{"source":"examples/src/indexes/vector_stores/voy.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":49}}}}],["1641ccec-6aec-48b3-b29a-2395ad820617",{"pageContent":"/* eslint-disable @typescript-eslint/no-explicit-any */\nimport weaviate from \"weaviate-ts-client\";\nimport { WeaviateStore } from \"langchain/vectorstores/weaviate\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nexport async function run() {\n  // Something wrong with the weaviate-ts-client types, so we need to disable\n  const client = (weaviate as any).client({\n    scheme: process.env.WEAVIATE_SCHEME || \"https\",\n    host: process.env.WEAVIATE_HOST || \"localhost\",\n    apiKey: new (weaviate as any).ApiKey(\n      process.env.WEAVIATE_API_KEY || \"default\"\n    ),\n  });\n\n  // Create a store for an existing index\n  const store = await WeaviateStore.fromExistingIndex(new OpenAIEmbeddings(), {\n    client,\n    indexName: \"Test\",\n    metadataKeys: [\"foo\"],\n  });\n\n  const docs = [{ pageContent: \"see ya!\", metadata: { foo: \"bar\" } }];\n\n  // Also supports an additional {ids: []} parameter for upsertion\n  const ids = await store.addDocuments(docs);\n\n  // Search the index without any filters\n  const results = await store.similaritySearch(\"see ya!\", 1);\n  console.log(results);\n  /*\n  [ Document { pageContent: 'see ya!', metadata: { foo: 'bar' } } ]\n  */\n\n  // Delete documents with ids\n  await store.delete({ ids });\n\n  const results2 = await store.similaritySearch(\"see ya!\", 1);\n  console.log(results2);\n  /*\n  []\n  */\n\n  const docs2 = [\n    { pageContent: \"hello world\", metadata: { foo: \"bar\" } },\n    { pageContent: \"hi there\", metadata: { foo: \"baz\" } },\n    { pageContent: \"how are you\", metadata: { foo: \"qux\" } },\n    { pageContent: \"hello world\", metadata: { foo: \"bar\" } },\n    { pageContent: \"bye now\", metadata: { foo: \"bar\" } },\n  ];\n\n  await store.addDocuments(docs2);\n\n  const results3 = await store.similaritySearch(\"hello world\", 1);\n  console.log(results3);\n  /*\n  [ Document { pageContent: 'hello world', metadata: { foo: 'bar' } } ]\n  */\n\n  // delete documents with filter\n  await store.delete({\n    filter: {\n      where: {\n        operator: \"Equal\",\n        path: [\"foo\"],\n        valueText: \"bar\",\n      },\n    },\n  });\n\n  const results4 = await store.similaritySearch(\"hello world\", 1, {\n    where: {\n      operator: \"Equal\",\n      path: [\"foo\"],\n      valueText: \"bar\",\n    },\n  });\n  console.log(results4);\n  /*\n  []\n  */\n}","metadata":{"source":"examples/src/indexes/vector_stores/weaviate_delete.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":82}}}}],["2badf650-ad14-40fa-82d5-a817041bf994",{"pageContent":"/* eslint-disable @typescript-eslint/no-explicit-any */\nimport weaviate from \"weaviate-ts-client\";\nimport { WeaviateStore } from \"langchain/vectorstores/weaviate\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nexport async function run() {\n  // Something wrong with the weaviate-ts-client types, so we need to disable\n  const client = (weaviate as any).client({\n    scheme: process.env.WEAVIATE_SCHEME || \"https\",\n    host: process.env.WEAVIATE_HOST || \"localhost\",\n    apiKey: new (weaviate as any).ApiKey(\n      process.env.WEAVIATE_API_KEY || \"default\"\n    ),\n  });\n\n  // Create a store and fill it with some texts + metadata\n  await WeaviateStore.fromTexts(\n    [\"hello world\", \"hi there\", \"how are you\", \"bye now\"],\n    [{ foo: \"bar\" }, { foo: \"baz\" }, { foo: \"qux\" }, { foo: \"bar\" }],\n    new OpenAIEmbeddings(),\n    {\n      client,\n      indexName: \"Test\",\n      textKey: \"text\",\n      metadataKeys: [\"foo\"],\n    }\n  );\n}","metadata":{"source":"examples/src/indexes/vector_stores/weaviate_fromTexts.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":28}}}}],["d2bf5f0f-29e8-450e-a3d5-957ef77fa854",{"pageContent":"/* eslint-disable @typescript-eslint/no-explicit-any */\nimport weaviate from \"weaviate-ts-client\";\nimport { WeaviateStore } from \"langchain/vectorstores/weaviate\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nexport async function run() {\n  // Something wrong with the weaviate-ts-client types, so we need to disable\n  const client = (weaviate as any).client({\n    scheme: process.env.WEAVIATE_SCHEME || \"https\",\n    host: process.env.WEAVIATE_HOST || \"localhost\",\n    apiKey: new (weaviate as any).ApiKey(\n      process.env.WEAVIATE_API_KEY || \"default\"\n    ),\n  });\n\n  // Create a store for an existing index\n  const store = await WeaviateStore.fromExistingIndex(new OpenAIEmbeddings(), {\n    client,\n    indexName: \"Test\",\n    metadataKeys: [\"foo\"],\n  });\n\n  // Search the index without any filters\n  const results = await store.similaritySearch(\"hello world\", 1);\n  console.log(results);\n  /*\n  [ Document { pageContent: 'hello world', metadata: { foo: 'bar' } } ]\n  */\n\n  // Search the index with a filter, in this case, only return results where\n  // the \"foo\" metadata key is equal to \"baz\", see the Weaviate docs for more\n  // https://weaviate.io/developers/weaviate/api/graphql/filters\n  const results2 = await store.similaritySearch(\"hello world\", 1, {\n    where: {\n      operator: \"Equal\",\n      path: [\"foo\"],\n      valueText: \"baz\",\n    },\n  });\n  console.log(results2);\n  /*\n  [ Document { pageContent: 'hi there', metadata: { foo: 'baz' } } ]\n  */\n}","metadata":{"source":"examples/src/indexes/vector_stores/weaviate_search.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":44}}}}],["3c428fa1-b620-40c2-bb3f-cdb31fcc6c86",{"pageContent":"import { XataVectorSearch } from \"langchain/vectorstores/xata\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { BaseClient } from \"@xata.io/client\";\nimport { Document } from \"langchain/document\";\nimport { VectorDBQAChain } from \"langchain/chains\";\nimport { OpenAI } from \"langchain/llms/openai\";\n\n// First, follow set-up instructions at\n// https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/xata\n\n// if you use the generated client, you don't need this function.\n// Just import getXataClient from the generated xata.ts instead.\nconst getXataClient = () => {\n  if (!process.env.XATA_API_KEY) {\n    throw new Error(\"XATA_API_KEY not set\");\n  }\n\n  if (!process.env.XATA_DB_URL) {\n    throw new Error(\"XATA_DB_URL not set\");\n  }\n  const xata = new BaseClient({\n    databaseURL: process.env.XATA_DB_URL,\n    apiKey: process.env.XATA_API_KEY,\n    branch: process.env.XATA_BRANCH || \"main\",\n  });\n  return xata;\n};\n\nexport async function run() {\n  const client = getXataClient();\n\n  const table = \"vectors\";\n  const embeddings = new OpenAIEmbeddings();\n  const store = new XataVectorSearch(embeddings, { client, table });\n\n  // Add documents\n  const docs = [\n    new Document({\n      pageContent: \"Xata is a Serverless Data platform based on PostgreSQL\",\n    }),\n    new Document({\n      pageContent:\n        \"Xata offers a built-in vector type that can be used to store and query vectors\",\n    }),\n    new Document({\n      pageContent: \"Xata includes similarity search\",\n    }),\n  ];\n\n  const ids = await store.addDocuments(docs);\n\n  // eslint-disable-next-line no-promise-executor-return\n  await new Promise((r) => setTimeout(r, 2000));\n\n  const model = new OpenAI();\n  const chain = VectorDBQAChain.fromLLM(model, store, {\n    k: 1,\n    returnSourceDocuments: true,\n  });\n  const response = await chain.call({ query: \"What is Xata?\" });\n\n  console.log(JSON.stringify(response, null, 2));\n\n  await store.delete({ ids });\n}","metadata":{"source":"examples/src/indexes/vector_stores/xata.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":65}}}}],["48bcbad5-cb53-44b8-9fa7-4e15ba6faf5d",{"pageContent":"import { XataVectorSearch } from \"langchain/vectorstores/xata\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { BaseClient } from \"@xata.io/client\";\nimport { Document } from \"langchain/document\";\n\n// First, follow set-up instructions at\n// https://js.langchain.com/docs/modules/data_connection/vectorstores/integrations/xata\n// Also, add a column named \"author\" to the \"vectors\" table.\n\n// if you use the generated client, you don't need this function.\n// Just import getXataClient from the generated xata.ts instead.\nconst getXataClient = () => {\n  if (!process.env.XATA_API_KEY) {\n    throw new Error(\"XATA_API_KEY not set\");\n  }\n\n  if (!process.env.XATA_DB_URL) {\n    throw new Error(\"XATA_DB_URL not set\");\n  }\n  const xata = new BaseClient({\n    databaseURL: process.env.XATA_DB_URL,\n    apiKey: process.env.XATA_API_KEY,\n    branch: process.env.XATA_BRANCH || \"main\",\n  });\n  return xata;\n};\n\nexport async function run() {\n  const client = getXataClient();\n  const table = \"vectors\";\n  const embeddings = new OpenAIEmbeddings();\n  const store = new XataVectorSearch(embeddings, { client, table });\n  // Add documents\n  const docs = [\n    new Document({\n      pageContent: \"Xata works great with Langchain.js\",\n      metadata: { author: \"Xata\" },\n    }),\n    new Document({\n      pageContent: \"Xata works great with Langchain\",\n      metadata: { author: \"Langchain\" },\n    }),\n    new Document({\n      pageContent: \"Xata includes similarity search\",\n      metadata: { author: \"Xata\" },\n    }),\n  ];\n  const ids = await store.addDocuments(docs);\n\n  // eslint-disable-next-line no-promise-executor-return\n  await new Promise((r) => setTimeout(r, 2000));\n\n  // author is applied as pre-filter to the similarity search\n  const results = await store.similaritySearchWithScore(\"xata works great\", 6, {\n    author: \"Langchain\",\n  });\n\n  console.log(JSON.stringify(results, null, 2));\n\n  await store.delete({ ids });\n}","metadata":{"source":"examples/src/indexes/vector_stores/xata_metadata.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":61}}}}],["14e2543e-f5c8-448d-bfa6-8e5c3c646406",{"pageContent":"import { Chroma } from \"langchain/vectorstores/chroma\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nconst embeddings = new OpenAIEmbeddings();\nconst vectorStore = new Chroma(embeddings, {\n  collectionName: \"test-deletion\",\n});\n\nconst documents = [\n  {\n    pageContent: `Tortoise: Labyrinth? Labyrinth? Could it Are we in the notorious Little\n    Harmonic Labyrinth of the dreaded Majotaur?`,\n    metadata: {\n      speaker: \"Tortoise\",\n    },\n  },\n  {\n    pageContent: \"Achilles: Yiikes! What is that?\",\n    metadata: {\n      speaker: \"Achilles\",\n    },\n  },\n  {\n    pageContent: `Tortoise: They say-although I person never believed it myself-that an I\n    Majotaur has created a tiny labyrinth sits in a pit in the middle of\n    it, waiting innocent victims to get lost in its fears complexity.\n    Then, when they wander and dazed into the center, he laughs and\n    laughs at them-so hard, that he laughs them to death!`,\n    metadata: {\n      speaker: \"Tortoise\",\n    },\n  },\n  {\n    pageContent: \"Achilles: Oh, no!\",\n    metadata: {\n      speaker: \"Achilles\",\n    },\n  },\n  {\n    pageContent: \"Tortoise: But it's only a myth. Courage, Achilles.\",\n    metadata: {\n      speaker: \"Tortoise\",\n    },\n  },\n];\n\n// Also supports an additional {ids: []} parameter for upsertion\nconst ids = await vectorStore.addDocuments(documents);\n\nconst response = await vectorStore.similaritySearch(\"scared\", 2);\nconsole.log(response);\n/*\n[\n  Document { pageContent: 'Achilles: Oh, no!', metadata: {} },\n  Document {\n    pageContent: 'Achilles: Yiikes! What is that?',\n    metadata: { id: 1 }\n  }\n]\n*/\n\n// You can also pass a \"filter\" parameter instead\nawait vectorStore.delete({ ids });\n\nconst response2 = await vectorStore.similaritySearch(\"scared\", 2);\nconsole.log(response2);\n\n/*\n  []\n*/","metadata":{"source":"examples/src/indexes/vector_stores/chroma/delete.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":70}}}}],["6017b316-3f9e-4445-9b55-b4acf12d6f39",{"pageContent":"import { Chroma } from \"langchain/vectorstores/chroma\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\n\n// Create docs with a loader\nconst loader = new TextLoader(\"src/document_loaders/example_data/example.txt\");\nconst docs = await loader.load();\n\n// Create vector store and index the docs\nconst vectorStore = await Chroma.fromDocuments(docs, new OpenAIEmbeddings(), {\n  collectionName: \"a-test-collection\",\n  url: \"http://localhost:8000\", // Optional, will default to this value\n  collectionMetadata: {\n    \"hnsw:space\": \"cosine\",\n  }, // Optional, can be used to specify the distance method of the embedding space https://docs.trychroma.com/usage-guide#changing-the-distance-function\n});\n\n// Search for the most similar document\nconst response = await vectorStore.similaritySearch(\"hello\", 1);\n\nconsole.log(response);\n/*\n[\n  Document {\n    pageContent: 'Foo\\nBar\\nBaz\\n\\n',\n    metadata: { source: 'src/document_loaders/example_data/example.txt' }\n  }\n]\n*/","metadata":{"source":"examples/src/indexes/vector_stores/chroma/fromDocs.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":29}}}}],["9345dc08-7e8c-4264-890b-183df07d52c0",{"pageContent":"import { Chroma } from \"langchain/vectorstores/chroma\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\n// text sample from Godel, Escher, Bach\nconst vectorStore = await Chroma.fromTexts(\n  [\n    `Tortoise: Labyrinth? Labyrinth? Could it Are we in the notorious Little\n        Harmonic Labyrinth of the dreaded Majotaur?`,\n    \"Achilles: Yiikes! What is that?\",\n    `Tortoise: They say-although I person never believed it myself-that an I\n        Majotaur has created a tiny labyrinth sits in a pit in the middle of\n        it, waiting innocent victims to get lost in its fears complexity.\n        Then, when they wander and dazed into the center, he laughs and\n        laughs at them-so hard, that he laughs them to death!`,\n    \"Achilles: Oh, no!\",\n    \"Tortoise: But it's only a myth. Courage, Achilles.\",\n  ],\n  [{ id: 2 }, { id: 1 }, { id: 3 }],\n  new OpenAIEmbeddings(),\n  {\n    collectionName: \"godel-escher-bach\",\n  }\n);\n\nconst response = await vectorStore.similaritySearch(\"scared\", 2);\n\nconsole.log(response);\n/*\n[\n  Document { pageContent: 'Achilles: Oh, no!', metadata: {} },\n  Document {\n    pageContent: 'Achilles: Yiikes! What is that?',\n    metadata: { id: 1 }\n  }\n]\n*/\n\n// You can also filter by metadata\nconst filteredResponse = await vectorStore.similaritySearch(\"scared\", 2, {\n  id: 1,\n});\n\nconsole.log(filteredResponse);\n/*\n[\n  Document {\n    pageContent: 'Achilles: Yiikes! What is that?',\n    metadata: { id: 1 }\n  }\n]\n*/","metadata":{"source":"examples/src/indexes/vector_stores/chroma/fromTexts.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":51}}}}],["f466bae7-23c6-4fd1-8691-38c7ebfa4f3b",{"pageContent":"import { Chroma } from \"langchain/vectorstores/chroma\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nconst vectorStore = await Chroma.fromExistingCollection(\n  new OpenAIEmbeddings(),\n  { collectionName: \"godel-escher-bach\" }\n);\n\nconst response = await vectorStore.similaritySearch(\"scared\", 2);\nconsole.log(response);\n/*\n[\n  Document { pageContent: 'Achilles: Oh, no!', metadata: {} },\n  Document {\n    pageContent: 'Achilles: Yiikes! What is that?',\n    metadata: { id: 1 }\n  }\n]\n*/","metadata":{"source":"examples/src/indexes/vector_stores/chroma/search.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":19}}}}],["c969a305-2876-4331-ad28-98a8713b2198",{"pageContent":"// @ts-nocheck\n\nimport type {\n  VectorizeIndex,\n  Fetcher,\n  Request,\n} from \"@cloudflare/workers-types\";\n\nimport { CloudflareVectorizeStore } from \"langchain/vectorstores/cloudflare_vectorize\";\nimport { CloudflareWorkersAIEmbeddings } from \"langchain/embeddings/cloudflare_workersai\";\n\nexport interface Env {\n  VECTORIZE_INDEX: VectorizeIndex;\n  AI: Fetcher;\n}\n\nexport default {\n  async fetch(request: Request, env: Env) {\n    const { pathname } = new URL(request.url);\n    const embeddings = new CloudflareWorkersAIEmbeddings({\n      binding: env.AI,\n      modelName: \"@cf/baai/bge-small-en-v1.5\",\n    });\n    const store = new CloudflareVectorizeStore(embeddings, {\n      index: env.VECTORIZE_INDEX,\n    });\n    if (pathname === \"/\") {\n      const results = await store.similaritySearch(\"hello\", 5);\n      return Response.json(results);\n    } else if (pathname === \"/load\") {\n      // Upsertion by id is supported\n      await store.addDocuments(\n        [\n          {\n            pageContent: \"hello\",\n            metadata: {},\n          },\n          {\n            pageContent: \"world\",\n            metadata: {},\n          },\n          {\n            pageContent: \"hi\",\n            metadata: {},\n          },\n        ],\n        { ids: [\"id1\", \"id2\", \"id3\"] }\n      );\n\n      return Response.json({ success: true });\n    } else if (pathname === \"/clear\") {\n      await store.delete({ ids: [\"id1\", \"id2\", \"id3\"] });\n      return Response.json({ success: true });\n    }\n\n    return Response.json({ error: \"Not Found\" }, { status: 404 });\n  },\n};","metadata":{"source":"examples/src/indexes/vector_stores/cloudflare_vectorize/example.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":58}}}}],["5f17ff55-1acd-41a4-922e-f8ec1728811a",{"pageContent":"\"use node\";\n\nimport { ConvexVectorStore } from \"langchain/vectorstores/convex\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { action } from \"./_generated/server.js\";\n\nexport const ingest = action({\n  args: {},\n  handler: async (ctx) => {\n    await ConvexVectorStore.fromTexts(\n      [\"Hello world\", \"Bye bye\", \"What's this?\"],\n      [{ prop: 2 }, { prop: 1 }, { prop: 3 }],\n      new OpenAIEmbeddings(),\n      { ctx }\n    );\n  },\n});","metadata":{"source":"examples/src/indexes/vector_stores/convex/fromTexts.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":17}}}}],["dc16c4c0-1ddf-4bd5-bbfa-ff463eefed5a",{"pageContent":"\"use node\";\n\nimport { ConvexVectorStore } from \"langchain/vectorstores/convex\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { v } from \"convex/values\";\nimport { action } from \"./_generated/server.js\";\n\nexport const search = action({\n  args: {\n    query: v.string(),\n  },\n  handler: async (ctx, args) => {\n    const vectorStore = new ConvexVectorStore(new OpenAIEmbeddings(), { ctx });\n\n    const resultOne = await vectorStore.similaritySearch(args.query, 1);\n    console.log(resultOne);\n  },\n});","metadata":{"source":"examples/src/indexes/vector_stores/convex/search.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":18}}}}],["5e48ba07-7f16-4cb9-b9a1-aeea67c662bf",{"pageContent":"/* eslint-disable */\n/**\n * Generated `api` utility.\n *\n * THIS CODE IS AUTOMATICALLY GENERATED.\n *\n * Generated by convex@1.3.1.\n * To regenerate, run `npx convex dev`.\n * @module\n */\n\nimport type {\n  ApiFromModules,\n  FilterApi,\n  FunctionReference,\n} from \"convex/server\";\n\n/**\n * A utility for referencing Convex functions in your app's API.\n *\n * Usage:\n * ```js\n * const myFunctionReference = api.myModule.myFunction;\n * ```\n */\ndeclare const fullApi: ApiFromModules<{}>;\nexport declare const api: FilterApi<\n  typeof fullApi,\n  FunctionReference<any, \"public\">\n>;\nexport declare const internal: FilterApi<\n  typeof fullApi,\n  FunctionReference<any, \"internal\">\n>;","metadata":{"source":"examples/src/indexes/vector_stores/convex/_generated/api.d.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":34}}}}],["03c0349e-73c3-4ad4-b38a-9e9fe307e222",{"pageContent":"/* eslint-disable */\n/**\n * Generated `api` utility.\n *\n * THIS CODE IS AUTOMATICALLY GENERATED.\n *\n * Generated by convex@1.3.1.\n * To regenerate, run `npx convex dev`.\n * @module\n */\n\nimport { anyApi } from \"convex/server\";\n\n/**\n * A utility for referencing Convex functions in your app's API.\n *\n * Usage:\n * ```js\n * const myFunctionReference = api.myModule.myFunction;\n * ```\n */\nexport const api = anyApi;\nexport const internal = anyApi;","metadata":{"source":"examples/src/indexes/vector_stores/convex/_generated/api.js","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":23}}}}],["5a93e026-63ac-4db2-b293-2123f030aefd",{"pageContent":"/* eslint-disable */\n/**\n * Generated data model types.\n *\n * THIS CODE IS AUTOMATICALLY GENERATED.\n *\n * Generated by convex@1.3.1.\n * To regenerate, run `npx convex dev`.\n * @module\n */\n\nimport { AnyDataModel } from \"convex/server\";\nimport type { GenericId } from \"convex/values\";\n\n/**\n * No `schema.ts` file found!\n *\n * This generated code has permissive types like `Doc = any` because\n * Convex doesn't know your schema. If you'd like more type safety, see\n * https://docs.convex.dev/using/schemas for instructions on how to add a\n * schema file.\n *\n * After you change a schema, rerun codegen with `npx convex dev`.\n */\n\n/**\n * The names of all of your Convex tables.\n */\nexport type TableNames = string;\n\n/**\n * The type of a document stored in Convex.\n */\nexport type Doc = any;\n\n/**\n * An identifier for a document in Convex.\n *\n * Convex documents are uniquely identified by their `Id`, which is accessible\n * on the `_id` field. To learn more, see [Document IDs](https://docs.convex.dev/using/document-ids).\n *\n * Documents can be loaded using `db.get(id)` in query and mutation functions.\n *\n * IDs are just strings at runtime, but this type can be used to distinguish them from other\n * strings when type checking.\n */\nexport type Id<TableName extends TableNames = TableNames> =\n  GenericId<TableName>;\n\n/**\n * A type describing your Convex data model.\n *\n * This type includes information about what tables you have, the type of\n * documents stored in those tables, and the indexes defined on them.\n *\n * This type is used to parameterize methods like `queryGeneric` and\n * `mutationGeneric` to make them type-safe.\n */\nexport type DataModel = AnyDataModel;","metadata":{"source":"examples/src/indexes/vector_stores/convex/_generated/dataModel.d.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":59}}}}],["3602a8d3-978e-4e42-ada7-ebd00c6bb873",{"pageContent":"/* eslint-disable */\n/**\n * Generated utilities for implementing server-side Convex query and mutation functions.\n *\n * THIS CODE IS AUTOMATICALLY GENERATED.\n *\n * Generated by convex@1.3.1.\n * To regenerate, run `npx convex dev`.\n * @module\n */\n\nimport {\n  ActionBuilder,\n  HttpActionBuilder,\n  MutationBuilder,\n  QueryBuilder,\n  GenericActionCtx,\n  GenericMutationCtx,\n  GenericQueryCtx,\n  GenericDatabaseReader,\n  GenericDatabaseWriter,\n} from \"convex/server\";\nimport type { DataModel } from \"./dataModel.js\";\n\n/**\n * Define a query in this Convex app's public API.\n *\n * This function will be allowed to read your Convex database and will be accessible from the client.\n *\n * @param func - The query function. It receives a {@link QueryCtx} as its first argument.\n * @returns The wrapped query. Include this as an `export` to name it and make it accessible.\n */\nexport declare const query: QueryBuilder<DataModel, \"public\">;\n\n/**\n * Define a query that is only accessible from other Convex functions (but not from the client).\n *\n * This function will be allowed to read from your Convex database. It will not be accessible from the client.\n *\n * @param func - The query function. It receives a {@link QueryCtx} as its first argument.\n * @returns The wrapped query. Include this as an `export` to name it and make it accessible.\n */\nexport declare const internalQuery: QueryBuilder<DataModel, \"internal\">;\n\n/**\n * Define a mutation in this Convex app's public API.\n *\n * This function will be allowed to modify your Convex database and will be accessible from the client.\n *\n * @param func - The mutation function. It receives a {@link MutationCtx} as its first argument.\n * @returns The wrapped mutation. Include this as an `export` to name it and make it accessible.\n */\nexport declare const mutation: MutationBuilder<DataModel, \"public\">;\n\n/**\n * Define a mutation that is only accessible from other Convex functions (but not from the client).\n *\n * This function will be allowed to modify your Convex database. It will not be accessible from the client.\n *\n * @param func - The mutation function. It receives a {@link MutationCtx} as its first argument.\n * @returns The wrapped mutation. Include this as an `export` to name it and make it accessible.\n */\nexport declare const internalMutation: MutationBuilder<DataModel, \"internal\">;\n\n/**\n * Define an action in this Convex app's public API.\n *\n * An action is a function which can execute any JavaScript code, including non-deterministic\n * code and code with side-effects, like calling third-party services.\n * They can be run in Convex's JavaScript environment or in Node.js using the \"use node\" directive.\n * They can interact with the database indirectly by calling queries and mutations using the {@link ActionCtx}.\n *\n * @param func - The action. It receives an {@link ActionCtx} as its first argument.\n * @returns The wrapped action. Include this as an `export` to name it and make it accessible.\n */\nexport declare const action: ActionBuilder<DataModel, \"public\">;\n\n/**\n * Define an action that is only accessible from other Convex functions (but not from the client).\n *\n * @param func - The function. It receives an {@link ActionCtx} as its first argument.\n * @returns The wrapped function. Include this as an `export` to name it and make it accessible.\n */\nexport declare const internalAction: ActionBuilder<DataModel, \"internal\">;\n\n/**\n * Define an HTTP action.\n *\n * This function will be used to respond to HTTP requests received by a Convex\n * deployment if the requests matches the path and method where this action\n * is routed. Be sure to route your action in `convex/http.js`.\n *\n * @param func - The function. It receives an {@link ActionCtx} as its first argument.\n * @returns The wrapped function. Import this function from `convex/http.js` and route it to hook it up.\n */\nexport declare const httpAction: HttpActionBuilder;","metadata":{"source":"examples/src/indexes/vector_stores/convex/_generated/server.d.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":96}}}}],["926f278e-9847-4121-9424-3ecc55899f59",{"pageContent":"/**\n * A set of services for use within Convex query functions.\n *\n * The query context is passed as the first argument to any Convex query\n * function run on the server.\n *\n * This differs from the {@link MutationCtx} because all of the services are\n * read-only.\n */\nexport type QueryCtx = GenericQueryCtx<DataModel>;\n\n/**\n * A set of services for use within Convex mutation functions.\n *\n * The mutation context is passed as the first argument to any Convex mutation\n * function run on the server.\n */\nexport type MutationCtx = GenericMutationCtx<DataModel>;\n\n/**\n * A set of services for use within Convex action functions.\n *\n * The action context is passed as the first argument to any Convex action\n * function run on the server.\n */\nexport type ActionCtx = GenericActionCtx<DataModel>;\n\n/**\n * An interface to read from the database within Convex query functions.\n *\n * The two entry points are {@link DatabaseReader.get}, which fetches a single\n * document by its {@link Id}, or {@link DatabaseReader.query}, which starts\n * building a query.\n */\nexport type DatabaseReader = GenericDatabaseReader<DataModel>;\n\n/**\n * An interface to read from and write to the database within Convex mutation\n * functions.\n *\n * Convex guarantees that all writes within a single mutation are\n * executed atomically, so you never have to worry about partial writes leaving\n * your data in an inconsistent state. See [the Convex Guide](https://docs.convex.dev/understanding/convex-fundamentals/functions#atomicity-and-optimistic-concurrency-control)\n * for the guarantees Convex provides your functions.\n */\nexport type DatabaseWriter = GenericDatabaseWriter<DataModel>;","metadata":{"source":"examples/src/indexes/vector_stores/convex/_generated/server.d.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":98,"to":143}}}}],["c129c5e6-959b-4027-a165-4c71c681a09f",{"pageContent":"/* eslint-disable */\n/**\n * Generated utilities for implementing server-side Convex query and mutation functions.\n *\n * THIS CODE IS AUTOMATICALLY GENERATED.\n *\n * Generated by convex@1.3.1.\n * To regenerate, run `npx convex dev`.\n * @module\n */\n\nimport {\n  actionGeneric,\n  httpActionGeneric,\n  queryGeneric,\n  mutationGeneric,\n  internalActionGeneric,\n  internalMutationGeneric,\n  internalQueryGeneric,\n} from \"convex/server\";\n\n/**\n * Define a query in this Convex app's public API.\n *\n * This function will be allowed to read your Convex database and will be accessible from the client.\n *\n * @param func - The query function. It receives a {@link QueryCtx} as its first argument.\n * @returns The wrapped query. Include this as an `export` to name it and make it accessible.\n */\nexport const query = queryGeneric;\n\n/**\n * Define a query that is only accessible from other Convex functions (but not from the client).\n *\n * This function will be allowed to read from your Convex database. It will not be accessible from the client.\n *\n * @param func - The query function. It receives a {@link QueryCtx} as its first argument.\n * @returns The wrapped query. Include this as an `export` to name it and make it accessible.\n */\nexport const internalQuery = internalQueryGeneric;\n\n/**\n * Define a mutation in this Convex app's public API.\n *\n * This function will be allowed to modify your Convex database and will be accessible from the client.\n *\n * @param func - The mutation function. It receives a {@link MutationCtx} as its first argument.\n * @returns The wrapped mutation. Include this as an `export` to name it and make it accessible.\n */\nexport const mutation = mutationGeneric;\n\n/**\n * Define a mutation that is only accessible from other Convex functions (but not from the client).\n *\n * This function will be allowed to modify your Convex database. It will not be accessible from the client.\n *\n * @param func - The mutation function. It receives a {@link MutationCtx} as its first argument.\n * @returns The wrapped mutation. Include this as an `export` to name it and make it accessible.\n */\nexport const internalMutation = internalMutationGeneric;\n\n/**\n * Define an action in this Convex app's public API.\n *\n * An action is a function which can execute any JavaScript code, including non-deterministic\n * code and code with side-effects, like calling third-party services.\n * They can be run in Convex's JavaScript environment or in Node.js using the \"use node\" directive.\n * They can interact with the database indirectly by calling queries and mutations using the {@link ActionCtx}.\n *\n * @param func - The action. It receives an {@link ActionCtx} as its first argument.\n * @returns The wrapped action. Include this as an `export` to name it and make it accessible.\n */\nexport const action = actionGeneric;\n\n/**\n * Define an action that is only accessible from other Convex functions (but not from the client).\n *\n * @param func - The function. It receives an {@link ActionCtx} as its first argument.\n * @returns The wrapped function. Include this as an `export` to name it and make it accessible.\n */\nexport const internalAction = internalActionGeneric;\n\n/**\n * Define a Convex HTTP action.\n *\n * @param func - The function. It receives an {@link ActionCtx} as its first argument, and a `Request` object\n * as its second.\n * @returns The wrapped endpoint function. Route a URL path to this function in `convex/http.js`.\n */\nexport const httpAction = httpActionGeneric;","metadata":{"source":"examples/src/indexes/vector_stores/convex/_generated/server.js","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":90}}}}],["9778d4c4-a046-4b7b-830a-04ad8d045895",{"pageContent":"import { Client, ClientOptions } from \"@elastic/elasticsearch\";\nimport { Document } from \"langchain/document\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport {\n  ElasticClientArgs,\n  ElasticVectorSearch,\n} from \"langchain/vectorstores/elasticsearch\";\nimport { VectorDBQAChain } from \"langchain/chains\";\n\n// to run this first run Elastic's docker-container with `docker-compose up -d --build`\nexport async function run() {\n  const config: ClientOptions = {\n    node: process.env.ELASTIC_URL ?? \"http://127.0.0.1:9200\",\n  };\n  if (process.env.ELASTIC_API_KEY) {\n    config.auth = {\n      apiKey: process.env.ELASTIC_API_KEY,\n    };\n  } else if (process.env.ELASTIC_USERNAME && process.env.ELASTIC_PASSWORD) {\n    config.auth = {\n      username: process.env.ELASTIC_USERNAME,\n      password: process.env.ELASTIC_PASSWORD,\n    };\n  }\n  const clientArgs: ElasticClientArgs = {\n    client: new Client(config),\n    indexName: process.env.ELASTIC_INDEX ?? \"test_vectorstore\",\n  };\n\n  // Index documents\n\n  const docs = [\n    new Document({\n      metadata: { foo: \"bar\" },\n      pageContent: \"Elasticsearch is a powerful vector db\",\n    }),\n    new Document({\n      metadata: { foo: \"bar\" },\n      pageContent: \"the quick brown fox jumped over the lazy dog\",\n    }),\n    new Document({\n      metadata: { baz: \"qux\" },\n      pageContent: \"lorem ipsum dolor sit amet\",\n    }),\n    new Document({\n      metadata: { baz: \"qux\" },\n      pageContent:\n        \"Elasticsearch a distributed, RESTful search engine optimized for speed and relevance on production-scale workloads.\",\n    }),\n  ];\n\n  const embeddings = new OpenAIEmbeddings();\n\n  // await ElasticVectorSearch.fromDocuments(docs, embeddings, clientArgs);\n  const vectorStore = new ElasticVectorSearch(embeddings, clientArgs);\n\n  // Also supports an additional {ids: []} parameter for upsertion\n  const ids = await vectorStore.addDocuments(docs);\n\n  /* Search the vector DB independently with meta filters */\n  const results = await vectorStore.similaritySearch(\"fox jump\", 1);\n  console.log(JSON.stringify(results, null, 2));\n  /* [\n        {\n          \"pageContent\": \"the quick brown fox jumped over the lazy dog\",\n          \"metadata\": {\n            \"foo\": \"bar\"\n          }\n        }\n    ]\n  */\n\n  /* Use as part of a chain (currently no metadata filters) for LLM query */\n  const model = new OpenAI();\n  const chain = VectorDBQAChain.fromLLM(model, vectorStore, {\n    k: 1,\n    returnSourceDocuments: true,\n  });\n  const response = await chain.call({ query: \"What is Elasticsearch?\" });\n\n  console.log(JSON.stringify(response, null, 2));\n  /*\n    {\n      \"text\": \" Elasticsearch is a distributed, RESTful search engine optimized for speed and relevance on production-scale workloads.\",\n      \"sourceDocuments\": [\n        {\n          \"pageContent\": \"Elasticsearch a distributed, RESTful search engine optimized for speed and relevance on production-scale workloads.\",\n          \"metadata\": {\n            \"baz\": \"qux\"\n          }\n        }\n      ]\n    }\n    */\n\n  await vectorStore.delete({ ids });\n\n  const response2 = await chain.call({ query: \"What is Elasticsearch?\" });\n\n  console.log(JSON.stringify(response2, null, 2));\n\n  /*\n    []\n  */\n}","metadata":{"source":"examples/src/indexes/vector_stores/elasticsearch/elasticsearch.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":106}}}}],["7d9f599c-32a3-429f-806d-ef1eed0ac7ca",{"pageContent":"import { LanceDB } from \"langchain/vectorstores/lancedb\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\nimport fs from \"node:fs/promises\";\nimport path from \"node:path\";\nimport os from \"node:os\";\nimport { connect } from \"vectordb\";\n\n// Create docs with a loader\nconst loader = new TextLoader(\"src/document_loaders/example_data/example.txt\");\nconst docs = await loader.load();\n\nexport const run = async () => {\n  const dir = await fs.mkdtemp(path.join(os.tmpdir(), \"lancedb-\"));\n  const db = await connect(dir);\n  const table = await db.createTable(\"vectors\", [\n    { vector: Array(1536), text: \"sample\", source: \"a\" },\n  ]);\n\n  const vectorStore = await LanceDB.fromDocuments(\n    docs,\n    new OpenAIEmbeddings(),\n    { table }\n  );\n\n  const resultOne = await vectorStore.similaritySearch(\"hello world\", 1);\n  console.log(resultOne);\n\n  // [\n  //   Document {\n  //     pageContent: 'Foo\\nBar\\nBaz\\n\\n',\n  //     metadata: { source: 'src/document_loaders/example_data/example.txt' }\n  //   }\n  // ]\n};","metadata":{"source":"examples/src/indexes/vector_stores/lancedb/fromDocs.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":35}}}}],["54fdbb82-2811-4a32-ac87-4ac0d833c3bf",{"pageContent":"import { LanceDB } from \"langchain/vectorstores/lancedb\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { connect } from \"vectordb\";\nimport * as fs from \"node:fs/promises\";\nimport * as path from \"node:path\";\nimport os from \"node:os\";\n\nexport const run = async () => {\n  const dir = await fs.mkdtemp(path.join(os.tmpdir(), \"lancedb-\"));\n  const db = await connect(dir);\n  const table = await db.createTable(\"vectors\", [\n    { vector: Array(1536), text: \"sample\", id: 1 },\n  ]);\n\n  const vectorStore = await LanceDB.fromTexts(\n    [\"Hello world\", \"Bye bye\", \"hello nice world\"],\n    [{ id: 2 }, { id: 1 }, { id: 3 }],\n    new OpenAIEmbeddings(),\n    { table }\n  );\n\n  const resultOne = await vectorStore.similaritySearch(\"hello world\", 1);\n  console.log(resultOne);\n  // [ Document { pageContent: 'hello nice world', metadata: { id: 3 } } ]\n};","metadata":{"source":"examples/src/indexes/vector_stores/lancedb/fromTexts.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":25}}}}],["d18538d5-aaec-4515-8f70-0b6146747b12",{"pageContent":"import { LanceDB } from \"langchain/vectorstores/lancedb\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { connect } from \"vectordb\";\nimport * as fs from \"node:fs/promises\";\nimport * as path from \"node:path\";\nimport os from \"node:os\";\n\n//\n//  You can open a LanceDB dataset created elsewhere, such as LangChain Python, by opening\n//     an existing table\n//\nexport const run = async () => {\n  const uri = await createdTestDb();\n  const db = await connect(uri);\n  const table = await db.openTable(\"vectors\");\n\n  const vectorStore = new LanceDB(new OpenAIEmbeddings(), { table });\n\n  const resultOne = await vectorStore.similaritySearch(\"hello world\", 1);\n  console.log(resultOne);\n  // [ Document { pageContent: 'Hello world', metadata: { id: 1 } } ]\n};\n\nasync function createdTestDb(): Promise<string> {\n  const dir = await fs.mkdtemp(path.join(os.tmpdir(), \"lancedb-\"));\n  const db = await connect(dir);\n  await db.createTable(\"vectors\", [\n    { vector: Array(1536), text: \"Hello world\", id: 1 },\n    { vector: Array(1536), text: \"Bye bye\", id: 2 },\n    { vector: Array(1536), text: \"hello nice world\", id: 3 },\n  ]);\n  return dir;\n}","metadata":{"source":"examples/src/indexes/vector_stores/lancedb/load.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":33}}}}],["d13f36d8-26d2-4f5f-8836-f54ee920940d",{"pageContent":"import { MomentoVectorIndex } from \"langchain/vectorstores/momento_vector_index\";\n// For browser/edge, adjust this to import from \"@gomomento/sdk-web\";\nimport {\n  PreviewVectorIndexClient,\n  VectorIndexConfigurations,\n  CredentialProvider,\n} from \"@gomomento/sdk\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\nimport { sleep } from \"langchain/util/time\";\n\n// Create docs with a loader\nconst loader = new TextLoader(\"src/document_loaders/example_data/example.txt\");\nconst docs = await loader.load();\n\nconst vectorStore = await MomentoVectorIndex.fromDocuments(\n  docs,\n  new OpenAIEmbeddings(),\n  {\n    client: new PreviewVectorIndexClient({\n      configuration: VectorIndexConfigurations.Laptop.latest(),\n      credentialProvider: CredentialProvider.fromEnvironmentVariable({\n        environmentVariableName: \"MOMENTO_API_KEY\",\n      }),\n    }),\n    indexName: \"langchain-example-index\",\n  }\n);\n\n// because indexing is async, wait for it to finish to search directly after\nawait sleep();\n\n// Search for the most similar document\nconst response = await vectorStore.similaritySearch(\"hello\", 1);\n\nconsole.log(response);\n/*\n[\n  Document {\n    pageContent: 'Foo\\nBar\\nBaz\\n\\n',\n    metadata: { source: 'src/document_loaders/example_data/example.txt' }\n  }\n]\n*/","metadata":{"source":"examples/src/indexes/vector_stores/momento_vector_index/fromDocs.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":44}}}}],["3e9afeca-da81-49fd-8008-2f46bcb6a5cf",{"pageContent":"import { MomentoVectorIndex } from \"langchain/vectorstores/momento_vector_index\";\n// For browser/edge, adjust this to import from \"@gomomento/sdk-web\";\nimport {\n  PreviewVectorIndexClient,\n  VectorIndexConfigurations,\n  CredentialProvider,\n} from \"@gomomento/sdk\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nconst vectorStore = new MomentoVectorIndex(new OpenAIEmbeddings(), {\n  client: new PreviewVectorIndexClient({\n    configuration: VectorIndexConfigurations.Laptop.latest(),\n    credentialProvider: CredentialProvider.fromEnvironmentVariable({\n      environmentVariableName: \"MOMENTO_API_KEY\",\n    }),\n  }),\n  indexName: \"langchain-example-index\",\n});\n\nconst response = await vectorStore.similaritySearch(\"hello\", 1);\n\nconsole.log(response);\n/*\n[\n  Document {\n    pageContent: 'Foo\\nBar\\nBaz\\n\\n',\n    metadata: { source: 'src/document_loaders/example_data/example.txt' }\n  }\n]\n*/","metadata":{"source":"examples/src/indexes/vector_stores/momento_vector_index/fromExisting.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":30}}}}],["d11b8577-df08-4c4f-bfc9-e598e5f14d68",{"pageContent":"import { MomentoVectorIndex } from \"langchain/vectorstores/momento_vector_index\";\n// For browser/edge, adjust this to import from \"@gomomento/sdk-web\";\nimport {\n  PreviewVectorIndexClient,\n  VectorIndexConfigurations,\n  CredentialProvider,\n} from \"@gomomento/sdk\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { sleep } from \"langchain/util/time\";\n\nconst vectorStore = await MomentoVectorIndex.fromTexts(\n  [\"hello world\", \"goodbye world\", \"salutations world\", \"farewell world\"],\n  {},\n  new OpenAIEmbeddings(),\n  {\n    client: new PreviewVectorIndexClient({\n      configuration: VectorIndexConfigurations.Laptop.latest(),\n      credentialProvider: CredentialProvider.fromEnvironmentVariable({\n        environmentVariableName: \"MOMENTO_API_KEY\",\n      }),\n    }),\n    indexName: \"langchain-example-index\",\n  },\n  { ids: [\"1\", \"2\", \"3\", \"4\"] }\n);\n\n// because indexing is async, wait for it to finish to search directly after\nawait sleep();\n\nconst response = await vectorStore.similaritySearch(\"hello\", 2);\n\nconsole.log(response);\n\n/*\n[\n  Document { pageContent: 'hello world', metadata: {} },\n  Document { pageContent: 'salutations world', metadata: {} }\n]\n*/","metadata":{"source":"examples/src/indexes/vector_stores/momento_vector_index/fromTexts.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":39}}}}],["a7ba6d0c-9ee5-458d-9ed1-0f246f8330bd",{"pageContent":"services:\n  database:\n    image: neo4j\n    ports:\n      - 7687:7687\n      - 7474:7474\n    environment:\n      - NEO4J_AUTH=neo4j/pleaseletmein","metadata":{"source":"examples/src/indexes/vector_stores/neo4j_vector/docker-compose.example.yml","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":8}}}}],["a604907b-9476-4f94-845f-2ff18e7a8018",{"pageContent":"import { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { Neo4jVectorStore } from \"langchain/vectorstores/neo4j_vector\";\n\n// Configuration object for Neo4j connection and other related settings\nconst config = {\n  url: \"bolt://localhost:7687\", // URL for the Neo4j instance\n  username: \"neo4j\", // Username for Neo4j authentication\n  password: \"pleaseletmein\", // Password for Neo4j authentication\n  indexName: \"vector\", // Name of the vector index\n  keywordIndexName: \"keyword\", // Name of the keyword index if using hybrid search\n  searchType: \"vector\" as const, // Type of search (e.g., vector, hybrid)\n  nodeLabel: \"Chunk\", // Label for the nodes in the graph\n  textNodeProperty: \"text\", // Property of the node containing text\n  embeddingNodeProperty: \"embedding\", // Property of the node containing embedding\n};\n\nconst documents = [\n  { pageContent: \"what's this\", metadata: { a: 2 } },\n  { pageContent: \"Cat drinks milk\", metadata: { a: 1 } },\n];\n\nconst neo4jVectorIndex = await Neo4jVectorStore.fromDocuments(\n  documents,\n  new OpenAIEmbeddings(),\n  config\n);\n\nconst results = await neo4jVectorIndex.similaritySearch(\"water\", 1);\n\nconsole.log(results);\n\n/*\n  [ Document { pageContent: 'Cat drinks milk', metadata: { a: 1 } } ]\n*/\n\nawait neo4jVectorIndex.close();","metadata":{"source":"examples/src/indexes/vector_stores/neo4j_vector/neo4j_vector.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":36}}}}],["d54296c0-7c76-4eee-82d2-49b2fd101e86",{"pageContent":"import { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { Neo4jVectorStore } from \"langchain/vectorstores/neo4j_vector\";\n\n/**\n * `fromExistingGraph` Method:\n *\n * Description:\n * This method initializes a `Neo4jVectorStore` instance using an existing graph in the Neo4j database.\n * It's designed to work with nodes that already have textual properties but might not have embeddings.\n * The method will compute and store embeddings for nodes that lack them.\n *\n * Note:\n * This method is particularly useful when you have a pre-existing graph with textual data and you want\n * to enhance it with vector embeddings for similarity searches without altering the original data structure.\n */\n\n// Configuration object for Neo4j connection and other related settings\nconst config = {\n  url: \"bolt://localhost:7687\", // URL for the Neo4j instance\n  username: \"neo4j\", // Username for Neo4j authentication\n  password: \"pleaseletmein\", // Password for Neo4j authentication\n  indexName: \"wikipedia\",\n  nodeLabel: \"Wikipedia\",\n  textNodeProperties: [\"title\", \"description\"],\n  embeddingNodeProperty: \"embedding\",\n  searchType: \"hybrid\" as const,\n};\n\n// You should have a populated Neo4j database to use this method\nconst neo4jVectorIndex = await Neo4jVectorStore.fromExistingGraph(\n  new OpenAIEmbeddings(),\n  config\n);\n\nawait neo4jVectorIndex.close();","metadata":{"source":"examples/src/indexes/vector_stores/neo4j_vector/neo4j_vector_existinggraph.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":35}}}}],["f4545a99-20a8-450d-8623-bcdafbc7c1a7",{"pageContent":"import { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { Neo4jVectorStore } from \"langchain/vectorstores/neo4j_vector\";\n\n/*\n * The retrievalQuery is a customizable Cypher query fragment used in the Neo4jVectorStore class to define how\n * search results should be retrieved and presented from the Neo4j database. It allows developers to specify\n * the format and structure of the data returned after a similarity search.\n * Mandatory columns for `retrievalQuery`:\n *\n * 1. text:\n *    - Description: Represents the textual content of the node.\n *    - Type: String\n *\n * 2. score:\n *    - Description: Represents the similarity score of the node in relation to the search query. A\n *      higher score indicates a closer match.\n *    - Type: Float (ranging between 0 and 1, where 1 is a perfect match)\n *\n * 3. metadata:\n *    - Description: Contains additional properties and information about the node. This can include\n *      any other attributes of the node that might be relevant to the application.\n *    - Type: Object (key-value pairs)\n *    - Example: { \"id\": \"12345\", \"category\": \"Books\", \"author\": \"John Doe\" }\n *\n * Note: While you can customize the `retrievalQuery` to fetch additional columns or perform\n * transformations, never omit the mandatory columns. The names of these columns (`text`, `score`,\n * and `metadata`) should remain consistent. Renaming them might lead to errors or unexpected behavior.\n */\n\n// Configuration object for Neo4j connection and other related settings\nconst config = {\n  url: \"bolt://localhost:7687\", // URL for the Neo4j instance\n  username: \"neo4j\", // Username for Neo4j authentication\n  password: \"pleaseletmein\", // Password for Neo4j authentication\n  retrievalQuery: `\n    RETURN node.text AS text, score, {a: node.a * 2} AS metadata\n  `,\n};\n\nconst documents = [\n  { pageContent: \"what's this\", metadata: { a: 2 } },\n  { pageContent: \"Cat drinks milk\", metadata: { a: 1 } },\n];\n\nconst neo4jVectorIndex = await Neo4jVectorStore.fromDocuments(\n  documents,\n  new OpenAIEmbeddings(),\n  config\n);\n\nconst results = await neo4jVectorIndex.similaritySearch(\"water\", 1);\n\nconsole.log(results);\n\n/*\n  [ Document { pageContent: 'Cat drinks milk', metadata: { a: 2 } } ]\n*/\n\nawait neo4jVectorIndex.close();","metadata":{"source":"examples/src/indexes/vector_stores/neo4j_vector/neo4j_vector_retrieval.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":59}}}}],["d543a7b5-87cb-47c4-81c6-154e1350d314",{"pageContent":"# Reference:\n#   https://opensearch.org/docs/latest/install-and-configure/install-opensearch/docker/#sample-docker-composeyml\nversion: '3'\nservices:\n  opensearch:\n    image: opensearchproject/opensearch:2.6.0\n    container_name: opensearch\n    environment:\n      - cluster.name=opensearch\n      - node.name=opensearch\n      - discovery.type=single-node\n      - bootstrap.memory_lock=true\n      - \"OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m\"\n      - \"DISABLE_INSTALL_DEMO_CONFIG=true\"\n      - \"DISABLE_SECURITY_PLUGIN=true\"\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n    volumes:\n      - opensearch_data:/usr/share/opensearch/data\n    ports:\n      - 9200:9200\n      - 9600:9600\n    networks:\n      - opensearch\n  opensearch-dashboards:\n    image: opensearchproject/opensearch-dashboards:latest # Make sure the version of opensearch-dashboards matches the version of opensearch installed on other nodes\n    container_name: opensearch-dashboards\n    ports:\n      - 5601:5601 # Map host port 5601 to container port 5601\n    expose:\n      - \"5601\" # Expose port 5601 for web access to OpenSearch Dashboards\n    environment:\n      OPENSEARCH_HOSTS: '[\"http://opensearch:9200\"]' # Define the OpenSearch nodes that OpenSearch Dashboards will query\n      DISABLE_SECURITY_DASHBOARDS_PLUGIN: \"true\" # disables security dashboards plugin in OpenSearch Dashboards\n    networks:\n      - opensearch\nnetworks:\n  opensearch:\nvolumes:\n  opensearch_data:","metadata":{"source":"examples/src/indexes/vector_stores/opensearch/docker-compose.yml","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":42}}}}],["b9cdf408-41f9-4ab1-bada-a9ed32d38cc1",{"pageContent":"import { Client } from \"@opensearch-project/opensearch\";\nimport { Document } from \"langchain/document\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { OpenSearchVectorStore } from \"langchain/vectorstores/opensearch\";\nimport * as uuid from \"uuid\";\n\nexport async function run() {\n  const client = new Client({\n    nodes: [process.env.OPENSEARCH_URL ?? \"http://127.0.0.1:9200\"],\n  });\n\n  const embeddings = new OpenAIEmbeddings();\n\n  const vectorStore = await OpenSearchVectorStore.fromTexts(\n    [\"Hello world\", \"Bye bye\", \"What's this?\"],\n    [{ id: 2 }, { id: 1 }, { id: 3 }],\n    embeddings,\n    {\n      client,\n      indexName: \"documents\",\n    }\n  );\n\n  const resultOne = await vectorStore.similaritySearch(\"Hello world\", 1);\n  console.log(resultOne);\n\n  const vectorStore2 = new OpenSearchVectorStore(embeddings, {\n    client,\n    indexName: \"custom\",\n  });\n\n  const documents = [\n    new Document({\n      pageContent: \"Do I dare to eat an apple?\",\n      metadata: {\n        foo: \"baz\",\n      },\n    }),\n    new Document({\n      pageContent: \"There is no better place than the hotel lobby\",\n      metadata: {\n        foo: \"bar\",\n      },\n    }),\n    new Document({\n      pageContent: \"OpenSearch is a powerful vector db\",\n      metadata: {\n        foo: \"bat\",\n      },\n    }),\n  ];\n  const vectors = Array.from({ length: documents.length }, (_, i) => [\n    i,\n    i + 1,\n    i + 2,\n  ]);\n  const ids = Array.from({ length: documents.length }, () => uuid.v4());\n  await vectorStore2.addVectors(vectors, documents, { ids });\n\n  const resultTwo = await vectorStore2.similaritySearchVectorWithScore(\n    vectors[0],\n    3\n  );\n  console.log(resultTwo);\n}","metadata":{"source":"examples/src/indexes/vector_stores/opensearch/opensearch.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":65}}}}],["1c9c2c37-a5c9-4599-95ec-bc4eb9770366",{"pageContent":"services:\n  db:\n    image: ankane/pgvector\n    ports:\n      - 5433:5432\n    volumes:\n      - ./data:/var/lib/postgresql/data\n    environment:\n      - POSTGRES_PASSWORD=ChangeMe\n      - POSTGRES_USER=myuser\n      - POSTGRES_DB=api","metadata":{"source":"examples/src/indexes/vector_stores/pgvector_vectorstore/docker-compose.example.yml","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":11}}}}],["913f6551-f236-4d84-91bf-1124f6d1ab8f",{"pageContent":"import { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { PGVectorStore } from \"langchain/vectorstores/pgvector\";\nimport { PoolConfig } from \"pg\";\n\n// First, follow set-up instructions at\n// https://js.langchain.com/docs/modules/indexes/vector_stores/integrations/pgvector\n\nconst config = {\n  postgresConnectionOptions: {\n    type: \"postgres\",\n    host: \"127.0.0.1\",\n    port: 5433,\n    user: \"myuser\",\n    password: \"ChangeMe\",\n    database: \"api\",\n  } as PoolConfig,\n  tableName: \"testlangchain\",\n  columns: {\n    idColumnName: \"id\",\n    vectorColumnName: \"vector\",\n    contentColumnName: \"content\",\n    metadataColumnName: \"metadata\",\n  },\n};\n\nconst pgvectorStore = await PGVectorStore.initialize(\n  new OpenAIEmbeddings(),\n  config\n);\n\nawait pgvectorStore.addDocuments([\n  { pageContent: \"what's this\", metadata: { a: 2 } },\n  { pageContent: \"Cat drinks milk\", metadata: { a: 1 } },\n]);\n\nconst results = await pgvectorStore.similaritySearch(\"water\", 1);\n\nconsole.log(results);\n\n/*\n  [ Document { pageContent: 'Cat drinks milk', metadata: { a: 1 } } ]\n*/\n\nawait pgvectorStore.end();","metadata":{"source":"examples/src/indexes/vector_stores/pgvector_vectorstore/pgvector.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":44}}}}],["6d00a45d-a6aa-4e12-ac1a-ded240412b03",{"pageContent":"# Add DATABASE_URL to .env file in this directory\nDATABASE_URL=postgresql://[USERNAME]:[PASSWORD]@[ADDR]/[DBNAME]","metadata":{"source":"examples/src/indexes/vector_stores/prisma_vectorstore/.env.example","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":2}}}}],["8452f9d0-2baa-4221-8222-6596e456a712",{"pageContent":"data\ndocker-compose.yml","metadata":{"source":"examples/src/indexes/vector_stores/prisma_vectorstore/.gitignore","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":2}}}}],["55a877bd-41cc-4b04-8635-2e9b44df253e",{"pageContent":"services:\n  db:\n    image: ankane/pgvector\n    ports:\n      - 5432:5432\n    volumes:\n      - ./data:/var/lib/postgresql/data\n    environment:\n      - POSTGRES_PASSWORD=\n      - POSTGRES_USER=\n      - POSTGRES_DB=","metadata":{"source":"examples/src/indexes/vector_stores/prisma_vectorstore/docker-compose.example.yml","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":11}}}}],["5a96ea5e-95d5-46c5-8cd7-3e485b50ab3e",{"pageContent":"import { PrismaVectorStore } from \"langchain/vectorstores/prisma\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { PrismaClient, Prisma, Document } from \"@prisma/client\";\n\nexport const run = async () => {\n  const db = new PrismaClient();\n\n  // Use the `withModel` method to get proper type hints for `metadata` field:\n  const vectorStore = PrismaVectorStore.withModel<Document>(db).create(\n    new OpenAIEmbeddings(),\n    {\n      prisma: Prisma,\n      tableName: \"Document\",\n      vectorColumnName: \"vector\",\n      columns: {\n        id: PrismaVectorStore.IdColumn,\n        content: PrismaVectorStore.ContentColumn,\n      },\n    }\n  );\n\n  const texts = [\"Hello world\", \"Bye bye\", \"What's this?\"];\n  await vectorStore.addModels(\n    await db.$transaction(\n      texts.map((content) => db.document.create({ data: { content } }))\n    )\n  );\n\n  const resultOne = await vectorStore.similaritySearch(\"Hello world\", 1);\n  console.log(resultOne);\n\n  // create an instance with default filter\n  const vectorStore2 = PrismaVectorStore.withModel<Document>(db).create(\n    new OpenAIEmbeddings(),\n    {\n      prisma: Prisma,\n      tableName: \"Document\",\n      vectorColumnName: \"vector\",\n      columns: {\n        id: PrismaVectorStore.IdColumn,\n        content: PrismaVectorStore.ContentColumn,\n      },\n      filter: {\n        content: {\n          equals: \"default\",\n        },\n      },\n    }\n  );\n\n  await vectorStore2.addModels(\n    await db.$transaction(\n      texts.map((content) => db.document.create({ data: { content } }))\n    )\n  );\n\n  // Use the default filter a.k.a {\"content\": \"default\"}\n  const resultTwo = await vectorStore.similaritySearch(\"Hello world\", 1);\n  console.log(resultTwo);\n\n  // Override the local filter\n  const resultThree = await vectorStore.similaritySearchWithScore(\n    \"Hello world\",\n    1,\n    { content: { equals: \"different_content\" } }\n  );\n  console.log(resultThree);\n};","metadata":{"source":"examples/src/indexes/vector_stores/prisma_vectorstore/prisma.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":68}}}}],["3b73843b-fc8c-495a-870e-37e9902415ea",{"pageContent":"// This is your Prisma schema file,\n// learn more about it in the docs: https://pris.ly/d/prisma-schema\n\ngenerator client {\n  provider = \"prisma-client-js\"\n}\n\ndatasource db {\n  provider = \"postgresql\"\n  url      = env(\"DATABASE_URL\")\n}\n\nmodel Document {\n  id        String                 @id @default(cuid())\n  content   String\n  namespace String?                @default(\"default\")\n  vector    Unsupported(\"vector\")?\n}","metadata":{"source":"examples/src/indexes/vector_stores/prisma_vectorstore/prisma/schema.prisma","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":18}}}}],["5bef3b86-803d-41fb-bbf1-a663e485b67e",{"pageContent":"# Please do not edit this file manually\n# It should be added in your version-control system (i.e. Git)\nprovider = \"postgresql\"","metadata":{"source":"examples/src/indexes/vector_stores/prisma_vectorstore/prisma/migrations/migration_lock.toml","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":3}}}}],["b3f27739-50fe-4217-b633-fd853f9bbc51",{"pageContent":"-- CreateTable\nCREATE EXTENSION IF NOT EXISTS vector;\nCREATE TABLE \"Document\" (\n    \"id\" TEXT NOT NULL,\n    \"content\" TEXT NOT NULL,\n    \"namespace\" TEXT DEFAULT 'default',\n    \"vector\" vector,\n\n    CONSTRAINT \"Document_pkey\" PRIMARY KEY (\"id\")\n);","metadata":{"source":"examples/src/indexes/vector_stores/prisma_vectorstore/prisma/migrations/00_init/migration.sql","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":10}}}}],["ae8c2cbc-f18c-40ba-b220-2954c62a9c30",{"pageContent":"import { QdrantVectorStore } from \"langchain/vectorstores/qdrant\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\n\n// Create docs with a loader\nconst loader = new TextLoader(\"src/document_loaders/example_data/example.txt\");\nconst docs = await loader.load();\n\nconst vectorStore = await QdrantVectorStore.fromDocuments(\n  docs,\n  new OpenAIEmbeddings(),\n  {\n    url: process.env.QDRANT_URL,\n    collectionName: \"a_test_collection\",\n  }\n);\n\n// Search for the most similar document\nconst response = await vectorStore.similaritySearch(\"hello\", 1);\n\nconsole.log(response);\n/*\n[\n  Document {\n    pageContent: 'Foo\\nBar\\nBaz\\n\\n',\n    metadata: { source: 'src/document_loaders/example_data/example.txt' }\n  }\n]\n*/","metadata":{"source":"examples/src/indexes/vector_stores/qdrant/fromDocs.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":29}}}}],["23f4b44a-cd8e-474d-a99d-e179ad339b8e",{"pageContent":"import { QdrantVectorStore } from \"langchain/vectorstores/qdrant\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nconst vectorStore = await QdrantVectorStore.fromExistingCollection(\n  new OpenAIEmbeddings(),\n  {\n    url: process.env.QDRANT_URL,\n    collectionName: \"goldel_escher_bach\",\n  }\n);\n\nconst response = await vectorStore.similaritySearch(\"scared\", 2);\n\nconsole.log(response);\n\n/*\n[\n  Document { pageContent: 'Achilles: Oh, no!', metadata: {} },\n  Document {\n    pageContent: 'Achilles: Yiikes! What is that?',\n    metadata: { id: 1 }\n  }\n]\n*/","metadata":{"source":"examples/src/indexes/vector_stores/qdrant/fromExisting.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":24}}}}],["bc9698b4-3234-464e-986a-437742463b1c",{"pageContent":"import { QdrantVectorStore } from \"langchain/vectorstores/qdrant\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n// text sample from Godel, Escher, Bach\nconst vectorStore = await QdrantVectorStore.fromTexts(\n  [\n    `Tortoise: Labyrinth? Labyrinth? Could it Are we in the notorious Little\nHarmonic Labyrinth of the dreaded Majotaur?`,\n    `Achilles: Yiikes! What is that?`,\n    `Tortoise: They say-although I person never believed it myself-that an I\n            Majotaur has created a tiny labyrinth sits in a pit in the middle of\n            it, waiting innocent victims to get lost in its fears complexity.\n            Then, when they wander and dazed into the center, he laughs and\n            laughs at them-so hard, that he laughs them to death!`,\n    `Achilles: Oh, no!`,\n    `Tortoise: But it's only a myth. Courage, Achilles.`,\n  ],\n  [{ id: 2 }, { id: 1 }, { id: 3 }, { id: 4 }, { id: 5 }],\n  new OpenAIEmbeddings(),\n  {\n    url: process.env.QDRANT_URL,\n    collectionName: \"goldel_escher_bach\",\n  }\n);\n\nconst response = await vectorStore.similaritySearch(\"scared\", 2);\n\nconsole.log(response);\n\n/*\n[\n  Document { pageContent: 'Achilles: Oh, no!', metadata: {} },\n  Document {\n    pageContent: 'Achilles: Yiikes! What is that?',\n    metadata: { id: 1 }\n  }\n]\n*/","metadata":{"source":"examples/src/indexes/vector_stores/qdrant/fromTexts.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":37}}}}],["e949ae37-917d-4b7f-bd1b-04cc28b2d12a",{"pageContent":"version: '3'\nservices:\n  redis:\n    container_name: redis-stack\n    image: redis/redis-stack:latest\n    ports:\n      - 6379:6379","metadata":{"source":"examples/src/indexes/vector_stores/redis/docker-compose.yml","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":7}}}}],["d47a769d-01a3-4828-99c0-b5d05ca24390",{"pageContent":"import { createClient } from \"redis\";\nimport { Document } from \"langchain/document\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { RedisVectorStore } from \"langchain/vectorstores/redis\";\n\nconst client = createClient({\n  url: process.env.REDIS_URL ?? \"redis://localhost:6379\",\n});\nawait client.connect();\n\nconst docs = [\n  new Document({\n    metadata: { foo: \"bar\" },\n    pageContent: \"redis is fast\",\n  }),\n  new Document({\n    metadata: { foo: \"bar\" },\n    pageContent: \"the quick brown fox jumped over the lazy dog\",\n  }),\n  new Document({\n    metadata: { baz: \"qux\" },\n    pageContent: \"lorem ipsum dolor sit amet\",\n  }),\n  new Document({\n    metadata: { baz: \"qux\" },\n    pageContent: \"consectetur adipiscing elit\",\n  }),\n];\n\nconst vectorStore = await RedisVectorStore.fromDocuments(\n  docs,\n  new OpenAIEmbeddings(),\n  {\n    redisClient: client,\n    indexName: \"docs\",\n  }\n);\n\nawait client.disconnect();","metadata":{"source":"examples/src/indexes/vector_stores/redis/redis.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":39}}}}],["a674cc62-8278-41e7-b51c-beb9cb0aa1c4",{"pageContent":"import { createClient } from \"redis\";\nimport { Document } from \"langchain/document\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { RedisVectorStore } from \"langchain/vectorstores/redis\";\n\nconst client = createClient({\n  url: process.env.REDIS_URL ?? \"redis://localhost:6379\",\n});\nawait client.connect();\n\nconst docs = [\n  new Document({\n    metadata: { foo: \"bar\" },\n    pageContent: \"redis is fast\",\n  }),\n  new Document({\n    metadata: { foo: \"bar\" },\n    pageContent: \"the quick brown fox jumped over the lazy dog\",\n  }),\n  new Document({\n    metadata: { baz: \"qux\" },\n    pageContent: \"lorem ipsum dolor sit amet\",\n  }),\n  new Document({\n    metadata: { baz: \"qux\" },\n    pageContent: \"consectetur adipiscing elit\",\n  }),\n];\n\nconst vectorStore = await RedisVectorStore.fromDocuments(\n  docs,\n  new OpenAIEmbeddings(),\n  {\n    redisClient: client,\n    indexName: \"docs\",\n  }\n);\n\nawait vectorStore.delete({ deleteAll: true });\n\nawait client.disconnect();","metadata":{"source":"examples/src/indexes/vector_stores/redis/redis_delete.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":41}}}}],["06db8d6b-4167-4bce-97d8-fc3c3d814235",{"pageContent":"import { createClient } from \"redis\";\nimport { Document } from \"langchain/document\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { RedisVectorStore } from \"langchain/vectorstores/redis\";\n\nconst client = createClient({\n  url: process.env.REDIS_URL ?? \"redis://localhost:6379\",\n});\nawait client.connect();\n\nconst docs = [\n  new Document({\n    metadata: { foo: \"bar\" },\n    pageContent: \"redis is fast\",\n  }),\n  new Document({\n    metadata: { foo: \"bar\" },\n    pageContent: \"the quick brown fox jumped over the lazy dog\",\n  }),\n  new Document({\n    metadata: { baz: \"qux\" },\n    pageContent: \"lorem ipsum dolor sit amet\",\n  }),\n  new Document({\n    metadata: { baz: \"qux\" },\n    pageContent: \"consectetur adipiscing elit\",\n  }),\n];\n\nconst vectorStore = await RedisVectorStore.fromDocuments(\n  docs,\n  new OpenAIEmbeddings(),\n  {\n    redisClient: client,\n    indexName: \"docs\",\n    createIndexOptions: {\n      TEMPORARY: 1000,\n    },\n  }\n);\n\nawait client.disconnect();","metadata":{"source":"examples/src/indexes/vector_stores/redis/redis_index_options.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":42}}}}],["b4c0a386-8461-45e4-b904-4160fc2b94bf",{"pageContent":"import { createClient } from \"redis\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { RetrievalQAChain } from \"langchain/chains\";\nimport { RedisVectorStore } from \"langchain/vectorstores/redis\";\n\nconst client = createClient({\n  url: process.env.REDIS_URL ?? \"redis://localhost:6379\",\n});\nawait client.connect();\n\nconst vectorStore = new RedisVectorStore(new OpenAIEmbeddings(), {\n  redisClient: client,\n  indexName: \"docs\",\n});\n\n/* Simple standalone search in the vector DB */\nconst simpleRes = await vectorStore.similaritySearch(\"redis\", 1);\nconsole.log(simpleRes);\n/*\n[\n  Document {\n    pageContent: \"redis is fast\",\n    metadata: { foo: \"bar\" }\n  }\n]\n*/\n\n/* Search in the vector DB using filters */\nconst filterRes = await vectorStore.similaritySearch(\"redis\", 3, [\"qux\"]);\nconsole.log(filterRes);\n/*\n[\n  Document {\n    pageContent: \"consectetur adipiscing elit\",\n    metadata: { baz: \"qux\" },\n  },\n  Document {\n    pageContent: \"lorem ipsum dolor sit amet\",\n    metadata: { baz: \"qux\" },\n  }\n]\n*/\n\n/* Usage as part of a chain */\nconst model = new OpenAI();\nconst chain = RetrievalQAChain.fromLLM(model, vectorStore.asRetriever(1), {\n  returnSourceDocuments: true,\n});\nconst chainRes = await chain.call({ query: \"What did the fox do?\" });\nconsole.log(chainRes);\n/*\n{\n  text: \" The fox jumped over the lazy dog.\",\n  sourceDocuments: [\n    Document {\n      pageContent: \"the quick brown fox jumped over the lazy dog\",\n      metadata: [Object]\n    }\n  ]\n}\n*/\n\nawait client.disconnect();","metadata":{"source":"examples/src/indexes/vector_stores/redis/redis_query.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":64}}}}],["e064f7a6-fbc3-45ce-84e2-c5f9037c349a",{"pageContent":"services:\n  db:\n    image: ankane/pgvector\n    ports:\n      - 5432:5432\n    volumes:\n      - ./data:/var/lib/postgresql/data\n    environment:\n      - POSTGRES_PASSWORD=ChangeMe\n      - POSTGRES_USER=myuser\n      - POSTGRES_DB=api","metadata":{"source":"examples/src/indexes/vector_stores/typeorm_vectorstore/docker-compose.example.yml","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":11}}}}],["1f4c0267-833e-4d37-b046-310680cd63dd",{"pageContent":"import { DataSourceOptions } from \"typeorm\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { TypeORMVectorStore } from \"langchain/vectorstores/typeorm\";\n\n// First, follow set-up instructions at\n// https://js.langchain.com/docs/modules/indexes/vector_stores/integrations/typeorm\n\nexport const run = async () => {\n  const args = {\n    postgresConnectionOptions: {\n      type: \"postgres\",\n      host: \"localhost\",\n      port: 5432,\n      username: \"myuser\",\n      password: \"ChangeMe\",\n      database: \"api\",\n    } as DataSourceOptions,\n  };\n\n  const typeormVectorStore = await TypeORMVectorStore.fromDataSource(\n    new OpenAIEmbeddings(),\n    args\n  );\n\n  await typeormVectorStore.ensureTableInDatabase();\n\n  await typeormVectorStore.addDocuments([\n    { pageContent: \"what's this\", metadata: { a: 2 } },\n    { pageContent: \"Cat drinks milk\", metadata: { a: 1 } },\n  ]);\n\n  const results = await typeormVectorStore.similaritySearch(\"hello\", 2);\n\n  console.log(results);\n};","metadata":{"source":"examples/src/indexes/vector_stores/typeorm_vectorstore/typeorm.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":35}}}}],["66894fb4-081c-4323-994f-f91377248eda",{"pageContent":"import { CohereEmbeddings } from \"langchain/embeddings/cohere\";\nimport { VercelPostgres } from \"langchain/vectorstores/vercel_postgres\";\n\n// Config is only required if you want to override default values.\nconst config = {\n  // tableName: \"testvercelvectorstorelangchain\",\n  // postgresConnectionOptions: {\n  //   connectionString: \"postgres://<username>:<password>@<hostname>:<port>/<dbname>\",\n  // },\n  // columns: {\n  //   idColumnName: \"id\",\n  //   vectorColumnName: \"vector\",\n  //   contentColumnName: \"content\",\n  //   metadataColumnName: \"metadata\",\n  // },\n};\n\nconst vercelPostgresStore = await VercelPostgres.initialize(\n  new CohereEmbeddings(),\n  config\n);\n\nconst docHello = {\n  pageContent: \"hello\",\n  metadata: { topic: \"nonsense\" },\n};\nconst docHi = { pageContent: \"hi\", metadata: { topic: \"nonsense\" } };\nconst docMitochondria = {\n  pageContent: \"Mitochondria is the powerhouse of the cell\",\n  metadata: { topic: \"science\" },\n};\n\nconst ids = await vercelPostgresStore.addDocuments([\n  docHello,\n  docHi,\n  docMitochondria,\n]);\n\nconst results = await vercelPostgresStore.similaritySearch(\"hello\", 2);\nconsole.log(results);\n/*\n  [\n    Document { pageContent: 'hello', metadata: { topic: 'nonsense' } },\n    Document { pageContent: 'hi', metadata: { topic: 'nonsense' } }\n  ]\n*/\n\n// Metadata filtering\nconst results2 = await vercelPostgresStore.similaritySearch(\n  \"Irrelevant query, metadata filtering\",\n  2,\n  {\n    topic: \"science\",\n  }\n);\nconsole.log(results2);\n/*\n  [\n    Document {\n      pageContent: 'Mitochondria is the powerhouse of the cell',\n      metadata: { topic: 'science' }\n    }\n  ]\n*/\n\n// Metadata filtering with IN-filters works as well\nconst results3 = await vercelPostgresStore.similaritySearch(\n  \"Irrelevant query, metadata filtering\",\n  3,\n  {\n    topic: { in: [\"science\", \"nonsense\"] },\n  }\n);\nconsole.log(results3);\n/*\n  [\n    Document {\n      pageContent: 'hello',\n      metadata: { topic: 'nonsense' }\n    },\n    Document {\n      pageContent: 'hi',\n      metadata: { topic: 'nonsense' }\n    },\n    Document {\n      pageContent: 'Mitochondria is the powerhouse of the cell',\n      metadata: { topic: 'science' }\n    }\n  ]\n*/\n\n// Upserting is supported as well\nawait vercelPostgresStore.addDocuments(\n  [\n    {\n      pageContent: \"ATP is the powerhouse of the cell\",\n      metadata: { topic: \"science\" },\n    },\n  ],\n  { ids: [ids[2]] }\n);\n\nconst results4 = await vercelPostgresStore.similaritySearch(\n  \"What is the powerhouse of the cell?\",\n  1\n);\nconsole.log(results4);\n/*\n  [\n    Document {\n      pageContent: 'ATP is the powerhouse of the cell',\n      metadata: { topic: 'science' }\n    }\n  ]\n*/\n\nawait vercelPostgresStore.delete({ ids: [ids[2]] });\n\nconst results5 = await vercelPostgresStore.similaritySearch(\n  \"No more metadata\",\n  2,\n  {\n    topic: \"science\",\n  }\n);\nconsole.log(results5);\n/*\n  []\n*/\n\n// Remember to call .end() to close the connection!\nawait vercelPostgresStore.end();","metadata":{"source":"examples/src/indexes/vector_stores/vercel_postgres/example.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":132}}}}],["8809f8aa-d53a-4ad9-937c-717aff003d44",{"pageContent":"import { ZepVectorStore } from \"langchain/vectorstores/zep\";\nimport { FakeEmbeddings } from \"langchain/embeddings/fake\";\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\nimport { randomUUID } from \"crypto\";\n\nconst loader = new TextLoader(\"src/document_loaders/example_data/example.txt\");\nconst docs = await loader.load();\nexport const run = async () => {\n  const collectionName = `collection${randomUUID().split(\"-\")[0]}`;\n\n  const zepConfig = {\n    apiUrl: \"http://localhost:8000\", // this should be the URL of your Zep implementation\n    collectionName,\n    embeddingDimensions: 1536, // this much match the width of the embeddings you're using\n    isAutoEmbedded: true, // If true, the vector store will automatically embed documents when they are added\n  };\n\n  const embeddings = new FakeEmbeddings();\n\n  const vectorStore = await ZepVectorStore.fromDocuments(\n    docs,\n    embeddings,\n    zepConfig\n  );\n\n  // Wait for the documents to be embedded\n  // eslint-disable-next-line no-constant-condition\n  while (true) {\n    const c = await vectorStore.client.document.getCollection(collectionName);\n    console.log(\n      `Embedding status: ${c.document_embedded_count}/${c.document_count} documents embedded`\n    );\n    // eslint-disable-next-line no-promise-executor-return\n    await new Promise((resolve) => setTimeout(resolve, 1000));\n    if (c.status === \"ready\") {\n      break;\n    }\n  }\n\n  const results = await vectorStore.similaritySearchWithScore(\"bar\", 3);\n\n  console.log(\"Similarity Results:\");\n  console.log(JSON.stringify(results));\n\n  const results2 = await vectorStore.maxMarginalRelevanceSearch(\"bar\", {\n    k: 3,\n  });\n\n  console.log(\"MMR Results:\");\n  console.log(JSON.stringify(results2));\n};","metadata":{"source":"examples/src/indexes/vector_stores/zep/zep_from_docs.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":51}}}}],["d15a4a71-0e48-4b08-8e48-c9dd7b3056a5",{"pageContent":"import { ZepVectorStore } from \"langchain/vectorstores/zep\";\nimport { Document } from \"langchain/document\";\nimport { FakeEmbeddings } from \"langchain/embeddings/fake\";\nimport { randomUUID } from \"crypto\";\n\nconst docs = [\n  new Document({\n    metadata: { album: \"Led Zeppelin IV\", year: 1971 },\n    pageContent:\n      \"Stairway to Heaven is one of the most iconic songs by Led Zeppelin.\",\n  }),\n  new Document({\n    metadata: { album: \"Led Zeppelin I\", year: 1969 },\n    pageContent:\n      \"Dazed and Confused was a standout track on Led Zeppelin's debut album.\",\n  }),\n  new Document({\n    metadata: { album: \"Physical Graffiti\", year: 1975 },\n    pageContent:\n      \"Kashmir, from Physical Graffiti, showcases Led Zeppelin's unique blend of rock and world music.\",\n  }),\n  new Document({\n    metadata: { album: \"Houses of the Holy\", year: 1973 },\n    pageContent:\n      \"The Rain Song is a beautiful, melancholic piece from Houses of the Holy.\",\n  }),\n  new Document({\n    metadata: { band: \"Black Sabbath\", album: \"Paranoid\", year: 1970 },\n    pageContent:\n      \"Paranoid is Black Sabbath's second studio album and includes some of their most notable songs.\",\n  }),\n  new Document({\n    metadata: {\n      band: \"Iron Maiden\",\n      album: \"The Number of the Beast\",\n      year: 1982,\n    },\n    pageContent:\n      \"The Number of the Beast is often considered Iron Maiden's best album.\",\n  }),\n  new Document({\n    metadata: { band: \"Metallica\", album: \"Master of Puppets\", year: 1986 },\n    pageContent:\n      \"Master of Puppets is widely regarded as Metallica's finest work.\",\n  }),\n  new Document({\n    metadata: { band: \"Megadeth\", album: \"Rust in Peace\", year: 1990 },\n    pageContent:\n      \"Rust in Peace is Megadeth's fourth studio album and features intricate guitar work.\",\n  }),\n];\n\nexport const run = async () => {\n  const collectionName = `collection${randomUUID().split(\"-\")[0]}`;\n\n  const zepConfig = {\n    apiUrl: \"http://localhost:8000\", // this should be the URL of your Zep implementation\n    collectionName,\n    embeddingDimensions: 1536, // this much match the width of the embeddings you're using\n    isAutoEmbedded: true, // If true, the vector store will automatically embed documents when they are added\n  };\n\n  const embeddings = new FakeEmbeddings();\n\n  const vectorStore = await ZepVectorStore.fromDocuments(\n    docs,\n    embeddings,\n    zepConfig\n  );\n\n  // Wait for the documents to be embedded\n  // eslint-disable-next-line no-constant-condition\n  while (true) {\n    const c = await vectorStore.client.document.getCollection(collectionName);\n    console.log(\n      `Embedding status: ${c.document_embedded_count}/${c.document_count} documents embedded`\n    );\n    // eslint-disable-next-line no-promise-executor-return\n    await new Promise((resolve) => setTimeout(resolve, 1000));\n    if (c.status === \"ready\") {\n      break;\n    }\n  }\n\n  vectorStore\n    .similaritySearchWithScore(\"sad music\", 3, {\n      where: { jsonpath: \"$[*] ? (@.year == 1973)\" }, // We should see a single result: The Rain Song\n    })\n    .then((results) => {\n      console.log(`\\n\\nSimilarity Results:\\n${JSON.stringify(results)}`);\n    })\n    .catch((e) => {\n      if (e.name === \"NotFoundError\") {\n        console.log(\"No results found\");\n      } else {\n        throw e;\n      }\n    });\n\n  // We're not filtering here, but rather demonstrating MMR at work.\n  // We could also add a filter to the MMR search, as we did with the similarity search above.\n  vectorStore\n    .maxMarginalRelevanceSearch(\"sad music\", {\n      k: 3,\n    })\n    .then((results) => {\n      console.log(`\\n\\nMMR Results:\\n${JSON.stringify(results)}`);\n    })\n    .catch((e) => {\n      if (e.name === \"NotFoundError\") {\n        console.log(\"No results found\");\n      } else {\n        throw e;\n      }\n    });\n};","metadata":{"source":"examples/src/indexes/vector_stores/zep/zep_with_metadata.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":116}}}}],["f40db729-b302-465b-96af-e8bf4827e7b6",{"pageContent":"import { ZepVectorStore } from \"langchain/vectorstores/zep\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\nimport { randomUUID } from \"crypto\";\n\nconst loader = new TextLoader(\"src/document_loaders/example_data/example.txt\");\nconst docs = await loader.load();\nexport const run = async () => {\n  const collectionName = `collection${randomUUID().split(\"-\")[0]}`;\n\n  const zepConfig = {\n    apiUrl: \"http://localhost:8000\", // this should be the URL of your Zep implementation\n    collectionName,\n    embeddingDimensions: 1536, // this much match the width of the embeddings you're using\n    isAutoEmbedded: false, // set to false to disable auto-embedding\n  };\n\n  const embeddings = new OpenAIEmbeddings();\n\n  const vectorStore = await ZepVectorStore.fromDocuments(\n    docs,\n    embeddings,\n    zepConfig\n  );\n\n  const results = await vectorStore.similaritySearchWithScore(\"bar\", 3);\n\n  console.log(\"Similarity Results:\");\n  console.log(JSON.stringify(results));\n\n  const results2 = await vectorStore.maxMarginalRelevanceSearch(\"bar\", {\n    k: 3,\n  });\n\n  console.log(\"MMR Results:\");\n  console.log(JSON.stringify(results2));\n};","metadata":{"source":"examples/src/indexes/vector_stores/zep/zep_with_openai_embeddings.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":37}}}}],["35d8d59a-ab67-4cd3-ba16-9f21030f9a0b",{"pageContent":"import { Bedrock } from \"langchain/llms/bedrock\";\n\nasync function test() {\n  const model = new Bedrock({model: \"bedrock-model-name\", region: \"aws-region\"});\n  const res = await model.call(\"Question: What would be a good company name a company that makes colorful socks?\\nAnswer:\");\n  console.log(res);\n}\ntest();","metadata":{"source":"examples/src/llms/bedrock.js","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":8}}}}],["20384620-a275-4078-a6e7-9347a00333cf",{"pageContent":"import { Cohere } from \"langchain/llms/cohere\";\n\nexport const run = async () => {\n  const model = new Cohere({\n    temperature: 0.7,\n    maxTokens: 20,\n    maxRetries: 5,\n  });\n  const res = await model.call(\n    \"Question: What would be a good company name a company that makes colorful socks?\\nAnswer:\"\n  );\n  console.log({ res });\n};","metadata":{"source":"examples/src/llms/cohere.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":13}}}}],["95b34b3e-ec06-4415-ac7e-9d98a0f4099a",{"pageContent":"import { GoogleVertexAI } from \"langchain/llms/googlevertexai\";\n\n/*\n * Before running this, you should make sure you have created a\n * Google Cloud Project that is permitted to the Vertex AI API.\n *\n * You will also need permission to access this project / API.\n * Typically, this is done in one of three ways:\n * - You are logged into an account permitted to that project.\n * - You are running this on a machine using a service account permitted to\n *   the project.\n * - The `GOOGLE_APPLICATION_CREDENTIALS` environment variable is set to the\n *   path of a credentials file for a service account permitted to the project.\n */\n\nconst model = new GoogleVertexAI({\n  model: \"code-bison\",\n  maxOutputTokens: 2048,\n});\nconst res = await model.call(\"A Javascript function that counts from 1 to 10.\");\nconsole.log({ res });","metadata":{"source":"examples/src/llms/googlevertexai-code-bison.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":21}}}}],["7f6f4051-63b9-4d79-ae9c-35f75b271953",{"pageContent":"import { GoogleVertexAI } from \"langchain/llms/googlevertexai\";\n\n/*\n * Before running this, you should make sure you have created a\n * Google Cloud Project that is permitted to the Vertex AI API.\n *\n * You will also need permission to access this project / API.\n * Typically, this is done in one of three ways:\n * - You are logged into an account permitted to that project.\n * - You are running this on a machine using a service account permitted to\n *   the project.\n * - The `GOOGLE_APPLICATION_CREDENTIALS` environment variable is set to the\n *   path of a credentials file for a service account permitted to the project.\n */\n\nconst model = new GoogleVertexAI({\n  model: \"code-gecko\",\n});\nconst res = await model.call(\"for (let co=0;\");\nconsole.log({ res });","metadata":{"source":"examples/src/llms/googlevertexai-code-gecko.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":20}}}}],["0eb5010f-6522-4ae3-9667-34f84fb7d432",{"pageContent":"import { GoogleVertexAI } from \"langchain/llms/googlevertexai\";\n\nconst model = new GoogleVertexAI({\n  temperature: 0.7,\n});\nconst stream = await model.stream(\n  \"What would be a good company name for a company that makes colorful socks?\"\n);\n\nfor await (const chunk of stream) {\n  console.log(\"\\n---------\\nChunk:\\n---------\\n\", chunk);\n}\n\n/*\n  ---------\n  Chunk:\n  ---------\n    1. Toe-tally Awesome Socks\n  2. The Sock Drawer\n  3. Happy Feet\n  4. \n\n  ---------\n  Chunk:\n  ---------\n  Sock It to Me\n  5. Crazy Color Socks\n  6. Wild and Wacky Socks\n  7. Fu\n\n  ---------\n  Chunk:\n  ---------\n  nky Feet\n  8. Mismatched Socks\n  9. Rainbow Socks\n  10. Sole Mates\n\n  ---------\n  Chunk:\n  ---------\n  \n\n*/","metadata":{"source":"examples/src/llms/googlevertexai-streaming.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":44}}}}],["f36ea1a1-02b5-4d30-acae-3778ac3bfb8a",{"pageContent":"import { GoogleVertexAI } from \"langchain/llms/googlevertexai\";\n// Or, if using the web entrypoint:\n// import { GoogleVertexAI } from \"langchain/llms/googlevertexai/web\";\n\n/*\n * Before running this, you should make sure you have created a\n * Google Cloud Project that is permitted to the Vertex AI API.\n *\n * You will also need permission to access this project / API.\n * Typically, this is done in one of three ways:\n * - You are logged into an account permitted to that project.\n * - You are running this on a machine using a service account permitted to\n *   the project.\n * - The `GOOGLE_APPLICATION_CREDENTIALS` environment variable is set to the\n *   path of a credentials file for a service account permitted to the project.\n */\nconst model = new GoogleVertexAI({\n  temperature: 0.7,\n});\nconst res = await model.call(\n  \"What would be a good company name for a company that makes colorful socks?\"\n);\nconsole.log({ res });","metadata":{"source":"examples/src/llms/googlevertexai.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":23}}}}],["9c181171-b58b-4a3c-9cee-009f7977bab1",{"pageContent":"import { HuggingFaceInference } from \"langchain/llms/hf\";\n\nexport const run = async () => {\n  const model = new HuggingFaceInference({\n    model: \"gpt2\",\n    temperature: 0.7,\n    maxTokens: 50,\n  });\n  const res = await model.call(\n    \"Question: What would be a good company name a company that makes colorful socks?\\nAnswer:\"\n  );\n  console.log({ res });\n};","metadata":{"source":"examples/src/llms/hf.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":13}}}}],["cba300bc-8d70-40d2-80ab-e333235e9394",{"pageContent":"import { Ollama } from \"langchain/llms/ollama\";\n\nconst ollama = new Ollama({\n  baseUrl: \"http://localhost:11434\", // Default value\n  model: \"llama2\", // Default value\n});\n\nconst stream = await ollama.stream(\n  `Translate \"I love programming\" into German.`\n);\n\nconst chunks = [];\nfor await (const chunk of stream) {\n  chunks.push(chunk);\n}\n\nconsole.log(chunks.join(\"\"));\n\n/*\n  I'm glad to help! \"I love programming\" can be translated to German as \"Ich liebe Programmieren.\"\n\n  It's important to note that the translation of \"I love\" in German is \"ich liebe,\" which is a more formal and polite way of saying \"I love.\" In informal situations, people might use \"mag ich\" or \"möchte ich\" instead.\n\n  Additionally, the word \"Programmieren\" is the correct term for \"programming\" in German. It's a combination of two words: \"Programm\" and \"-ieren,\" which means \"to do something.\" So, the full translation of \"I love programming\" would be \"Ich liebe Programmieren.\n*/","metadata":{"source":"examples/src/llms/ollama.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":25}}}}],["0c3b2d83-f370-4895-85b7-59822844d598",{"pageContent":"import { OpenAIChat } from \"langchain/llms/openai\";\n\nexport const run = async () => {\n  const model = new OpenAIChat({\n    prefixMessages: [\n      {\n        role: \"system\",\n        content: \"You are a helpful assistant that answers in pirate language\",\n      },\n    ],\n    maxTokens: 50,\n  });\n  const res = await model.call(\n    \"What would be a good company name a company that makes colorful socks?\"\n  );\n  console.log({ res });\n};","metadata":{"source":"examples/src/llms/openai-chat.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":17}}}}],["53b35dab-7c1e-4266-bdc4-62c29f1b4014",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\n\nexport const run = async () => {\n  const model = new OpenAI({\n    modelName: \"gpt-4\",\n    temperature: 0.7,\n    maxTokens: 1000,\n    maxRetries: 5,\n  });\n  const res = await model.call(\n    \"Question: What would be a good company name a company that makes colorful socks?\\nAnswer:\"\n  );\n  console.log({ res });\n};","metadata":{"source":"examples/src/llms/openai.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":14}}}}],["99154967-9be7-42b8-851f-4afac2240ecf",{"pageContent":"import { PortkeyChat } from \"langchain/chat_models/portkey\";\nimport { SystemMessage } from \"langchain/schema\";\n\nexport const run = async () => {\n  const model = new PortkeyChat({\n    mode: \"single\",\n    llms: [\n      {\n        provider: \"openai\",\n        virtual_key: \"open-ai-key-1234\",\n        model: \"gpt-3.5-turbo\",\n        max_tokens: 2000,\n      },\n    ],\n  });\n  const chatPrompt = [new SystemMessage(\"Question: Write a story\")];\n  const res = await model.stream(chatPrompt);\n  for await (const i of res) {\n    process.stdout.write(i.content);\n  }\n};","metadata":{"source":"examples/src/llms/portkey-chat.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":21}}}}],["d03142b9-6cce-4be0-a05c-ca427e612df2",{"pageContent":"import { Portkey } from \"langchain/llms/portkey\";\n\nexport const run = async () => {\n  const model = new Portkey({\n    mode: \"single\",\n    llms: [\n      {\n        provider: \"openai\",\n        virtual_key: \"open-ai-key-1234\",\n        model: \"text-davinci-003\",\n        max_tokens: 2000,\n      },\n    ],\n  });\n  const res = await model.stream(\n    \"Question: Write a story about a king\\nAnswer:\"\n  );\n  for await (const i of res) {\n    process.stdout.write(i);\n  }\n};","metadata":{"source":"examples/src/llms/portkey.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":21}}}}],["d59b32ac-aeb3-4058-8e72-24d0e2194d62",{"pageContent":"import { Replicate } from \"langchain/llms/replicate\";\n\nexport const run = async () => {\n  const model = new Replicate({\n    model:\n      \"replicate/flan-t5-xl:3ae0799123a1fe11f8c89fd99632f843fc5f7a761630160521c4253149754523\",\n  });\n  const res = await model.call(\n    \"Question: What would be a good company name a company that makes colorful socks?\\nAnswer:\"\n  );\n  console.log({ res });\n};","metadata":{"source":"examples/src/llms/replicate.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":12}}}}],["7a96f192-6b00-4ea1-85d3-a21164170be6",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { BufferMemory } from \"langchain/memory\";\nimport { LLMChain } from \"langchain/chains\";\nimport { PromptTemplate } from \"langchain/prompts\";\n\nconst memory = new BufferMemory({ memoryKey: \"chat_history\" });\nconst model = new OpenAI({ temperature: 0.9 });\nconst prompt =\n  PromptTemplate.fromTemplate(`The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\nCurrent conversation:\n{chat_history}\nHuman: {input}\nAI:`);\nconst chain = new LLMChain({ llm: model, prompt, memory });\n\nconst res1 = await chain.call({ input: \"Hi! I'm Jim.\" });\nconsole.log({ res1 });\n\nconst res2 = await chain.call({ input: \"What's my name?\" });\nconsole.log({ res2 });","metadata":{"source":"examples/src/memory/buffer.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":21}}}}],["d94ca7ec-5e3d-4418-9a90-a9d464d1b172",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { BufferWindowMemory } from \"langchain/memory\";\nimport { LLMChain } from \"langchain/chains\";\nimport { PromptTemplate } from \"langchain/prompts\";\n\nexport const run = async () => {\n  const memory = new BufferWindowMemory({ memoryKey: \"chat_history\", k: 1 });\n  const model = new OpenAI({ temperature: 0.9 });\n  const template = `The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\n    Current conversation:\n    {chat_history}\n    Human: {input}\n    AI:`;\n\n  const prompt = PromptTemplate.fromTemplate(template);\n  const chain = new LLMChain({ llm: model, prompt, memory });\n  const res1 = await chain.call({ input: \"Hi! I'm Jim.\" });\n  console.log({ res1 });\n  const res2 = await chain.call({ input: \"What's my name?\" });\n  console.log({ res2 });\n};","metadata":{"source":"examples/src/memory/buffer_window.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":22}}}}],["dced0d88-10b5-43d0-a355-3f9562c312e6",{"pageContent":"import { BufferMemory } from \"langchain/memory\";\nimport { CassandraChatMessageHistory } from \"langchain/stores/message/cassandra\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { ConversationChain } from \"langchain/chains\";\n\nconst memory = new BufferMemory({\n  chatHistory: new CassandraChatMessageHistory({\n    cloud: {\n      secureConnectBundle: \"<path to your secure bundle>\",\n    },\n    credentials: {\n      username: \"token\",\n      password: \"<your Cassandra access token>\",\n    },\n    keyspace: \"langchain\",\n    table: \"message_history\",\n    sessionId: \"<some unique session identifier>\",\n  }),\n});\n\nconst model = new ChatOpenAI();\nconst chain = new ConversationChain({ llm: model, memory });\n\nconst res1 = await chain.call({ input: \"Hi! I'm Jonathan.\" });\nconsole.log({ res1 });\n/*\n{\n  res1: {\n    text: \"Hello Jonathan! How can I assist you today?\"\n  }\n}\n*/\n\nconst res2 = await chain.call({ input: \"What did I just say my name was?\" });\nconsole.log({ res2 });\n\n/*\n{\n  res1: {\n    text: \"You said your name was Jonathan.\"\n  }\n}\n*/","metadata":{"source":"examples/src/memory/cassandra-store.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":43}}}}],["acb93910-9fec-4275-9b7a-f1cc40e9e655",{"pageContent":"import type { D1Database } from \"@cloudflare/workers-types\";\n\nimport { BufferMemory } from \"langchain/memory\";\nimport { CloudflareD1MessageHistory } from \"langchain/stores/message/cloudflare_d1\";\nimport { ChatAnthropic } from \"langchain/chat_models/anthropic\";\nimport { ChatPromptTemplate, MessagesPlaceholder } from \"langchain/prompts\";\nimport { RunnableSequence } from \"langchain/schema/runnable\";\nimport { StringOutputParser } from \"langchain/schema/output_parser\";\n\nexport interface Env {\n  DB: D1Database;\n\n  ANTHROPIC_API_KEY: string;\n}\n\nexport default {\n  async fetch(request: Request, env: Env): Promise<Response> {\n    try {\n      const { searchParams } = new URL(request.url);\n      const input = searchParams.get(\"input\");\n      if (!input) {\n        throw new Error(`Missing \"input\" parameter`);\n      }\n      const memory = new BufferMemory({\n        returnMessages: true,\n        chatHistory: new CloudflareD1MessageHistory({\n          tableName: \"stored_message\",\n          sessionId: \"example\",\n          database: env.DB,\n        }),\n      });\n      const prompt = ChatPromptTemplate.fromPromptMessages([\n        [\"system\", \"You are a helpful chatbot\"],\n        new MessagesPlaceholder(\"history\"),\n        [\"human\", \"{input}\"],\n      ]);\n      const model = new ChatAnthropic({\n        anthropicApiKey: env.ANTHROPIC_API_KEY,\n      });\n\n      const chain = RunnableSequence.from([\n        {\n          input: (initialInput) => initialInput.input,\n          memory: () => memory.loadMemoryVariables({}),\n        },\n        {\n          input: (previousOutput) => previousOutput.input,\n          history: (previousOutput) => previousOutput.memory.history,\n        },\n        prompt,\n        model,\n        new StringOutputParser(),\n      ]);\n\n      const chainInput = { input };\n\n      const res = await chain.invoke(chainInput);\n      await memory.saveContext(chainInput, {\n        output: res,\n      });\n\n      return new Response(JSON.stringify(res), {\n        headers: { \"content-type\": \"application/json\" },\n      });\n    } catch (err: any) {\n      console.log(err.message);\n      return new Response(err.message, { status: 500 });\n    }\n  },\n};","metadata":{"source":"examples/src/memory/cloudflare_d1.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":70}}}}],["0a158352-1a46-4236-a552-af2e586ee5c2",{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport {\n  BufferMemory,\n  CombinedMemory,\n  ConversationSummaryMemory,\n} from \"langchain/memory\";\nimport { ConversationChain } from \"langchain/chains\";\nimport { PromptTemplate } from \"langchain/prompts\";\n\n// buffer memory\nconst bufferMemory = new BufferMemory({\n  memoryKey: \"chat_history_lines\",\n  inputKey: \"input\",\n});\n\n// summary memory\nconst summaryMemory = new ConversationSummaryMemory({\n  llm: new ChatOpenAI({ modelName: \"gpt-3.5-turbo\", temperature: 0 }),\n  inputKey: \"input\",\n  memoryKey: \"conversation_summary\",\n});\n\n//\nconst memory = new CombinedMemory({\n  memories: [bufferMemory, summaryMemory],\n});\n\nconst _DEFAULT_TEMPLATE = `The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\nSummary of conversation:\n{conversation_summary}\nCurrent conversation:\n{chat_history_lines}\nHuman: {input}\nAI:`;\n\nconst PROMPT = new PromptTemplate({\n  inputVariables: [\"input\", \"conversation_summary\", \"chat_history_lines\"],\n  template: _DEFAULT_TEMPLATE,\n});\nconst model = new ChatOpenAI({ temperature: 0.9, verbose: true });\nconst chain = new ConversationChain({ llm: model, memory, prompt: PROMPT });\n\nconst res1 = await chain.call({ input: \"Hi! I'm Jim.\" });\nconsole.log({ res1 });\n\n/*\n  {\n    res1: {\n      response: \"Hello Jim! It's nice to meet you. How can I assist you today?\"\n    }\n  }\n*/\n\nconst res2 = await chain.call({ input: \"Can you tell me a joke?\" });\nconsole.log({ res2 });\n\n/*\n  {\n    res2: {\n      response: 'Why did the scarecrow win an award? Because he was outstanding in his field!'\n    }\n  }\n*/\n\nconst res3 = await chain.call({\n  input: \"What's my name and what joke did you just tell?\",\n});\nconsole.log({ res3 });\n\n/*\n  {\n    res3: {\n      response: 'Your name is Jim. The joke I just told was about a scarecrow winning an award because he was outstanding in his field.'\n    }\n  }\n*/","metadata":{"source":"examples/src/memory/combined.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":77}}}}],["135b71d2-6136-49f4-a0ba-68eb601b0bf2",{"pageContent":"import { BufferMemory } from \"langchain/memory\";\nimport { DynamoDBChatMessageHistory } from \"langchain/stores/message/dynamodb\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { ConversationChain } from \"langchain/chains\";\n\nconst memory = new BufferMemory({\n  chatHistory: new DynamoDBChatMessageHistory({\n    tableName: \"langchain\",\n    partitionKey: \"id\",\n    sessionId: new Date().toISOString(), // Or some other unique identifier for the conversation\n    config: {\n      region: \"us-east-2\",\n      credentials: {\n        accessKeyId: \"<your AWS access key id>\",\n        secretAccessKey: \"<your AWS secret access key>\",\n      },\n    },\n  }),\n});\n\nconst model = new ChatOpenAI();\nconst chain = new ConversationChain({ llm: model, memory });\n\nconst res1 = await chain.call({ input: \"Hi! I'm Jim.\" });\nconsole.log({ res1 });\n/*\n{\n  res1: {\n    text: \"Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?\"\n  }\n}\n*/\n\nconst res2 = await chain.call({ input: \"What did I just say my name was?\" });\nconsole.log({ res2 });\n\n/*\n{\n  res1: {\n    text: \"You said your name was Jim.\"\n  }\n}\n*/","metadata":{"source":"examples/src/memory/dynamodb-store.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":43}}}}],["fe94e96a-56cd-495d-a6dc-6b52def9f21b",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport {\n  EntityMemory,\n  ENTITY_MEMORY_CONVERSATION_TEMPLATE,\n} from \"langchain/memory\";\nimport { LLMChain } from \"langchain/chains\";\n\nexport const run = async () => {\n  const memory = new EntityMemory({\n    llm: new OpenAI({ temperature: 0 }),\n    chatHistoryKey: \"history\", // Default value\n    entitiesKey: \"entities\", // Default value\n  });\n  const model = new OpenAI({ temperature: 0.9 });\n  const chain = new LLMChain({\n    llm: model,\n    prompt: ENTITY_MEMORY_CONVERSATION_TEMPLATE, // Default prompt - must include the set chatHistoryKey and entitiesKey as input variables.\n    memory,\n  });\n\n  const res1 = await chain.call({ input: \"Hi! I'm Jim.\" });\n  console.log({\n    res1,\n    memory: await memory.loadMemoryVariables({ input: \"Who is Jim?\" }),\n  });\n\n  const res2 = await chain.call({\n    input: \"I work in construction. What about you?\",\n  });\n  console.log({\n    res2,\n    memory: await memory.loadMemoryVariables({ input: \"Who is Jim?\" }),\n  });\n};","metadata":{"source":"examples/src/memory/entity.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":34}}}}],["7d1b0fce-193d-4cb3-9cc5-53cc1a45220b",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport {\n  EntityMemory,\n  ENTITY_MEMORY_CONVERSATION_TEMPLATE,\n} from \"langchain/memory\";\nimport { LLMChain } from \"langchain/chains\";\n\nconst memory = new EntityMemory({\n  llm: new OpenAI({ temperature: 0 }),\n});\nconst model = new OpenAI({ temperature: 0.9 });\nconst chain = new LLMChain({\n  llm: model,\n  prompt: ENTITY_MEMORY_CONVERSATION_TEMPLATE,\n  memory,\n});\n\nawait chain.call({ input: \"Hi! I'm Jim.\" });\n\nawait chain.call({\n  input: \"I work in sales. What about you?\",\n});\n\nconst res = await chain.call({\n  input: \"My office is the Utica branch of Dunder Mifflin. What about you?\",\n});\nconsole.log({\n  res,\n  memory: await memory.loadMemoryVariables({ input: \"Who is Jim?\" }),\n});\n\n/*\n  {\n    res: \"As an AI language model, I don't have an office in the traditional sense. I exist entirely in digital space and am here to assist you with any questions or tasks you may have. Is there anything specific you need help with regarding your work at the Utica branch of Dunder Mifflin?\",\n    memory: {\n      entities: {\n        Jim: 'Jim is a human named Jim who works in sales.',\n        Utica: 'Utica is the location of the branch of Dunder Mifflin where Jim works.',\n        'Dunder Mifflin': 'Dunder Mifflin has a branch in Utica.'\n      }\n    }\n  }\n*/","metadata":{"source":"examples/src/memory/entity_memory_inspection.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":43}}}}],["9b9a225d-1655-4212-b277-6afe9317e0c3",{"pageContent":"import { BufferMemory } from \"langchain/memory\";\nimport { FirestoreChatMessageHistory } from \"langchain/stores/message/firestore\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { ConversationChain } from \"langchain/chains\";\n\nconst memory = new BufferMemory({\n  chatHistory: new FirestoreChatMessageHistory({\n    collectionName: \"langchain\",\n    sessionId: \"lc-example\",\n    userId: \"a@example.com\",\n    config: { projectId: \"your-project-id\" },\n  }),\n});\n\nconst model = new ChatOpenAI();\nconst chain = new ConversationChain({ llm: model, memory });\n\nconst res1 = await chain.call({ input: \"Hi! I'm Jim.\" });\nconsole.log({ res1 });\n/*\n{\n  res1: {\n    text: \"Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?\"\n  }\n}\n*/\n\nconst res2 = await chain.call({ input: \"What did I just say my name was?\" });\nconsole.log({ res2 });\n\n/*\n{\n  res1: {\n    text: \"You said your name was Jim.\"\n  }\n}\n*/","metadata":{"source":"examples/src/memory/firestore.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":37}}}}],["9a6474ac-9216-480c-97a2-419f3e114bdc",{"pageContent":"import {\n  CacheClient,\n  Configurations,\n  CredentialProvider,\n} from \"@gomomento/sdk\"; // `from \"gomomento/sdk-web\";` for browser/edge\nimport { BufferMemory } from \"langchain/memory\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { ConversationChain } from \"langchain/chains\";\nimport { MomentoChatMessageHistory } from \"langchain/stores/message/momento\";\n\n// See https://github.com/momentohq/client-sdk-javascript for connection options\nconst client = new CacheClient({\n  configuration: Configurations.Laptop.v1(),\n  credentialProvider: CredentialProvider.fromEnvironmentVariable({\n    environmentVariableName: \"MOMENTO_API_KEY\",\n  }),\n  defaultTtlSeconds: 60 * 60 * 24,\n});\n\n// Create a unique session ID\nconst sessionId = new Date().toISOString();\nconst cacheName = \"langchain\";\n\nconst memory = new BufferMemory({\n  chatHistory: await MomentoChatMessageHistory.fromProps({\n    client,\n    cacheName,\n    sessionId,\n    sessionTtl: 300,\n  }),\n});\nconsole.log(\n  `cacheName=${cacheName} and sessionId=${sessionId} . This will be used to store the chat history. You can inspect the values at your Momento console at https://console.gomomento.com.`\n);\n\nconst model = new ChatOpenAI({\n  modelName: \"gpt-3.5-turbo\",\n  temperature: 0,\n});\n\nconst chain = new ConversationChain({ llm: model, memory });\n\nconst res1 = await chain.call({ input: \"Hi! I'm Jim.\" });\nconsole.log({ res1 });\n/*\n{\n  res1: {\n    text: \"Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?\"\n  }\n}\n*/\n\nconst res2 = await chain.call({ input: \"What did I just say my name was?\" });\nconsole.log({ res2 });\n\n/*\n{\n  res1: {\n    text: \"You said your name was Jim.\"\n  }\n}\n*/\n\n// See the chat history in the Momento\nconsole.log(await memory.chatHistory.getMessages());","metadata":{"source":"examples/src/memory/momento.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":65}}}}],["3970cd37-b395-4022-ba1d-9f5875fd0761",{"pageContent":"import { MongoClient, ObjectId } from \"mongodb\";\nimport { BufferMemory } from \"langchain/memory\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { ConversationChain } from \"langchain/chains\";\nimport { MongoDBChatMessageHistory } from \"langchain/stores/message/mongodb\";\n\nconst client = new MongoClient(process.env.MONGODB_ATLAS_URI || \"\");\nawait client.connect();\nconst collection = client.db(\"langchain\").collection(\"memory\");\n\n// generate a new sessionId string\nconst sessionId = new ObjectId().toString();\n\nconst memory = new BufferMemory({\n  chatHistory: new MongoDBChatMessageHistory({\n    collection,\n    sessionId,\n  }),\n});\n\nconst model = new ChatOpenAI({\n  modelName: \"gpt-3.5-turbo\",\n  temperature: 0,\n});\n\nconst chain = new ConversationChain({ llm: model, memory });\n\nconst res1 = await chain.call({ input: \"Hi! I'm Jim.\" });\nconsole.log({ res1 });\n/*\n  {\n    res1: {\n      text: \"Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?\"\n    }\n  }\n  */\n\nconst res2 = await chain.call({ input: \"What did I just say my name was?\" });\nconsole.log({ res2 });\n\n/*\n  {\n    res1: {\n      text: \"You said your name was Jim.\"\n    }\n  }\n  */\n\n// See the chat history in the MongoDb\nconsole.log(await memory.chatHistory.getMessages());\n\n// clear chat history\nawait memory.chatHistory.clear();","metadata":{"source":"examples/src/memory/mongodb.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":53}}}}],["5817ab63-0a4a-4997-8b11-7381f2d9bc28",{"pageContent":"import { MotorheadMemory } from \"langchain/memory\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { ConversationChain } from \"langchain/chains\";\n\n// Managed Example (visit https://getmetal.io to get your keys)\n// const managedMemory = new MotorheadMemory({\n//   memoryKey: \"chat_history\",\n//   sessionId: \"test\",\n//   apiKey: \"MY_API_KEY\",\n//   clientId: \"MY_CLIENT_ID\",\n// });\n\n// Self Hosted Example\nconst memory = new MotorheadMemory({\n  memoryKey: \"chat_history\",\n  sessionId: \"test\",\n  url: \"localhost:8080\", // Required for self hosted\n});\n\nconst model = new ChatOpenAI({\n  modelName: \"gpt-3.5-turbo\",\n  temperature: 0,\n});\n\nconst chain = new ConversationChain({ llm: model, memory });\n\nconst res1 = await chain.call({ input: \"Hi! I'm Jim.\" });\nconsole.log({ res1 });\n/*\n{\n  res1: {\n    text: \"Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?\"\n  }\n}\n*/\n\nconst res2 = await chain.call({ input: \"What did I just say my name was?\" });\nconsole.log({ res2 });\n\n/*\n{\n  res1: {\n    text: \"You said your name was Jim.\"\n  }\n}\n*/","metadata":{"source":"examples/src/memory/motorhead.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":46}}}}],["db43cc0b-242a-48d8-905a-76477ed4ece5",{"pageContent":"import { BufferMemory } from \"langchain/memory\";\nimport { PlanetScaleChatMessageHistory } from \"langchain/stores/message/planetscale\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { ConversationChain } from \"langchain/chains\";\n\nconst memory = new BufferMemory({\n  chatHistory: new PlanetScaleChatMessageHistory({\n    tableName: \"stored_message\",\n    sessionId: \"lc-example\",\n    config: {\n      url: \"ADD_YOURS_HERE\", // Override with your own database instance's URL\n    },\n  }),\n});\n\nconst model = new ChatOpenAI();\nconst chain = new ConversationChain({ llm: model, memory });\n\nconst res1 = await chain.call({ input: \"Hi! I'm Jim.\" });\nconsole.log({ res1 });\n/*\n{\n  res1: {\n    text: \"Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?\"\n  }\n}\n*/\n\nconst res2 = await chain.call({ input: \"What did I just say my name was?\" });\nconsole.log({ res2 });\n\n/*\n{\n  res1: {\n    text: \"You said your name was Jim.\"\n  }\n}\n*/","metadata":{"source":"examples/src/memory/planetscale.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":38}}}}],["0ed827e8-62be-4e36-b5b6-bfbdf02113a5",{"pageContent":"import { BufferMemory } from \"langchain/memory\";\nimport { PlanetScaleChatMessageHistory } from \"langchain/stores/message/planetscale\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { ConversationChain } from \"langchain/chains\";\nimport { Client } from \"@planetscale/database\";\n\n// Create your own Planetscale database client\nconst client = new Client({\n  url: \"ADD_YOURS_HERE\", // Override with your own database instance's URL\n});\n\nconst memory = new BufferMemory({\n  chatHistory: new PlanetScaleChatMessageHistory({\n    tableName: \"stored_message\",\n    sessionId: \"lc-example\",\n    client, // You can reuse your existing database client\n  }),\n});\n\nconst model = new ChatOpenAI();\nconst chain = new ConversationChain({ llm: model, memory });\n\nconst res1 = await chain.call({ input: \"Hi! I'm Jim.\" });\nconsole.log({ res1 });\n/*\n{\n  res1: {\n    text: \"Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?\"\n  }\n}\n*/\n\nconst res2 = await chain.call({ input: \"What did I just say my name was?\" });\nconsole.log({ res2 });\n\n/*\n{\n  res1: {\n    text: \"You said your name was Jim.\"\n  }\n}\n*/","metadata":{"source":"examples/src/memory/planetscale_advanced.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":42}}}}],["8dcf07ff-390e-4741-83e1-ab86779d253f",{"pageContent":"import { Redis } from \"ioredis\";\nimport { BufferMemory } from \"langchain/memory\";\nimport { RedisChatMessageHistory } from \"langchain/stores/message/ioredis\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { ConversationChain } from \"langchain/chains\";\n\nconst client = new Redis(\"redis://localhost:6379\");\n\nconst memory = new BufferMemory({\n  chatHistory: new RedisChatMessageHistory({\n    sessionId: new Date().toISOString(),\n    sessionTTL: 300,\n    client,\n  }),\n});\n\nconst model = new ChatOpenAI({\n  modelName: \"gpt-3.5-turbo\",\n  temperature: 0,\n});\n\nconst chain = new ConversationChain({ llm: model, memory });\n\nconst res1 = await chain.call({ input: \"Hi! I'm Jim.\" });\nconsole.log({ res1 });\n/*\n{\n  res1: {\n    text: \"Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?\"\n  }\n}\n*/\n\nconst res2 = await chain.call({ input: \"What did I just say my name was?\" });\nconsole.log({ res2 });\n\n/*\n{\n  res1: {\n    text: \"You said your name was Jim.\"\n  }\n}\n*/","metadata":{"source":"examples/src/memory/redis-advanced.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":43}}}}],["78940d70-c7ce-43e6-8538-7a73fb3860e7",{"pageContent":"import { Redis } from \"ioredis\";\nimport { BufferMemory } from \"langchain/memory\";\nimport { RedisChatMessageHistory } from \"langchain/stores/message/ioredis\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { ConversationChain } from \"langchain/chains\";\n\n// Uses ioredis to facilitate Sentinel Connections see their docs for details on setting up more complex Sentinels: https://github.com/redis/ioredis#sentinel\nconst client = new Redis({\n  sentinels: [\n    { host: \"localhost\", port: 26379 },\n    { host: \"localhost\", port: 26380 },\n  ],\n  name: \"mymaster\",\n});\n\nconst memory = new BufferMemory({\n  chatHistory: new RedisChatMessageHistory({\n    sessionId: new Date().toISOString(),\n    sessionTTL: 300,\n    client,\n  }),\n});\n\nconst model = new ChatOpenAI({ temperature: 0.5 });\n\nconst chain = new ConversationChain({ llm: model, memory });\n\nconst res1 = await chain.call({ input: \"Hi! I'm Jim.\" });\nconsole.log({ res1 });\n/*\n{\n  res1: {\n    text: \"Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?\"\n  }\n}\n*/\n\nconst res2 = await chain.call({ input: \"What did I just say my name was?\" });\nconsole.log({ res2 });\n\n/*\n{\n  res1: {\n    text: \"You said your name was Jim.\"\n  }\n}\n*/","metadata":{"source":"examples/src/memory/redis-sentinel.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":47}}}}],["dc6d8f8a-6229-4b7e-8f84-5322c0d11ea2",{"pageContent":"import { BufferMemory } from \"langchain/memory\";\nimport { RedisChatMessageHistory } from \"langchain/stores/message/ioredis\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { ConversationChain } from \"langchain/chains\";\n\nconst memory = new BufferMemory({\n  chatHistory: new RedisChatMessageHistory({\n    sessionId: new Date().toISOString(), // Or some other unique identifier for the conversation\n    sessionTTL: 300, // 5 minutes, omit this parameter to make sessions never expire\n    url: \"redis://localhost:6379\", // Default value, override with your own instance's URL\n  }),\n});\n\nconst model = new ChatOpenAI({\n  modelName: \"gpt-3.5-turbo\",\n  temperature: 0,\n});\n\nconst chain = new ConversationChain({ llm: model, memory });\n\nconst res1 = await chain.call({ input: \"Hi! I'm Jim.\" });\nconsole.log({ res1 });\n/*\n{\n  res1: {\n    text: \"Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?\"\n  }\n}\n*/\n\nconst res2 = await chain.call({ input: \"What did I just say my name was?\" });\nconsole.log({ res2 });\n\n/*\n{\n  res1: {\n    text: \"You said your name was Jim.\"\n  }\n}\n*/","metadata":{"source":"examples/src/memory/redis.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":40}}}}],["60292f3d-fef3-4589-ba8e-b12bab676868",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { ConversationSummaryBufferMemory } from \"langchain/memory\";\nimport { ConversationChain } from \"langchain/chains\";\nimport {\n  ChatPromptTemplate,\n  HumanMessagePromptTemplate,\n  MessagesPlaceholder,\n  SystemMessagePromptTemplate,\n} from \"langchain/prompts\";\n\n// summary buffer memory\nconst memory = new ConversationSummaryBufferMemory({\n  llm: new OpenAI({ modelName: \"text-davinci-003\", temperature: 0 }),\n  maxTokenLimit: 10,\n});\n\nawait memory.saveContext({ input: \"hi\" }, { output: \"whats up\" });\nawait memory.saveContext({ input: \"not much you\" }, { output: \"not much\" });\nconst history = await memory.loadMemoryVariables({});\nconsole.log({ history });\n/*\n  {\n    history: {\n      history: 'System: \\n' +\n        'The human greets the AI, to which the AI responds.\\n' +\n        'Human: not much you\\n' +\n        'AI: not much'\n    }\n  }\n*/\n\n// We can also get the history as a list of messages (this is useful if you are using this with a chat prompt).\nconst chatPromptMemory = new ConversationSummaryBufferMemory({\n  llm: new ChatOpenAI({ modelName: \"gpt-3.5-turbo\", temperature: 0 }),\n  maxTokenLimit: 10,\n  returnMessages: true,\n});\nawait chatPromptMemory.saveContext({ input: \"hi\" }, { output: \"whats up\" });\nawait chatPromptMemory.saveContext(\n  { input: \"not much you\" },\n  { output: \"not much\" }\n);\n\n// We can also utilize the predict_new_summary method directly.\nconst messages = await chatPromptMemory.chatHistory.getMessages();\nconst previous_summary = \"\";\nconst predictSummary = await chatPromptMemory.predictNewSummary(\n  messages,\n  previous_summary\n);\nconsole.log(JSON.stringify(predictSummary));\n\n// Using in a chain\n// Let's walk through an example, again setting verbose to true so we can see the prompt.\nconst chatPrompt = ChatPromptTemplate.fromMessages([\n  SystemMessagePromptTemplate.fromTemplate(\n    \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\"\n  ),\n  new MessagesPlaceholder(\"history\"),\n  HumanMessagePromptTemplate.fromTemplate(\"{input}\"),\n]);\n\nconst model = new ChatOpenAI({ temperature: 0.9, verbose: true });\nconst chain = new ConversationChain({\n  llm: model,\n  memory: chatPromptMemory,\n  prompt: chatPrompt,\n});\n\nconst res1 = await chain.predict({ input: \"Hi, what's up?\" });\nconsole.log({ res1 });\n/*\n  {\n    res1: 'Hello! I am an AI language model, always ready to have a conversation. How can I assist you today?'\n  }\n*/\n\nconst res2 = await chain.predict({\n  input: \"Just working on writing some documentation!\",\n});\nconsole.log({ res2 });\n/*\n  {\n    res2: \"That sounds productive! Documentation is an important aspect of many projects. Is there anything specific you need assistance with regarding your documentation? I'm here to help!\"\n  }\n*/\n\nconst res3 = await chain.predict({\n  input: \"For LangChain! Have you heard of it?\",\n});\nconsole.log({ res3 });\n/*\n  {\n    res3: 'Yes, I am familiar with LangChain! It is a blockchain-based language learning platform that aims to connect language learners with native speakers for real-time practice and feedback. It utilizes smart contracts to facilitate secure transactions and incentivize participation. Users can earn tokens by providing language learning services or consuming them for language lessons.'\n  }\n*/\n\nconst res4 = await chain.predict({\n  input:\n    \"That's not the right one, although a lot of people confuse it for that!\",\n});\nconsole.log({ res4 });\n\n/*\n  {\n    res4: \"I apologize for the confusion! Could you please provide some more information about the LangChain you're referring to? That way, I can better understand and assist you with writing documentation for it.\"\n  }\n*/","metadata":{"source":"examples/src/memory/summary_buffer.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":109}}}}],["9bb1b44b-d6b9-4232-9bec-008707744c7c",{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { ConversationSummaryMemory } from \"langchain/memory\";\nimport { LLMChain } from \"langchain/chains\";\nimport { PromptTemplate } from \"langchain/prompts\";\n\nexport const run = async () => {\n  const memory = new ConversationSummaryMemory({\n    memoryKey: \"chat_history\",\n    llm: new ChatOpenAI({ modelName: \"gpt-3.5-turbo\", temperature: 0 }),\n  });\n\n  const model = new ChatOpenAI();\n  const prompt =\n    PromptTemplate.fromTemplate(`The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\n  Current conversation:\n  {chat_history}\n  Human: {input}\n  AI:`);\n  const chain = new LLMChain({ llm: model, prompt, memory });\n\n  const res1 = await chain.call({ input: \"Hi! I'm Jim.\" });\n  console.log({ res1, memory: await memory.loadMemoryVariables({}) });\n  /*\n  {\n    res1: {\n      text: \"Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?\"\n    },\n    memory: {\n      chat_history: 'Jim introduces himself to the AI and the AI greets him and offers assistance.'\n    }\n  }\n  */\n\n  const res2 = await chain.call({ input: \"What's my name?\" });\n  console.log({ res2, memory: await memory.loadMemoryVariables({}) });\n  /*\n  {\n    res2: {\n      text: \"Your name is Jim. It's nice to meet you, Jim. How can I assist you today?\"\n    },\n    memory: {\n      chat_history: 'Jim introduces himself to the AI and the AI greets him and offers assistance. The AI addresses Jim by name and asks how it can assist him.'\n    }\n  }\n  */\n};","metadata":{"source":"examples/src/memory/summary_chat.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":47}}}}],["f248fdf5-9435-4059-8ab5-f75596745aca",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { ConversationSummaryMemory } from \"langchain/memory\";\nimport { LLMChain } from \"langchain/chains\";\nimport { PromptTemplate } from \"langchain/prompts\";\n\nexport const run = async () => {\n  const memory = new ConversationSummaryMemory({\n    memoryKey: \"chat_history\",\n    llm: new OpenAI({ modelName: \"gpt-3.5-turbo\", temperature: 0 }),\n  });\n\n  const model = new OpenAI({ temperature: 0.9 });\n  const prompt =\n    PromptTemplate.fromTemplate(`The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\n  Current conversation:\n  {chat_history}\n  Human: {input}\n  AI:`);\n  const chain = new LLMChain({ llm: model, prompt, memory });\n\n  const res1 = await chain.call({ input: \"Hi! I'm Jim.\" });\n  console.log({ res1, memory: await memory.loadMemoryVariables({}) });\n  /*\n  {\n    res1: {\n      text: \" Hi Jim, I'm AI! It's nice to meet you. I'm an AI programmed to provide information about the environment around me. Do you have any specific questions about the area that I can answer for you?\"\n    },\n    memory: {\n      chat_history: 'Jim introduces himself to the AI and the AI responds, introducing itself as a program designed to provide information about the environment. The AI offers to answer any specific questions Jim may have about the area.'\n    }\n  }\n  */\n\n  const res2 = await chain.call({ input: \"What's my name?\" });\n  console.log({ res2, memory: await memory.loadMemoryVariables({}) });\n  /*\n  {\n    res2: { text: ' You told me your name is Jim.' },\n    memory: {\n      chat_history: 'Jim introduces himself to the AI and the AI responds, introducing itself as a program designed to provide information about the environment. The AI offers to answer any specific questions Jim may have about the area. Jim asks the AI what his name is, and the AI responds that Jim had previously told it his name.'\n    }\n  }\n  */\n};","metadata":{"source":"examples/src/memory/summary_llm.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":45}}}}],["df682c64-2b4c-404c-a7b7-4de8943506e5",{"pageContent":"import { BufferMemory } from \"langchain/memory\";\nimport { UpstashRedisChatMessageHistory } from \"langchain/stores/message/upstash_redis\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { ConversationChain } from \"langchain/chains\";\n\nconst memory = new BufferMemory({\n  chatHistory: new UpstashRedisChatMessageHistory({\n    sessionId: new Date().toISOString(), // Or some other unique identifier for the conversation\n    sessionTTL: 300, // 5 minutes, omit this parameter to make sessions never expire\n    config: {\n      url: \"https://ADD_YOURS_HERE.upstash.io\", // Override with your own instance's URL\n      token: \"********\", // Override with your own instance's token\n    },\n  }),\n});\n\nconst model = new ChatOpenAI({\n  modelName: \"gpt-3.5-turbo\",\n  temperature: 0,\n});\n\nconst chain = new ConversationChain({ llm: model, memory });\n\nconst res1 = await chain.call({ input: \"Hi! I'm Jim.\" });\nconsole.log({ res1 });\n/*\n{\n  res1: {\n    text: \"Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?\"\n  }\n}\n*/\n\nconst res2 = await chain.call({ input: \"What did I just say my name was?\" });\nconsole.log({ res2 });\n\n/*\n{\n  res1: {\n    text: \"You said your name was Jim.\"\n  }\n}\n*/","metadata":{"source":"examples/src/memory/upstash_redis.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":43}}}}],["3f7ef881-b049-4c57-80f2-81d320795d72",{"pageContent":"import { Redis } from \"@upstash/redis\";\nimport { BufferMemory } from \"langchain/memory\";\nimport { UpstashRedisChatMessageHistory } from \"langchain/stores/message/upstash_redis\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { ConversationChain } from \"langchain/chains\";\n\n// Create your own Redis client\nconst client = new Redis({\n  url: \"https://ADD_YOURS_HERE.upstash.io\",\n  token: \"********\",\n});\n\nconst memory = new BufferMemory({\n  chatHistory: new UpstashRedisChatMessageHistory({\n    sessionId: new Date().toISOString(),\n    sessionTTL: 300,\n    client, // You can reuse your existing Redis client\n  }),\n});\n\nconst model = new ChatOpenAI({\n  modelName: \"gpt-3.5-turbo\",\n  temperature: 0,\n});\n\nconst chain = new ConversationChain({ llm: model, memory });\n\nconst res1 = await chain.call({ input: \"Hi! I'm Jim.\" });\nconsole.log({ res1 });\n/*\n{\n  res1: {\n    text: \"Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?\"\n  }\n}\n*/\n\nconst res2 = await chain.call({ input: \"What did I just say my name was?\" });\nconsole.log({ res2 });\n\n/*\n{\n  res1: {\n    text: \"You said your name was Jim.\"\n  }\n}\n*/","metadata":{"source":"examples/src/memory/upstash_redis_advanced.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":47}}}}],["a1997a8a-65c0-47b2-beb6-9f7305ee6a7e",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { VectorStoreRetrieverMemory } from \"langchain/memory\";\nimport { LLMChain } from \"langchain/chains\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nconst vectorStore = new MemoryVectorStore(new OpenAIEmbeddings());\nconst memory = new VectorStoreRetrieverMemory({\n  // 1 is how many documents to return, you might want to return more, eg. 4\n  vectorStoreRetriever: vectorStore.asRetriever(1),\n  memoryKey: \"history\",\n});\n\n// First let's save some information to memory, as it would happen when\n// used inside a chain.\nawait memory.saveContext(\n  { input: \"My favorite food is pizza\" },\n  { output: \"thats good to know\" }\n);\nawait memory.saveContext(\n  { input: \"My favorite sport is soccer\" },\n  { output: \"...\" }\n);\nawait memory.saveContext({ input: \"I don't the Celtics\" }, { output: \"ok\" });\n\n// Now let's use the memory to retrieve the information we saved.\nconsole.log(\n  await memory.loadMemoryVariables({ prompt: \"what sport should i watch?\" })\n);\n/*\n{ history: 'input: My favorite sport is soccer\\noutput: ...' }\n*/\n\n// Now let's use it in a chain.\nconst model = new OpenAI({ temperature: 0.9 });\nconst prompt =\n  PromptTemplate.fromTemplate(`The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\nRelevant pieces of previous conversation:\n{history}\n\n(You do not need to use these pieces of information if not relevant)\n\nCurrent conversation:\nHuman: {input}\nAI:`);\nconst chain = new LLMChain({ llm: model, prompt, memory });\n\nconst res1 = await chain.call({ input: \"Hi, my name is Perry, what's up?\" });\nconsole.log({ res1 });\n/*\n{\n  res1: {\n    text: \" Hi Perry, I'm doing great! I'm currently exploring different topics related to artificial intelligence like natural language processing and machine learning. What about you? What have you been up to lately?\"\n  }\n}\n*/\n\nconst res2 = await chain.call({ input: \"what's my favorite sport?\" });\nconsole.log({ res2 });\n/*\n{ res2: { text: ' You said your favorite sport is soccer.' } }\n*/\n\nconst res3 = await chain.call({ input: \"what's my name?\" });\nconsole.log({ res3 });\n/*\n{ res3: { text: ' Your name is Perry.' } }\n*/","metadata":{"source":"examples/src/memory/vector_store.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":70}}}}],["faaad1e7-a9d5-4cbc-a010-6109c3b9afaa",{"pageContent":"import { BufferMemory } from \"langchain/memory\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { ConversationChain } from \"langchain/chains\";\nimport { XataChatMessageHistory } from \"langchain/stores/message/xata\";\nimport { BaseClient } from \"@xata.io/client\";\n\n// Before running this example, see the docs at\n// https://js.langchain.com/docs/modules/memory/integrations/xata\n\n// if you use the generated client, you don't need this function.\n// Just import getXataClient from the generated xata.ts instead.\nconst getXataClient = () => {\n  if (!process.env.XATA_API_KEY) {\n    throw new Error(\"XATA_API_KEY not set\");\n  }\n\n  if (!process.env.XATA_DB_URL) {\n    throw new Error(\"XATA_DB_URL not set\");\n  }\n  const xata = new BaseClient({\n    databaseURL: process.env.XATA_DB_URL,\n    apiKey: process.env.XATA_API_KEY,\n    branch: process.env.XATA_BRANCH || \"main\",\n  });\n  return xata;\n};\n\nconst memory = new BufferMemory({\n  chatHistory: new XataChatMessageHistory({\n    table: \"messages\",\n    sessionId: new Date().toISOString(), // Or some other unique identifier for the conversation\n    client: getXataClient(),\n    createTable: false, // Explicitly set to false if the table is already created\n  }),\n});\n\nconst model = new ChatOpenAI();\nconst chain = new ConversationChain({ llm: model, memory });\n\nconst res1 = await chain.call({ input: \"Hi! I'm Jim.\" });\nconsole.log({ res1 });\n/*\n{\n  res1: {\n    text: \"Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?\"\n  }\n}\n*/\n\nconst res2 = await chain.call({ input: \"What did I just say my name was?\" });\nconsole.log({ res2 });\n\n/*\n{\n  res1: {\n    text: \"You said your name was Jim.\"\n  }\n}\n*/","metadata":{"source":"examples/src/memory/xata-advanced.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":59}}}}],["6fc03d1e-bfb9-456e-97e9-9616fb480c8c",{"pageContent":"import { BufferMemory } from \"langchain/memory\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { ConversationChain } from \"langchain/chains\";\nimport { XataChatMessageHistory } from \"langchain/stores/message/xata\";\nimport { BaseClient } from \"@xata.io/client\";\n\n// if you use the generated client, you don't need this function.\n// Just import getXataClient from the generated xata.ts instead.\nconst getXataClient = () => {\n  if (!process.env.XATA_API_KEY) {\n    throw new Error(\"XATA_API_KEY not set\");\n  }\n\n  if (!process.env.XATA_DB_URL) {\n    throw new Error(\"XATA_DB_URL not set\");\n  }\n  const xata = new BaseClient({\n    databaseURL: process.env.XATA_DB_URL,\n    apiKey: process.env.XATA_API_KEY,\n    branch: process.env.XATA_BRANCH || \"main\",\n  });\n  return xata;\n};\n\nconst memory = new BufferMemory({\n  chatHistory: new XataChatMessageHistory({\n    table: \"messages\",\n    sessionId: new Date().toISOString(), // Or some other unique identifier for the conversation\n    client: getXataClient(),\n    apiKey: process.env.XATA_API_KEY, // The API key is needed for creating the table.\n  }),\n});\n\nconst model = new ChatOpenAI();\nconst chain = new ConversationChain({ llm: model, memory });\n\nconst res1 = await chain.call({ input: \"Hi! I'm Jim.\" });\nconsole.log({ res1 });\n/*\n{\n  res1: {\n    text: \"Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?\"\n  }\n}\n*/\n\nconst res2 = await chain.call({ input: \"What did I just say my name was?\" });\nconsole.log({ res2 });\n\n/*\n{\n  res1: {\n    text: \"You said your name was Jim.\"\n  }\n}\n*/","metadata":{"source":"examples/src/memory/xata.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":56}}}}],["0b60c100-8b4b-4c67-8809-34abb3f6dff6",{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { ConversationChain } from \"langchain/chains\";\nimport { ZepMemory } from \"langchain/memory/zep\";\nimport { randomUUID } from \"crypto\";\n\nconst sessionId = randomUUID(); // This should be unique for each user or each user's session.\nconst zepURL = \"http://localhost:8000\";\n\nconst memory = new ZepMemory({\n  sessionId,\n  baseURL: zepURL,\n  // This is optional. If you've enabled JWT authentication on your Zep server, you can\n  // pass it in here. See https://docs.getzep.com/deployment/auth\n  apiKey: \"change_this_key\",\n});\n\nconst model = new ChatOpenAI({\n  modelName: \"gpt-3.5-turbo\",\n  temperature: 0,\n});\n\nconst chain = new ConversationChain({ llm: model, memory });\nconsole.log(\"Memory Keys:\", memory.memoryKeys);\n\nconst res1 = await chain.call({ input: \"Hi! I'm Jim.\" });\nconsole.log({ res1 });\n/*\n{\n  res1: {\n    text: \"Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?\"\n  }\n}\n*/\n\nconst res2 = await chain.call({ input: \"What did I just say my name was?\" });\nconsole.log({ res2 });\n\n/*\n{\n  res1: {\n    text: \"You said your name was Jim.\"\n  }\n}\n*/\nconsole.log(\"Session ID: \", sessionId);\nconsole.log(\"Memory: \", await memory.loadMemoryVariables({}));","metadata":{"source":"examples/src/memory/zep.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":46}}}}],["47b7cd89-7f69-4c3d-a385-465d86798616",{"pageContent":"\"use node\";\n\nimport { v } from \"convex/values\";\nimport { BufferMemory } from \"langchain/memory\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { ConversationChain } from \"langchain/chains\";\nimport { ConvexChatMessageHistory } from \"langchain/stores/message/convex\";\nimport { action } from \"./_generated/server.js\";\n\nexport const ask = action({\n  args: { sessionId: v.string() },\n  handler: async (ctx, args) => {\n    // pass in a sessionId string\n    const { sessionId } = args;\n\n    const memory = new BufferMemory({\n      chatHistory: new ConvexChatMessageHistory({ sessionId, ctx }),\n    });\n\n    const model = new ChatOpenAI({\n      modelName: \"gpt-3.5-turbo\",\n      temperature: 0,\n    });\n\n    const chain = new ConversationChain({ llm: model, memory });\n\n    const res1 = await chain.call({ input: \"Hi! I'm Jim.\" });\n    console.log({ res1 });\n    /*\n      {\n        res1: {\n          text: \"Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?\"\n        }\n      }\n    */\n\n    const res2 = await chain.call({\n      input: \"What did I just say my name was?\",\n    });\n    console.log({ res2 });\n\n    /*\n      {\n        res2: {\n          text: \"You said your name was Jim.\"\n        }\n      }\n    */\n\n    // See the chat history in the Convex database\n    console.log(await memory.chatHistory.getMessages());\n\n    // clear chat history\n    await memory.chatHistory.clear();\n  },\n});","metadata":{"source":"examples/src/memory/convex/convex.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":56}}}}],["7a394032-f855-42ba-9e89-bc0dd2348765",{"pageContent":"/* eslint-disable */\n/**\n * Generated `api` utility.\n *\n * THIS CODE IS AUTOMATICALLY GENERATED.\n *\n * Generated by convex@1.3.1.\n * To regenerate, run `npx convex dev`.\n * @module\n */\n\nimport type {\n  ApiFromModules,\n  FilterApi,\n  FunctionReference,\n} from \"convex/server\";\n\n/**\n * A utility for referencing Convex functions in your app's API.\n *\n * Usage:\n * ```js\n * const myFunctionReference = api.myModule.myFunction;\n * ```\n */\ndeclare const fullApi: ApiFromModules<{}>;\nexport declare const api: FilterApi<\n  typeof fullApi,\n  FunctionReference<any, \"public\">\n>;\nexport declare const internal: FilterApi<\n  typeof fullApi,\n  FunctionReference<any, \"internal\">\n>;","metadata":{"source":"examples/src/memory/convex/_generated/api.d.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":34}}}}],["901abc08-4286-4878-801e-2870447a4771",{"pageContent":"/* eslint-disable */\n/**\n * Generated `api` utility.\n *\n * THIS CODE IS AUTOMATICALLY GENERATED.\n *\n * Generated by convex@1.3.1.\n * To regenerate, run `npx convex dev`.\n * @module\n */\n\nimport { anyApi } from \"convex/server\";\n\n/**\n * A utility for referencing Convex functions in your app's API.\n *\n * Usage:\n * ```js\n * const myFunctionReference = api.myModule.myFunction;\n * ```\n */\nexport const api = anyApi;\nexport const internal = anyApi;","metadata":{"source":"examples/src/memory/convex/_generated/api.js","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":23}}}}],["e13c1e5e-815e-4a67-8649-baacbf593f9f",{"pageContent":"/* eslint-disable */\n/**\n * Generated data model types.\n *\n * THIS CODE IS AUTOMATICALLY GENERATED.\n *\n * Generated by convex@1.3.1.\n * To regenerate, run `npx convex dev`.\n * @module\n */\n\nimport { AnyDataModel } from \"convex/server\";\nimport type { GenericId } from \"convex/values\";\n\n/**\n * No `schema.ts` file found!\n *\n * This generated code has permissive types like `Doc = any` because\n * Convex doesn't know your schema. If you'd like more type safety, see\n * https://docs.convex.dev/using/schemas for instructions on how to add a\n * schema file.\n *\n * After you change a schema, rerun codegen with `npx convex dev`.\n */\n\n/**\n * The names of all of your Convex tables.\n */\nexport type TableNames = string;\n\n/**\n * The type of a document stored in Convex.\n */\nexport type Doc = any;\n\n/**\n * An identifier for a document in Convex.\n *\n * Convex documents are uniquely identified by their `Id`, which is accessible\n * on the `_id` field. To learn more, see [Document IDs](https://docs.convex.dev/using/document-ids).\n *\n * Documents can be loaded using `db.get(id)` in query and mutation functions.\n *\n * IDs are just strings at runtime, but this type can be used to distinguish them from other\n * strings when type checking.\n */\nexport type Id<TableName extends TableNames = TableNames> =\n  GenericId<TableName>;\n\n/**\n * A type describing your Convex data model.\n *\n * This type includes information about what tables you have, the type of\n * documents stored in those tables, and the indexes defined on them.\n *\n * This type is used to parameterize methods like `queryGeneric` and\n * `mutationGeneric` to make them type-safe.\n */\nexport type DataModel = AnyDataModel;","metadata":{"source":"examples/src/memory/convex/_generated/dataModel.d.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":59}}}}],["0f679b97-e83a-4d6b-88d5-480566e2466b",{"pageContent":"/* eslint-disable */\n/**\n * Generated utilities for implementing server-side Convex query and mutation functions.\n *\n * THIS CODE IS AUTOMATICALLY GENERATED.\n *\n * Generated by convex@1.3.1.\n * To regenerate, run `npx convex dev`.\n * @module\n */\n\nimport {\n  ActionBuilder,\n  HttpActionBuilder,\n  MutationBuilder,\n  QueryBuilder,\n  GenericActionCtx,\n  GenericMutationCtx,\n  GenericQueryCtx,\n  GenericDatabaseReader,\n  GenericDatabaseWriter,\n} from \"convex/server\";\nimport type { DataModel } from \"./dataModel.js\";\n\n/**\n * Define a query in this Convex app's public API.\n *\n * This function will be allowed to read your Convex database and will be accessible from the client.\n *\n * @param func - The query function. It receives a {@link QueryCtx} as its first argument.\n * @returns The wrapped query. Include this as an `export` to name it and make it accessible.\n */\nexport declare const query: QueryBuilder<DataModel, \"public\">;\n\n/**\n * Define a query that is only accessible from other Convex functions (but not from the client).\n *\n * This function will be allowed to read from your Convex database. It will not be accessible from the client.\n *\n * @param func - The query function. It receives a {@link QueryCtx} as its first argument.\n * @returns The wrapped query. Include this as an `export` to name it and make it accessible.\n */\nexport declare const internalQuery: QueryBuilder<DataModel, \"internal\">;\n\n/**\n * Define a mutation in this Convex app's public API.\n *\n * This function will be allowed to modify your Convex database and will be accessible from the client.\n *\n * @param func - The mutation function. It receives a {@link MutationCtx} as its first argument.\n * @returns The wrapped mutation. Include this as an `export` to name it and make it accessible.\n */\nexport declare const mutation: MutationBuilder<DataModel, \"public\">;\n\n/**\n * Define a mutation that is only accessible from other Convex functions (but not from the client).\n *\n * This function will be allowed to modify your Convex database. It will not be accessible from the client.\n *\n * @param func - The mutation function. It receives a {@link MutationCtx} as its first argument.\n * @returns The wrapped mutation. Include this as an `export` to name it and make it accessible.\n */\nexport declare const internalMutation: MutationBuilder<DataModel, \"internal\">;\n\n/**\n * Define an action in this Convex app's public API.\n *\n * An action is a function which can execute any JavaScript code, including non-deterministic\n * code and code with side-effects, like calling third-party services.\n * They can be run in Convex's JavaScript environment or in Node.js using the \"use node\" directive.\n * They can interact with the database indirectly by calling queries and mutations using the {@link ActionCtx}.\n *\n * @param func - The action. It receives an {@link ActionCtx} as its first argument.\n * @returns The wrapped action. Include this as an `export` to name it and make it accessible.\n */\nexport declare const action: ActionBuilder<DataModel, \"public\">;\n\n/**\n * Define an action that is only accessible from other Convex functions (but not from the client).\n *\n * @param func - The function. It receives an {@link ActionCtx} as its first argument.\n * @returns The wrapped function. Include this as an `export` to name it and make it accessible.\n */\nexport declare const internalAction: ActionBuilder<DataModel, \"internal\">;\n\n/**\n * Define an HTTP action.\n *\n * This function will be used to respond to HTTP requests received by a Convex\n * deployment if the requests matches the path and method where this action\n * is routed. Be sure to route your action in `convex/http.js`.\n *\n * @param func - The function. It receives an {@link ActionCtx} as its first argument.\n * @returns The wrapped function. Import this function from `convex/http.js` and route it to hook it up.\n */\nexport declare const httpAction: HttpActionBuilder;","metadata":{"source":"examples/src/memory/convex/_generated/server.d.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":96}}}}],["115586bf-897a-40a0-ba8a-d18e4241b4f5",{"pageContent":"/**\n * A set of services for use within Convex query functions.\n *\n * The query context is passed as the first argument to any Convex query\n * function run on the server.\n *\n * This differs from the {@link MutationCtx} because all of the services are\n * read-only.\n */\nexport type QueryCtx = GenericQueryCtx<DataModel>;\n\n/**\n * A set of services for use within Convex mutation functions.\n *\n * The mutation context is passed as the first argument to any Convex mutation\n * function run on the server.\n */\nexport type MutationCtx = GenericMutationCtx<DataModel>;\n\n/**\n * A set of services for use within Convex action functions.\n *\n * The action context is passed as the first argument to any Convex action\n * function run on the server.\n */\nexport type ActionCtx = GenericActionCtx<DataModel>;\n\n/**\n * An interface to read from the database within Convex query functions.\n *\n * The two entry points are {@link DatabaseReader.get}, which fetches a single\n * document by its {@link Id}, or {@link DatabaseReader.query}, which starts\n * building a query.\n */\nexport type DatabaseReader = GenericDatabaseReader<DataModel>;\n\n/**\n * An interface to read from and write to the database within Convex mutation\n * functions.\n *\n * Convex guarantees that all writes within a single mutation are\n * executed atomically, so you never have to worry about partial writes leaving\n * your data in an inconsistent state. See [the Convex Guide](https://docs.convex.dev/understanding/convex-fundamentals/functions#atomicity-and-optimistic-concurrency-control)\n * for the guarantees Convex provides your functions.\n */\nexport type DatabaseWriter = GenericDatabaseWriter<DataModel>;","metadata":{"source":"examples/src/memory/convex/_generated/server.d.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":98,"to":143}}}}],["c211ef52-052b-4095-ba16-0d1ab751a494",{"pageContent":"/* eslint-disable */\n/**\n * Generated utilities for implementing server-side Convex query and mutation functions.\n *\n * THIS CODE IS AUTOMATICALLY GENERATED.\n *\n * Generated by convex@1.3.1.\n * To regenerate, run `npx convex dev`.\n * @module\n */\n\nimport {\n  actionGeneric,\n  httpActionGeneric,\n  queryGeneric,\n  mutationGeneric,\n  internalActionGeneric,\n  internalMutationGeneric,\n  internalQueryGeneric,\n} from \"convex/server\";\n\n/**\n * Define a query in this Convex app's public API.\n *\n * This function will be allowed to read your Convex database and will be accessible from the client.\n *\n * @param func - The query function. It receives a {@link QueryCtx} as its first argument.\n * @returns The wrapped query. Include this as an `export` to name it and make it accessible.\n */\nexport const query = queryGeneric;\n\n/**\n * Define a query that is only accessible from other Convex functions (but not from the client).\n *\n * This function will be allowed to read from your Convex database. It will not be accessible from the client.\n *\n * @param func - The query function. It receives a {@link QueryCtx} as its first argument.\n * @returns The wrapped query. Include this as an `export` to name it and make it accessible.\n */\nexport const internalQuery = internalQueryGeneric;\n\n/**\n * Define a mutation in this Convex app's public API.\n *\n * This function will be allowed to modify your Convex database and will be accessible from the client.\n *\n * @param func - The mutation function. It receives a {@link MutationCtx} as its first argument.\n * @returns The wrapped mutation. Include this as an `export` to name it and make it accessible.\n */\nexport const mutation = mutationGeneric;\n\n/**\n * Define a mutation that is only accessible from other Convex functions (but not from the client).\n *\n * This function will be allowed to modify your Convex database. It will not be accessible from the client.\n *\n * @param func - The mutation function. It receives a {@link MutationCtx} as its first argument.\n * @returns The wrapped mutation. Include this as an `export` to name it and make it accessible.\n */\nexport const internalMutation = internalMutationGeneric;\n\n/**\n * Define an action in this Convex app's public API.\n *\n * An action is a function which can execute any JavaScript code, including non-deterministic\n * code and code with side-effects, like calling third-party services.\n * They can be run in Convex's JavaScript environment or in Node.js using the \"use node\" directive.\n * They can interact with the database indirectly by calling queries and mutations using the {@link ActionCtx}.\n *\n * @param func - The action. It receives an {@link ActionCtx} as its first argument.\n * @returns The wrapped action. Include this as an `export` to name it and make it accessible.\n */\nexport const action = actionGeneric;\n\n/**\n * Define an action that is only accessible from other Convex functions (but not from the client).\n *\n * @param func - The function. It receives an {@link ActionCtx} as its first argument.\n * @returns The wrapped function. Include this as an `export` to name it and make it accessible.\n */\nexport const internalAction = internalActionGeneric;\n\n/**\n * Define a Convex HTTP action.\n *\n * @param func - The function. It receives an {@link ActionCtx} as its first argument, and a `Request` object\n * as its second.\n * @returns The wrapped endpoint function. Route a URL path to this function in `convex/http.js`.\n */\nexport const httpAction = httpActionGeneric;","metadata":{"source":"examples/src/memory/convex/_generated/server.js","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":90}}}}],["9a00a0c8-eb1a-4818-bb14-bd1856f5e410",{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { HumanMessage, SystemMessage } from \"langchain/schema\";\n\nexport const run = async () => {\n  const chat = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" });\n  // Pass in a list of messages to `call` to start a conversation. In this simple example, we only pass in one message.\n  const responseA = await chat.call([\n    new HumanMessage(\n      \"What is a good name for a company that makes colorful socks?\"\n    ),\n  ]);\n  console.log(responseA);\n  // AIMessage { text: '\\n\\nRainbow Sox Co.' }\n\n  // You can also pass in multiple messages to start a conversation.\n  // The first message is a system message that describes the context of the conversation.\n  // The second message is a human message that starts the conversation.\n  const responseB = await chat.call([\n    new SystemMessage(\n      \"You are a helpful assistant that translates English to French.\"\n    ),\n    new HumanMessage(\"Translate: I love programming.\"),\n  ]);\n  console.log(responseB);\n  // AIMessage { text: \"J'aime programmer.\" }\n\n  // Similar to LLMs, you can also use `generate` to generate chat completions for multiple sets of messages.\n  const responseC = await chat.generate([\n    [\n      new SystemMessage(\n        \"You are a helpful assistant that translates English to French.\"\n      ),\n      new HumanMessage(\n        \"Translate this sentence from English to French. I love programming.\"\n      ),\n    ],\n    [\n      new SystemMessage(\n        \"You are a helpful assistant that translates English to French.\"\n      ),\n      new HumanMessage(\n        \"Translate this sentence from English to French. I love artificial intelligence.\"\n      ),\n    ],\n  ]);\n  console.log(responseC);\n  /*\n  {\n    generations: [\n      [\n        {\n          text: \"J'aime programmer.\",\n          message: AIMessage { text: \"J'aime programmer.\" },\n        }\n      ],\n      [\n        {\n          text: \"J'aime l'intelligence artificielle.\",\n          message: AIMessage { text: \"J'aime l'intelligence artificielle.\" }\n        }\n      ]\n    ]\n  }\n  */\n};","metadata":{"source":"examples/src/models/chat/chat.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":65}}}}],["78ba7b6f-1724-4631-8747-0421ea1e1507",{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { HumanMessage } from \"langchain/schema\";\n\nconst model = new ChatOpenAI({ temperature: 1 });\nconst controller = new AbortController();\n\n// Call `controller.abort()` somewhere to cancel the request.\n\nconst res = await model.call(\n  [\n    new HumanMessage(\n      \"What is a good name for a company that makes colorful socks?\"\n    ),\n  ],\n  { signal: controller.signal }\n);\n\nconsole.log(res);\n/*\n'\\n\\nSocktastic Colors'\n*/","metadata":{"source":"examples/src/models/chat/chat_cancellation.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":21}}}}],["cef147f7-5e64-4a96-a345-70454035e25b",{"pageContent":"import { HumanMessage, LLMResult } from \"langchain/schema\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { Serialized } from \"langchain/load/serializable\";\n\n// We can pass in a list of CallbackHandlers to the LLM constructor to get callbacks for various events.\nconst model = new ChatOpenAI({\n  callbacks: [\n    {\n      handleLLMStart: async (llm: Serialized, prompts: string[]) => {\n        console.log(JSON.stringify(llm, null, 2));\n        console.log(JSON.stringify(prompts, null, 2));\n      },\n      handleLLMEnd: async (output: LLMResult) => {\n        console.log(JSON.stringify(output, null, 2));\n      },\n      handleLLMError: async (err: Error) => {\n        console.error(err);\n      },\n    },\n  ],\n});\n\nawait model.call([\n  new HumanMessage(\n    \"What is a good name for a company that makes colorful socks?\"\n  ),\n]);\n/*\n{\n  \"name\": \"openai\"\n}\n[\n  \"Human: What is a good name for a company that makes colorful socks?\"\n]\n{\n  \"generations\": [\n    [\n      {\n        \"text\": \"Rainbow Soles\",\n        \"message\": {\n          \"text\": \"Rainbow Soles\"\n        }\n      }\n    ]\n  ],\n  \"llmOutput\": {\n    \"tokenUsage\": {\n      \"completionTokens\": 4,\n      \"promptTokens\": 21,\n      \"totalTokens\": 25\n    }\n  }\n}\n*/","metadata":{"source":"examples/src/models/chat/chat_debugging.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":54}}}}],["30807eef-63c3-44ab-b8b2-60b49251a2cc",{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { HumanMessage } from \"langchain/schema\";\n\nconst chat = new ChatOpenAI({});\n// Pass in a list of messages to `call` to start a conversation. In this simple example, we only pass in one message.\nconst response = await chat.call([\n  new HumanMessage(\n    \"What is a good name for a company that makes colorful socks?\"\n  ),\n]);\nconsole.log(response);\n// AIMessage { text: '\\n\\nRainbow Sox Co.' }","metadata":{"source":"examples/src/models/chat/chat_quick_start.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":12}}}}],["46a439f0-9d47-4e47-ac85-a94ab530cc4d",{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { HumanMessage } from \"langchain/schema\";\n\nconst chat = new ChatOpenAI({\n  maxTokens: 25,\n  streaming: true,\n});\n\nconst response = await chat.call([new HumanMessage(\"Tell me a joke.\")], {\n  callbacks: [\n    {\n      handleLLMNewToken(token: string) {\n        console.log({ token });\n      },\n    },\n  ],\n});\n\nconsole.log(response);\n// { token: '' }\n// { token: '\\n\\n' }\n// { token: 'Why' }\n// { token: ' don' }\n// { token: \"'t\" }\n// { token: ' scientists' }\n// { token: ' trust' }\n// { token: ' atoms' }\n// { token: '?\\n\\n' }\n// { token: 'Because' }\n// { token: ' they' }\n// { token: ' make' }\n// { token: ' up' }\n// { token: ' everything' }\n// { token: '.' }\n// { token: '' }\n// AIMessage {\n//   text: \"\\n\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything.\"\n// }","metadata":{"source":"examples/src/models/chat/chat_streaming.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":38}}}}],["d18343c5-26aa-44a4-a8d1-0e7f20e4cb13",{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { HumanMessage } from \"langchain/schema\";\n\nconst chat = new ChatOpenAI({\n  streaming: true,\n  callbacks: [\n    {\n      handleLLMNewToken(token: string) {\n        process.stdout.write(token);\n      },\n    },\n  ],\n});\n\nawait chat.call([new HumanMessage(\"Write me a song about sparkling water.\")]);\n/*\nVerse 1:\nBubbles rise, crisp and clear\nRefreshing taste that brings us cheer\nSparkling water, so light and pure\nQuenches our thirst, it's always secure\n\nChorus:\nSparkling water, oh how we love\nIts fizzy bubbles and grace above\nIt's the perfect drink, anytime, anyplace\nRefreshing as it gives us a taste\n\nVerse 2:\nFrom morning brunch to evening feast\nIt's the perfect drink for a treat\nA sip of it brings a smile so bright\nOur thirst is quenched in just one sip so light\n...\n*/","metadata":{"source":"examples/src/models/chat/chat_streaming_stdout.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":35}}}}],["25677882-bd19-41c5-9afd-98406cd4598e",{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\n\nconst chat = new ChatOpenAI({\n  maxTokens: 25,\n});\n\n// Pass in a human message. Also accepts a raw string, which is automatically\n// inferred to be a human message.\nconst stream = await chat.stream([[\"human\", \"Tell me a joke about bears.\"]]);\n\nfor await (const chunk of stream) {\n  console.log(chunk);\n}\n/*\nAIMessageChunk {\n  content: '',\n  additional_kwargs: {}\n}\nAIMessageChunk {\n  content: 'Why',\n  additional_kwargs: {}\n}\nAIMessageChunk {\n  content: ' did',\n  additional_kwargs: {}\n}\nAIMessageChunk {\n  content: ' the',\n  additional_kwargs: {}\n}\nAIMessageChunk {\n  content: ' bear',\n  additional_kwargs: {}\n}\nAIMessageChunk {\n  content: ' bring',\n  additional_kwargs: {}\n}\nAIMessageChunk {\n  content: ' a',\n  additional_kwargs: {}\n}\n...\n*/","metadata":{"source":"examples/src/models/chat/chat_streaming_stream_method.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":44}}}}],["d2bd86e9-9770-4370-a5ae-da0f9a9a3fbf",{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { HumanMessage } from \"langchain/schema\";\n\nconst chat = new ChatOpenAI({ temperature: 1 });\n\nconst response = await chat.call(\n  [\n    new HumanMessage(\n      \"What is a good name for a company that makes colorful socks?\"\n    ),\n  ],\n  { timeout: 1000 } // 1s timeout\n);\nconsole.log(response);\n// AIMessage { text: '\\n\\nRainbow Sox Co.' }","metadata":{"source":"examples/src/models/chat/chat_timeout.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":15}}}}],["d2a41afb-cd88-4fb3-9dba-9a08bbe080f8",{"pageContent":"import { ChatAnthropic } from \"langchain/chat_models/anthropic\";\n\nconst model = new ChatAnthropic({\n  temperature: 0.9,\n  anthropicApiKey: \"YOUR-API-KEY\", // In Node.js defaults to process.env.ANTHROPIC_API_KEY\n});","metadata":{"source":"examples/src/models/chat/integration_anthropic.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":6}}}}],["b2f51566-f752-4e54-8754-21fa32fde8f2",{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\n\nconst model = new ChatOpenAI({\n  temperature: 0.9,\n  azureOpenAIApiKey: \"SOME_SECRET_VALUE\", // In Node.js defaults to process.env.AZURE_OPENAI_API_KEY\n  azureOpenAIApiVersion: \"YOUR-API-VERSION\", // In Node.js defaults to process.env.AZURE_OPENAI_API_VERSION\n  azureOpenAIApiInstanceName: \"{MY_INSTANCE_NAME}\", // In Node.js defaults to process.env.AZURE_OPENAI_API_INSTANCE_NAME\n  azureOpenAIApiDeploymentName: \"{DEPLOYMENT_NAME}\", // In Node.js defaults to process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME\n});","metadata":{"source":"examples/src/models/chat/integration_azure_openai.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":9}}}}],["a4e08933-e940-48e9-bb50-0005d19b2788",{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\n\nconst model = new ChatOpenAI({\n  temperature: 0.9,\n  azureOpenAIApiKey: \"SOME_SECRET_VALUE\", // In Node.js defaults to process.env.AZURE_OPENAI_API_KEY\n  azureOpenAIApiVersion: \"YOUR-API-VERSION\", // In Node.js defaults to process.env.AZURE_OPENAI_API_VERSION\n  azureOpenAIApiDeploymentName: \"{DEPLOYMENT_NAME}\", // In Node.js defaults to process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME\n  azureOpenAIBasePath:\n    \"https://westeurope.api.microsoft.com/openai/deployments\", // In Node.js defaults to process.env.AZURE_OPENAI_BASE_PATH\n});","metadata":{"source":"examples/src/models/chat/integration_azure_openai_base_path.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":10}}}}],["2cc4e9b1-b892-4f69-b523-77163dd0be0c",{"pageContent":"import { ChatBaiduWenxin } from \"langchain/chat_models/baiduwenxin\";\nimport { HumanMessage } from \"langchain/schema\";\n\n// Default model is ERNIE-Bot-turbo\nconst ernieTurbo = new ChatBaiduWenxin({\n  baiduApiKey: \"YOUR-API-KEY\", // In Node.js defaults to process.env.BAIDU_API_KEY\n  baiduSecretKey: \"YOUR-SECRET-KEY\", // In Node.js defaults to process.env.BAIDU_SECRET_KEY\n});\n\n// Use ERNIE-Bot\nconst ernie = new ChatBaiduWenxin({\n  modelName: \"ERNIE-Bot\", // Available models: ERNIE-Bot, ERNIE-Bot-turbo, ERNIE-Bot-4\n  temperature: 1,\n  baiduApiKey: \"YOUR-API-KEY\", // In Node.js defaults to process.env.BAIDU_API_KEY\n  baiduSecretKey: \"YOUR-SECRET-KEY\", // In Node.js defaults to process.env.BAIDU_SECRET_KEY\n});\n\nconst messages = [new HumanMessage(\"Hello\")];\n\nlet res = await ernieTurbo.call(messages);\n/*\nAIChatMessage {\n  text: 'Hello! How may I assist you today?',\n  name: undefined,\n  additional_kwargs: {}\n  }\n}\n*/\n\nres = await ernie.call(messages);\n/*\nAIChatMessage {\n  text: 'Hello! How may I assist you today?',\n  name: undefined,\n  additional_kwargs: {}\n  }\n}\n*/","metadata":{"source":"examples/src/models/chat/integration_baiduwenxin.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":38}}}}],["55559229-02e3-4ac6-9a0d-0673869f0729",{"pageContent":"import { BedrockChat } from \"langchain/chat_models/bedrock\";\n// Or, from web environments:\n// import { BedrockChat } from \"langchain/chat_models/bedrock/web\";\n\nimport { HumanMessage } from \"langchain/schema\";\n\n// If no credentials are provided, the default credentials from\n// @aws-sdk/credential-provider-node will be used.\nconst model = new BedrockChat({\n  model: \"anthropic.claude-v2\",\n  region: \"us-east-1\",\n  // endpointUrl: \"custom.amazonaws.com\",\n  // credentials: {\n  //   accessKeyId: process.env.BEDROCK_AWS_ACCESS_KEY_ID!,\n  //   secretAccessKey: process.env.BEDROCK_AWS_SECRET_ACCESS_KEY!,\n  // },\n  // modelKwargs: {},\n});\n\nconst res = await model.invoke([\n  new HumanMessage({ content: \"Tell me a joke\" }),\n]);\nconsole.log(res);\n\n/*\n  AIMessage {\n    content: \" Here's a silly joke: \\n\" +\n      '\\n' +\n      'What do you call a dog magician? A labracadabrador!',\n    name: undefined,\n    additional_kwargs: {}\n  }\n*/","metadata":{"source":"examples/src/models/chat/integration_bedrock.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":33}}}}],["b6c7cb5c-d53c-4578-a05d-676e0637a4ff",{"pageContent":"import { NIBittensorChatModel } from \"langchain/experimental/chat_models/bittensor\";\nimport { HumanMessage } from \"langchain/schema\";\n\nconst chat = new NIBittensorChatModel();\nconst message = new HumanMessage(\"What is bittensor?\");\nconst res = await chat.call([message]);\nconsole.log({ res });\n/*\n  {\n    res: \"\\nBittensor is opensource protocol...\"\n  }\n */","metadata":{"source":"examples/src/models/chat/integration_bittensor.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":12}}}}],["059bb9ae-f3f0-4bb7-8b38-5981dd778192",{"pageContent":"import { ChatCloudflareWorkersAI } from \"langchain/chat_models/cloudflare_workersai\";\n\nconst model = new ChatCloudflareWorkersAI({\n  model: \"@cf/meta/llama-2-7b-chat-int8\", // Default value\n  cloudflareAccountId: process.env.CLOUDFLARE_ACCOUNT_ID,\n  cloudflareApiToken: process.env.CLOUDFLARE_API_TOKEN,\n  // Pass a custom base URL to use Cloudflare AI Gateway\n  // baseUrl: `https://gateway.ai.cloudflare.com/v1/{YOUR_ACCOUNT_ID}/{GATEWAY_NAME}/workers-ai/`,\n});\n\nconst response = await model.invoke([\n  [\"system\", \"You are a helpful assistant that translates English to German.\"],\n  [\"human\", `Translate \"I love programming\".`],\n]);\n\nconsole.log(response);\n\n/*\nAIMessage {\n  content: `Sure! Here's the translation of \"I love programming\" into German:\\n` +\n    '\\n' +\n    '\"Ich liebe Programmieren.\"\\n' +\n    '\\n' +\n    'In this sentence, \"Ich\" means \"I,\" \"liebe\" means \"love,\" and \"Programmieren\" means \"programming.\"',\n  additional_kwargs: {}\n}\n*/\n\nconst stream = await model.stream([\n  [\"system\", \"You are a helpful assistant that translates English to German.\"],\n  [\"human\", `Translate \"I love programming\".`],\n]);\n\nfor await (const chunk of stream) {\n  console.log(chunk);\n}\n\n/*\n  AIMessageChunk {\n    content: 'S',\n    additional_kwargs: {}\n  }\n  AIMessageChunk {\n    content: 'ure',\n    additional_kwargs: {}\n  }\n  AIMessageChunk {\n    content: '!',\n    additional_kwargs: {}\n  }\n  AIMessageChunk {\n    content: ' Here',\n    additional_kwargs: {}\n  }\n  ...\n*/","metadata":{"source":"examples/src/models/chat/integration_cloudflare_workersai.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":56}}}}],["278202f4-d17b-4791-8985-6f6044228db3",{"pageContent":"import { FakeListChatModel } from \"langchain/chat_models/fake\";\nimport { HumanMessage } from \"langchain/schema\";\nimport { StringOutputParser } from \"langchain/schema/output_parser\";\n\n/**\n * The FakeListChatModel can be used to simulate ordered predefined responses.\n */\n\nconst chat = new FakeListChatModel({\n  responses: [\"I'll callback later.\", \"You 'console' them!\"],\n});\n\nconst firstMessage = new HumanMessage(\"You want to hear a JavasSript joke?\");\nconst secondMessage = new HumanMessage(\n  \"How do you cheer up a JavaScript developer?\"\n);\nconst firstResponse = await chat.call([firstMessage]);\nconst secondResponse = await chat.call([secondMessage]);\n\nconsole.log({ firstResponse });\nconsole.log({ secondResponse });\n\n/**\n * The FakeListChatModel can also be used to simulate streamed responses.\n */\n\nconst stream = await chat\n  .pipe(new StringOutputParser())\n  .stream(`You want to hear a JavasSript joke?`);\nconst chunks = [];\nfor await (const chunk of stream) {\n  chunks.push(chunk);\n}\n\nconsole.log(chunks.join(\"\"));\n\n/**\n * The FakeListChatModel can also be used to simulate delays in either either synchronous or streamed responses.\n */\n\nconst slowChat = new FakeListChatModel({\n  responses: [\"Because Oct 31 equals Dec 25\", \"You 'console' them!\"],\n  sleep: 1000,\n});\n\nconst thirdMessage = new HumanMessage(\n  \"Why do programmers always mix up Halloween and Christmas?\"\n);\nconst slowResponse = await slowChat.call([thirdMessage]);\nconsole.log({ slowResponse });\n\nconst slowStream = await slowChat\n  .pipe(new StringOutputParser())\n  .stream(\"How do you cheer up a JavaScript developer?\");\nconst slowChunks = [];\nfor await (const chunk of slowStream) {\n  slowChunks.push(chunk);\n}\n\nconsole.log(slowChunks.join(\"\"));","metadata":{"source":"examples/src/models/chat/integration_fake.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":60}}}}],["c09c627c-f1f3-41ae-a2cb-a7b3aff4ffed",{"pageContent":"import { ChatFireworks } from \"langchain/chat_models/fireworks\";\n\nconst model = new ChatFireworks({\n  temperature: 0.9,\n  // In Node.js defaults to process.env.FIREWORKS_API_KEY\n  fireworksApiKey: \"YOUR-API-KEY\",\n});","metadata":{"source":"examples/src/models/chat/integration_fireworks.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":7}}}}],["967a457e-0878-4afd-ad26-95e5fb3e4052",{"pageContent":"import { ChatGooglePaLM } from \"langchain/chat_models/googlepalm\";\nimport { AIMessage, HumanMessage, SystemMessage } from \"langchain/schema\";\n\nexport const run = async () => {\n  const model = new ChatGooglePaLM({\n    apiKey: \"<YOUR API KEY>\", // or set it in environment variable as `GOOGLE_PALM_API_KEY`\n    temperature: 0.7, // OPTIONAL\n    modelName: \"models/chat-bison-001\", // OPTIONAL\n    topK: 40, // OPTIONAL\n    topP: 1, // OPTIONAL\n    examples: [\n      // OPTIONAL\n      {\n        input: new HumanMessage(\"What is your favorite sock color?\"),\n        output: new AIMessage(\"My favorite sock color be arrrr-ange!\"),\n      },\n    ],\n  });\n\n  // ask questions\n  const questions = [\n    new SystemMessage(\n      \"You are a funny assistant that answers in pirate language.\"\n    ),\n    new HumanMessage(\"What is your favorite food?\"),\n  ];\n\n  // You can also use the model as part of a chain\n  const res = await model.call(questions);\n  console.log({ res });\n};","metadata":{"source":"examples/src/models/chat/integration_googlepalm.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":31}}}}],["d892d12e-24dd-41e6-a280-8f1d925610e4",{"pageContent":"import { AIMessage, HumanMessage, SystemMessage } from \"langchain/schema\";\n\nimport { ChatGoogleVertexAI } from \"langchain/chat_models/googlevertexai\";\n// Or, if using the web entrypoint:\n// import { ChatGoogleVertexAI } from \"langchain/chat_models/googlevertexai/web\";\n\nconst examples = [\n  {\n    input: new HumanMessage(\"What is your favorite sock color?\"),\n    output: new AIMessage(\"My favorite sock color be arrrr-ange!\"),\n  },\n];\nconst model = new ChatGoogleVertexAI({\n  temperature: 0.7,\n  examples,\n});\nconst questions = [\n  new SystemMessage(\n    \"You are a funny assistant that answers in pirate language.\"\n  ),\n  new HumanMessage(\"What is your favorite food?\"),\n];\n// You can also use the model as part of a chain\nconst res = await model.invoke(questions);\nconsole.log({ res });","metadata":{"source":"examples/src/models/chat/integration_googlevertexai-examples.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":25}}}}],["1321f7ec-4f17-4f86-a99e-6a448f492595",{"pageContent":"import { ChatGoogleVertexAI } from \"langchain/chat_models/googlevertexai\";\n// Or, if using the web entrypoint:\n// import { ChatGoogleVertexAI } from \"langchain/chat_models/googlevertexai/web\";\n\nconst model = new ChatGoogleVertexAI({\n  temperature: 0.7,\n});\nconst stream = await model.stream([\n  [\"system\", \"You are a funny assistant that answers in pirate language.\"],\n  [\"human\", \"What is your favorite food?\"],\n]);\n\nfor await (const chunk of stream) {\n  console.log(chunk);\n}\n\n/*\nAIMessageChunk {\n  content: ' Ahoy there, matey! My favorite food be fish, cooked any way ye ',\n  additional_kwargs: {}\n}\nAIMessageChunk {\n  content: 'like!',\n  additional_kwargs: {}\n}\nAIMessageChunk {\n  content: '',\n  name: undefined,\n  additional_kwargs: {}\n}\n*/","metadata":{"source":"examples/src/models/chat/integration_googlevertexai-streaming.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":31}}}}],["4ffdd619-af59-4a62-8618-d75185b0b43e",{"pageContent":"import { ChatGoogleVertexAI } from \"langchain/chat_models/googlevertexai\";\n// Or, if using the web entrypoint:\n// import { ChatGoogleVertexAI } from \"langchain/chat_models/googlevertexai/web\";\n\nconst model = new ChatGoogleVertexAI({\n  temperature: 0.7,\n});","metadata":{"source":"examples/src/models/chat/integration_googlevertexai.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":7}}}}],["7589322e-44f2-4ffc-b0e2-d646b4d96da2",{"pageContent":"import { ChatIflytekXinghuo } from \"langchain/chat_models/iflytek_xinghuo\";\nimport { HumanMessage } from \"langchain/schema\";\n\nconst model = new ChatIflytekXinghuo();\n\nconst messages1 = [new HumanMessage(\"Nice to meet you!\")];\n\nconst res1 = await model.call(messages1);\n\nconsole.log(res1);\n\nconst messages2 = [new HumanMessage(\"Hello\")];\n\nconst res2 = await model.call(messages2);\n\nconsole.log(res2);","metadata":{"source":"examples/src/models/chat/integration_iflytek_xinghuo.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":16}}}}],["393b460f-e139-4229-8ef4-3621b310d2b8",{"pageContent":"import { ChatLlamaCpp } from \"langchain/chat_models/llama_cpp\";\nimport { HumanMessage } from \"langchain/schema\";\n\nconst llamaPath = \"/Replace/with/path/to/your/model/gguf-llama2-q4_0.bin\";\n\nconst model = new ChatLlamaCpp({ modelPath: llamaPath });\n\nconst response = await model.call([\n  new HumanMessage({ content: \"My name is John.\" }),\n]);\nconsole.log({ response });\n\n/*\n  AIMessage {\n    lc_serializable: true,\n    lc_kwargs: {\n      content: 'Hello John.',\n      additional_kwargs: {}\n    },\n    lc_namespace: [ 'langchain', 'schema' ],\n    content: 'Hello John.',\n    name: undefined,\n    additional_kwargs: {}\n  }\n*/","metadata":{"source":"examples/src/models/chat/integration_llama_cpp.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":25}}}}],["293910ef-a6dc-42de-accb-a6bdff90406b",{"pageContent":"import { ChatLlamaCpp } from \"langchain/chat_models/llama_cpp\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport { LLMChain } from \"langchain/chains\";\n\nconst llamaPath = \"/Replace/with/path/to/your/model/gguf-llama2-q4_0.bin\";\n\nconst model = new ChatLlamaCpp({ modelPath: llamaPath, temperature: 0.5 });\n\nconst prompt = PromptTemplate.fromTemplate(\n  \"What is a good name for a company that makes {product}?\"\n);\nconst chain = new LLMChain({ llm: model, prompt });\n\nconst response = await chain.call({ product: \"colorful socks\" });\n\nconsole.log({ response });\n\n/*\n  {\n  text: `I'm not sure what you mean by \"colorful socks\" but here are some ideas:\\n` +\n    '\\n' +\n    '- Sock-it to me!\\n' +\n    '- Socks Away\\n' +\n    '- Fancy Footwear'\n  }\n*/","metadata":{"source":"examples/src/models/chat/integration_llama_cpp_chain.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":26}}}}],["b8c1a065-30a6-4bae-98a3-e1e678e10db5",{"pageContent":"import { ChatLlamaCpp } from \"langchain/chat_models/llama_cpp\";\nimport { SystemMessage, HumanMessage } from \"langchain/schema\";\n\nconst llamaPath = \"/Replace/with/path/to/your/model/gguf-llama2-q4_0.bin\";\n\nconst model = new ChatLlamaCpp({ modelPath: llamaPath });\n\nconst response = await model.call([\n  new SystemMessage(\n    \"You are a pirate, responses must be very verbose and in pirate dialect, add 'Arr, m'hearty!' to each sentence.\"\n  ),\n  new HumanMessage(\"Tell me where Llamas come from?\"),\n]);\nconsole.log({ response });\n\n/*\n  AIMessage {\n    lc_serializable: true,\n    lc_kwargs: {\n      content: \"Arr, m'hearty! Llamas come from the land of Peru.\",\n      additional_kwargs: {}\n    },\n    lc_namespace: [ 'langchain', 'schema' ],\n    content: \"Arr, m'hearty! Llamas come from the land of Peru.\",\n    name: undefined,\n    additional_kwargs: {}\n  }\n*/","metadata":{"source":"examples/src/models/chat/integration_llama_cpp_system.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":28}}}}],["43a5008d-128a-4b0c-a260-76fd283541f5",{"pageContent":"import { HumanMessage } from \"langchain/schema\";\nimport { ChatMinimax } from \"langchain/chat_models/minimax\";\n\n// Use abab5.5\nconst abab5_5 = new ChatMinimax({\n  modelName: \"abab5.5-chat\",\n  botSetting: [\n    {\n      bot_name: \"MM Assistant\",\n      content: \"MM Assistant is an AI Assistant developed by minimax.\",\n    },\n  ],\n});\nconst messages = [\n  new HumanMessage({\n    content: \"Hello\",\n  }),\n];\n\nconst res = await abab5_5.invoke(messages);\nconsole.log(res);\n\n/*\nAIChatMessage {\n  text: 'Hello! How may I assist you today?',\n  name: undefined,\n  additional_kwargs: {}\n  }\n}\n*/\n\n// use abab5\nconst abab5 = new ChatMinimax({\n  proVersion: false,\n  modelName: \"abab5-chat\",\n  minimaxGroupId: process.env.MINIMAX_GROUP_ID, // In Node.js defaults to process.env.MINIMAX_GROUP_ID\n  minimaxApiKey: process.env.MINIMAX_API_KEY, // In Node.js defaults to process.env.MINIMAX_API_KEY\n});\n\nconst result = await abab5.invoke([\n  new HumanMessage({\n    content: \"Hello\",\n    name: \"XiaoMing\",\n  }),\n]);\nconsole.log(result);\n\n/*\nAIMessage {\n  lc_serializable: true,\n  lc_kwargs: {\n    content: 'Hello! Can I help you with anything?',\n    additional_kwargs: { function_call: undefined }\n  },\n  lc_namespace: [ 'langchain', 'schema' ],\n  content: 'Hello! Can I help you with anything?',\n  name: undefined,\n  additional_kwargs: { function_call: undefined }\n}\n */","metadata":{"source":"examples/src/models/chat/integration_minimax.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":60}}}}],["d7035aa1-f090-46cb-b635-f5202d8e5ef6",{"pageContent":"import { ChatOllama } from \"langchain/chat_models/ollama\";\nimport { StringOutputParser } from \"langchain/schema/output_parser\";\n\nconst model = new ChatOllama({\n  baseUrl: \"http://localhost:11434\", // Default value\n  model: \"llama2\", // Default value\n});\n\nconst stream = await model\n  .pipe(new StringOutputParser())\n  .stream(`Translate \"I love programming\" into German.`);\n\nconst chunks = [];\nfor await (const chunk of stream) {\n  chunks.push(chunk);\n}\n\nconsole.log(chunks.join(\"\"));\n\n/*\n  Thank you for your question! I'm happy to help. However, I must point out that the phrase \"I love programming\" is not grammatically correct in German. The word \"love\" does not have a direct translation in German, and it would be more appropriate to say \"I enjoy programming\" or \"I am passionate about programming.\"\n\n  In German, you can express your enthusiasm for something like this:\n\n  * Ich möchte Programmieren (I want to program)\n  * Ich mag Programmieren (I like to program)\n  * Ich bin passioniert über Programmieren (I am passionate about programming)\n\n  I hope this helps! Let me know if you have any other questions.\n*/","metadata":{"source":"examples/src/models/chat/integration_ollama.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":30}}}}],["a1a6ff9d-e8ef-49ea-8adb-4c7c36f4c8b9",{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { HumanMessage } from \"langchain/schema\";\nimport { SerpAPI } from \"langchain/tools\";\n\nconst model = new ChatOpenAI({\n  temperature: 0.9,\n  openAIApiKey: \"YOUR-API-KEY\", // In Node.js defaults to process.env.OPENAI_API_KEY\n});\n\n// You can also pass tools or functions to the model, learn more here\n// https://platform.openai.com/docs/guides/gpt/function-calling\n\nconst modelForFunctionCalling = new ChatOpenAI({\n  modelName: \"gpt-4\",\n  temperature: 0,\n});\n\nawait modelForFunctionCalling.predictMessages(\n  [new HumanMessage(\"What is the weather in New York?\")],\n  { tools: [new SerpAPI()] }\n  // Tools will be automatically formatted as functions in the OpenAI format\n);\n/*\nAIMessage {\n  text: '',\n  name: undefined,\n  additional_kwargs: {\n    function_call: {\n      name: 'search',\n      arguments: '{\\n  \"input\": \"current weather in New York\"\\n}'\n    }\n  }\n}\n*/\n\nawait modelForFunctionCalling.predictMessages(\n  [new HumanMessage(\"What is the weather in New York?\")],\n  {\n    functions: [\n      {\n        name: \"get_current_weather\",\n        description: \"Get the current weather in a given location\",\n        parameters: {\n          type: \"object\",\n          properties: {\n            location: {\n              type: \"string\",\n              description: \"The city and state, e.g. San Francisco, CA\",\n            },\n            unit: { type: \"string\", enum: [\"celsius\", \"fahrenheit\"] },\n          },\n          required: [\"location\"],\n        },\n      },\n    ],\n    // You can set the `function_call` arg to force the model to use a function\n    function_call: {\n      name: \"get_current_weather\",\n    },\n  }\n);\n/*\nAIMessage {\n  text: '',\n  name: undefined,\n  additional_kwargs: {\n    function_call: {\n      name: 'get_current_weather',\n      arguments: '{\\n  \"location\": \"New York\"\\n}'\n    }\n  }\n}\n*/","metadata":{"source":"examples/src/models/chat/integration_openai.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":73}}}}],["a6731dad-b1d0-43e9-8840-6c62d5982375",{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\n\nconst model = new ChatOpenAI({\n  temperature: 0.9,\n  configuration: {\n    baseURL: \"https://your_custom_url.com\",\n  },\n});\n\nconst message = await model.invoke(\"Hi there!\");\n\nconsole.log(message);\n\n/*\n  AIMessage {\n    content: 'Hello! How can I assist you today?',\n    additional_kwargs: { function_call: undefined }\n  }\n*/","metadata":{"source":"examples/src/models/chat/integration_openai_custom_base.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":19}}}}],["a10fc623-cc39-4120-9296-4126edeb39e2",{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\n\nconst model = new ChatOpenAI({\n  temperature: 0.9,\n  modelName: \"ft:gpt-3.5-turbo-0613:{ORG_NAME}::{MODEL_ID}\",\n});\n\nconst message = await model.invoke(\"Hi there!\");\n\nconsole.log(message);\n\n/*\n  AIMessage {\n    content: 'Hello! How can I assist you today?',\n    additional_kwargs: { function_call: undefined }\n  }\n*/","metadata":{"source":"examples/src/models/chat/integration_openai_fine_tune.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":17}}}}],["860fd9b5-2f0a-41d3-b045-9af7cde13308",{"pageContent":"import { ChatYandexGPT } from \"langchain/chat_models/yandex\";\nimport { HumanMessage, SystemMessage } from \"langchain/schema\";\n\nconst chat = new ChatYandexGPT();\n\nconst res = await chat.call([\n  new SystemMessage(\n    \"You are a helpful assistant that translates English to French.\"\n  ),\n  new HumanMessage(\"I love programming.\"),\n]);\nconsole.log(res);\n\n/*\nAIMessage {\n  lc_serializable: true,\n  lc_kwargs: { content: \"Je t'aime programmer.\", additional_kwargs: {} },\n  lc_namespace: [ 'langchain', 'schema' ],\n  content: \"Je t'aime programmer.\",\n  name: undefined,\n  additional_kwargs: {}\n}\n */","metadata":{"source":"examples/src/models/chat/integration_yandex.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":23}}}}],["786d84ee-a81e-460a-89f9-71820a94084c",{"pageContent":"import {\n  ChatPromptTemplate,\n  HumanMessagePromptTemplate,\n  SystemMessagePromptTemplate,\n} from \"langchain/prompts\";\nimport { LLMChain } from \"langchain/chains\";\nimport { ChatMinimax } from \"langchain/chat_models/minimax\";\n\n// We can also construct an LLMChain from a ChatPromptTemplate and a chat model.\nconst chat = new ChatMinimax({ temperature: 0.01 });\n\nconst chatPrompt = ChatPromptTemplate.fromMessages([\n  SystemMessagePromptTemplate.fromTemplate(\n    \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n  ),\n  HumanMessagePromptTemplate.fromTemplate(\"{text}\"),\n]);\nconst chainB = new LLMChain({\n  prompt: chatPrompt,\n  llm: chat,\n});\n\nconst resB = await chainB.call({\n  input_language: \"English\",\n  output_language: \"Chinese\",\n  text: \"I love programming.\",\n});\nconsole.log({ resB });","metadata":{"source":"examples/src/models/chat/minimax_chain.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":28}}}}],["f70841e7-0009-4ac7-a861-ab238761227c",{"pageContent":"import { HumanMessage } from \"langchain/schema\";\nimport { ChatMinimax } from \"langchain/chat_models/minimax\";\n\nconst functionSchema = {\n  name: \"get_weather\",\n  description: \" Get weather information.\",\n  parameters: {\n    type: \"object\",\n    properties: {\n      location: {\n        type: \"string\",\n        description: \" The location to get the weather\",\n      },\n    },\n    required: [\"location\"],\n  },\n};\n\n// Bind function arguments to the model.\n// All subsequent invoke calls will use the bound parameters.\n// \"functions.parameters\" must be formatted as JSON Schema\nconst model = new ChatMinimax({\n  botSetting: [\n    {\n      bot_name: \"MM Assistant\",\n      content: \"MM Assistant is an AI Assistant developed by minimax.\",\n    },\n  ],\n}).bind({\n  functions: [functionSchema],\n});\n\nconst result = await model.invoke([\n  new HumanMessage({\n    content: \" What is the weather like in NewYork tomorrow?\",\n    name: \"I\",\n  }),\n]);\n\nconsole.log(result);\n\n/*\nAIMessage {\n  lc_serializable: true,\n  lc_kwargs: { content: '', additional_kwargs: { function_call: [Object] } },\n  lc_namespace: [ 'langchain', 'schema' ],\n  content: '',\n  name: undefined,\n  additional_kwargs: {\n    function_call: { name: 'get_weather', arguments: '{\"location\": \"NewYork\"}' }\n  }\n}\n*/\n\n// Alternatively, you can pass function call arguments as an additional argument as a one-off:\n\nconst minimax = new ChatMinimax({\n  modelName: \"abab5.5-chat\",\n  botSetting: [\n    {\n      bot_name: \"MM Assistant\",\n      content: \"MM Assistant is an AI Assistant developed by minimax.\",\n    },\n  ],\n});\n\nconst result2 = await minimax.call(\n  [new HumanMessage(\"What is the weather like in NewYork tomorrow?\")],\n  {\n    functions: [functionSchema],\n  }\n);\nconsole.log(result2);\n\n/*\nAIMessage {\n  lc_serializable: true,\n  lc_kwargs: { content: '', additional_kwargs: { function_call: [Object] } },\n  lc_namespace: [ 'langchain', 'schema' ],\n  content: '',\n  name: undefined,\n  additional_kwargs: {\n    function_call: { name: 'get_weather', arguments: '{\"location\": \"NewYork\"}' }\n  }\n}\n */","metadata":{"source":"examples/src/models/chat/minimax_functions.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":86}}}}],["6805561d-8c51-45f2-b3ae-27ad232a8895",{"pageContent":"import { HumanMessage } from \"langchain/schema\";\nimport { z } from \"zod\";\nimport { zodToJsonSchema } from \"zod-to-json-schema\";\nimport { ChatMinimax } from \"langchain/chat_models/minimax\";\n\nconst extractionFunctionZodSchema = z.object({\n  location: z.string().describe(\" The location to get the weather\"),\n});\n\n// Bind function arguments to the model.\n// \"functions.parameters\" must be formatted as JSON Schema.\n// We translate the above Zod schema into JSON schema using the \"zodToJsonSchema\" package.\n\nconst model = new ChatMinimax({\n  modelName: \"abab5.5-chat\",\n  botSetting: [\n    {\n      bot_name: \"MM Assistant\",\n      content: \"MM Assistant is an AI Assistant developed by minimax.\",\n    },\n  ],\n}).bind({\n  functions: [\n    {\n      name: \"get_weather\",\n      description: \" Get weather information.\",\n      parameters: zodToJsonSchema(extractionFunctionZodSchema),\n    },\n  ],\n});\n\nconst result = await model.invoke([\n  new HumanMessage({\n    content: \" What is the weather like in Shanghai tomorrow?\",\n    name: \"XiaoMing\",\n  }),\n]);\n\nconsole.log(result);\n\n/*\nAIMessage {\n  lc_serializable: true,\n  lc_kwargs: { content: '', additional_kwargs: { function_call: [Object] } },\n  lc_namespace: [ 'langchain', 'schema' ],\n  content: '',\n  name: undefined,\n  additional_kwargs: {\n    function_call: { name: 'get_weather', arguments: '{\"location\": \"Shanghai\"}' }\n  }\n}\n*/","metadata":{"source":"examples/src/models/chat/minimax_functions_zod.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":52}}}}],["bf5b1ffc-c161-45ec-a77f-addfc435de3f",{"pageContent":"import { ChatMinimax } from \"langchain/chat_models/minimax\";\nimport {\n  ChatPromptTemplate,\n  HumanMessagePromptTemplate,\n} from \"langchain/prompts\";\nimport { HumanMessage } from \"langchain/schema\";\n\nconst model = new ChatMinimax({\n  modelName: \"abab5.5-chat\",\n  botSetting: [\n    {\n      bot_name: \"MM Assistant\",\n      content: \"MM Assistant is an AI Assistant developed by minimax.\",\n    },\n  ],\n}).bind({\n  replyConstraints: {\n    sender_type: \"BOT\",\n    sender_name: \"MM Assistant\",\n    glyph: {\n      type: \"raw\",\n      raw_glyph: \"The translated text：{{gen 'content'}}\",\n    },\n  },\n});\n\nconst messagesTemplate = ChatPromptTemplate.fromMessages([\n  HumanMessagePromptTemplate.fromTemplate(\n    \" Please help me translate the following sentence in English： {text}\"\n  ),\n]);\n\nconst messages = await messagesTemplate.formatMessages({ text: \"我是谁\" });\nconst result = await model.invoke(messages);\n\nconsole.log(result);\n\n/*\nAIMessage {\n  lc_serializable: true,\n  lc_kwargs: {\n    content: 'The translated text： Who am I\\x02',\n    additional_kwargs: { function_call: undefined }\n  },\n  lc_namespace: [ 'langchain', 'schema' ],\n  content: 'The translated text： Who am I\\x02',\n  name: undefined,\n  additional_kwargs: { function_call: undefined }\n}\n*/\n\n// use json_value\n\nconst modelMinimax = new ChatMinimax({\n  modelName: \"abab5.5-chat\",\n  botSetting: [\n    {\n      bot_name: \"MM Assistant\",\n      content: \"MM Assistant is an AI Assistant developed by minimax.\",\n    },\n  ],\n}).bind({\n  replyConstraints: {\n    sender_type: \"BOT\",\n    sender_name: \"MM Assistant\",\n    glyph: {\n      type: \"json_value\",\n      json_properties: {\n        name: {\n          type: \"string\",\n        },\n        age: {\n          type: \"number\",\n        },\n        is_student: {\n          type: \"boolean\",\n        },\n        is_boy: {\n          type: \"boolean\",\n        },\n        courses: {\n          type: \"object\",\n          properties: {\n            name: {\n              type: \"string\",\n            },\n            score: {\n              type: \"number\",\n            },\n          },\n        },\n      },\n    },\n  },\n});\n\nconst result2 = await modelMinimax.invoke([\n  new HumanMessage({\n    content:\n      \" My name is Yue Wushuang, 18 years old this year, just finished the test with 99.99 points.\",\n    name: \"XiaoMing\",\n  }),\n]);\n\nconsole.log(result2);\n\n/*\nAIMessage {\n  lc_serializable: true,\n  lc_kwargs: {\n    content: '{\\n' +\n      '  \"name\": \"Yue Wushuang\",\\n' +\n      '  \"is_student\": true,\\n' +\n      '  \"is_boy\": false,\\n' +\n      '  \"courses\":   {\\n' +\n      '    \"name\": \"Mathematics\",\\n' +\n      '    \"score\": 99.99\\n' +\n      '   },\\n' +\n      '  \"age\": 18\\n' +\n      ' }',\n    additional_kwargs: { function_call: undefined }\n  }\n}\n\n */","metadata":{"source":"examples/src/models/chat/minimax_glyph.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":125}}}}],["656ddab8-4a84-4a98-8172-a3ada5bbc923",{"pageContent":"import { HumanMessage } from \"langchain/schema\";\nimport { ChatMinimax } from \"langchain/chat_models/minimax\";\n\nconst model = new ChatMinimax({\n  modelName: \"abab5.5-chat\",\n  botSetting: [\n    {\n      bot_name: \"MM Assistant\",\n      content: \"MM Assistant is an AI Assistant developed by minimax.\",\n    },\n  ],\n}).bind({\n  plugins: [\"plugin_web_search\"],\n});\n\nconst result = await model.invoke([\n  new HumanMessage({\n    content: \" What is the weather like in NewYork tomorrow?\",\n  }),\n]);\n\nconsole.log(result);\n\n/*\nAIMessage {\n  lc_serializable: true,\n  lc_kwargs: {\n    content: 'The weather in Shanghai tomorrow is expected to be hot. Please note that this is just a forecast and the actual weather conditions may vary.',\n    additional_kwargs: { function_call: undefined }\n  },\n  lc_namespace: [ 'langchain', 'schema' ],\n  content: 'The weather in Shanghai tomorrow is expected to be hot. Please note that this is just a forecast and the actual weather conditions may vary.',\n  name: undefined,\n  additional_kwargs: { function_call: undefined }\n}\n*/","metadata":{"source":"examples/src/models/chat/minimax_plugins.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":36}}}}],["8c068934-55ae-4177-af99-5eb4444c8707",{"pageContent":"import { AIMessage, HumanMessage } from \"langchain/schema\";\nimport { ChatMinimax } from \"langchain/chat_models/minimax\";\n\nconst model = new ChatMinimax({\n  modelName: \"abab5.5-chat\",\n  botSetting: [\n    {\n      bot_name: \"MM Assistant\",\n      content: \"MM Assistant is an AI Assistant developed by minimax.\",\n    },\n  ],\n}).bind({\n  sampleMessages: [\n    new HumanMessage({\n      content: \" Turn A5 into red and modify the content to minimax.\",\n    }),\n    new AIMessage({\n      content: \"select A5 color red change minimax\",\n    }),\n  ],\n});\n\nconst result = await model.invoke([\n  new HumanMessage({\n    content:\n      ' Please reply to my content according to the following requirements: According to the following interface list, give the order and parameters of calling the interface for the content I gave. You just need to give the order and parameters of calling the interface, and do not give any other output. The following is the available interface list: select: select specific table position, input parameter use letters and numbers to determine, for example \"B13\"; color: dye the selected table position, input parameters use the English name of the color, for example \"red\"; change: modify the selected table position, input parameters use strings.',\n  }),\n  new HumanMessage({\n    content: \" Process B6 to gray and modify the content to question.\",\n  }),\n]);\n\nconsole.log(result);","metadata":{"source":"examples/src/models/chat/minimax_sample_messages.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":33}}}}],["28567242-dff5-447b-86c5-62b5146daccd",{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { HumanMessage } from \"langchain/schema\";\n\nconst extractionFunctionSchema = {\n  name: \"extractor\",\n  description: \"Extracts fields from the input.\",\n  parameters: {\n    type: \"object\",\n    properties: {\n      tone: {\n        type: \"string\",\n        enum: [\"positive\", \"negative\"],\n        description: \"The overall tone of the input\",\n      },\n      word_count: {\n        type: \"number\",\n        description: \"The number of words in the input\",\n      },\n      chat_response: {\n        type: \"string\",\n        description: \"A response to the human's input\",\n      },\n    },\n    required: [\"tone\", \"word_count\", \"chat_response\"],\n  },\n};\n\nconst model = new ChatOpenAI({\n  modelName: \"gpt-4\",\n}).bind({\n  functions: [extractionFunctionSchema],\n  function_call: { name: \"extractor\" },\n});\n\nconst result = await model.invoke([new HumanMessage(\"What a beautiful day!\")]);\n\nconsole.log(result);\n/*\nAIMessage {\n  lc_serializable: true,\n  lc_kwargs: { content: '', additional_kwargs: { function_call: [Object] } },\n  lc_namespace: [ 'langchain', 'schema' ],\n  content: '',\n  name: undefined,\n  additional_kwargs: {\n    function_call: {\n      name: 'extractor',\n      arguments: '{\\n' +\n        '  \"tone\": \"positive\",\\n' +\n        '  \"word_count\": 4,\\n' +\n        `  \"chat_response\": \"I'm glad you're enjoying the day! What makes it so beautiful for you?\"\\n` +\n        '}'\n    }\n  }\n}\n*/","metadata":{"source":"examples/src/models/chat/openai_functions.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":56}}}}],["d3cad356-45b8-4364-bed4-50f7ba098af8",{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { HumanMessage } from \"langchain/schema\";\nimport { z } from \"zod\";\nimport { zodToJsonSchema } from \"zod-to-json-schema\";\n\nconst extractionFunctionSchema = {\n  name: \"extractor\",\n  description: \"Extracts fields from the input.\",\n  parameters: zodToJsonSchema(\n    z.object({\n      tone: z\n        .enum([\"positive\", \"negative\"])\n        .describe(\"The overall tone of the input\"),\n      entity: z.string().describe(\"The entity mentioned in the input\"),\n      word_count: z.number().describe(\"The number of words in the input\"),\n      chat_response: z.string().describe(\"A response to the human's input\"),\n      final_punctuation: z\n        .optional(z.string())\n        .describe(\"The final punctuation mark in the input, if any.\"),\n    })\n  ),\n};\n\nconst model = new ChatOpenAI({\n  modelName: \"gpt-4\",\n}).bind({\n  functions: [extractionFunctionSchema],\n  function_call: { name: \"extractor\" },\n});\n\nconst result = await model.invoke([new HumanMessage(\"What a beautiful day!\")]);\n\nconsole.log(result);\n/*\nAIMessage {\n  lc_serializable: true,\n  lc_kwargs: { content: '', additional_kwargs: { function_call: [Object] } },\n  lc_namespace: [ 'langchain', 'schema' ],\n  content: '',\n  name: undefined,\n  additional_kwargs: {\n    function_call: {\n      name: 'extractor',\n      arguments: '{\\n' +\n        '\"tone\": \"positive\",\\n' +\n        '\"entity\": \"day\",\\n' +\n        '\"word_count\": 4,\\n' +\n        `\"chat_response\": \"I'm glad you're enjoying the day!\",\\n` +\n        '\"final_punctuation\": \"!\"\\n' +\n        '}'\n    }\n  }\n}\n*/","metadata":{"source":"examples/src/models/chat/openai_functions_zod.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":54}}}}],["98312199-230d-4cca-8e7f-d084973e2f7c",{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { PromptTemplate } from \"langchain/prompts\";\n\nconst chat = new ChatOpenAI({});\n// Create a prompt to start the conversation.\nconst prompt =\n  PromptTemplate.fromTemplate(`You're a dog, good luck with the conversation.\nQuestion: {question}`);\n// Define your runnable by piping the prompt into the chat model.\nconst runnable = prompt.pipe(chat);\n// Call .invoke() and pass in the input defined in the prompt template.\nconst response = await runnable.invoke({ question: \"Who's a good boy??\" });\nconsole.log(response);\n// AIMessage { content: \"Woof woof! Thank you for asking! I believe I'm a good boy! I try my best to be a good dog and make my humans happy. Wagging my tail happily here! How can I make your day better?\" }","metadata":{"source":"examples/src/models/chat/runnable_chat_quick_start.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":14}}}}],["4d2b34b6-ac63-4b8d-9139-621f30bcb259",{"pageContent":"import { z } from \"zod\";\nimport { zodToJsonSchema } from \"zod-to-json-schema\";\n\nimport { AnthropicFunctions } from \"langchain/experimental/chat_models/anthropic_functions\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport { JsonOutputFunctionsParser } from \"langchain/output_parsers\";\n\nconst EXTRACTION_TEMPLATE = `Extract and save the relevant entities mentioned in the following passage together with their properties.\n\nPassage:\n{input}\n`;\n\nconst prompt = PromptTemplate.fromTemplate(EXTRACTION_TEMPLATE);\n\n// Use Zod for easier schema declaration\nconst schema = z.object({\n  people: z.array(\n    z.object({\n      name: z.string().describe(\"The name of a person\"),\n      height: z.number().describe(\"The person's height\"),\n      hairColor: z.optional(z.string()).describe(\"The person's hair color\"),\n    })\n  ),\n});\n\nconst model = new AnthropicFunctions({\n  temperature: 0.1,\n}).bind({\n  functions: [\n    {\n      name: \"information_extraction\",\n      description: \"Extracts the relevant information from the passage.\",\n      parameters: {\n        type: \"object\",\n        properties: zodToJsonSchema(schema),\n      },\n    },\n  ],\n  function_call: {\n    name: \"information_extraction\",\n  },\n});\n\nconst chain = await prompt.pipe(model).pipe(new JsonOutputFunctionsParser());\n\nconst response = await chain.invoke({\n  input:\n    \"Alex is 5 feet tall. Claudia is 1 foot taller than Alex and jumps higher than him. Claudia is a brunette and Alex is blonde.\",\n});\n\nconsole.log(response);\n\n/*\n  {\n    people: [\n      { name: 'Alex', height: 5, hairColor: 'blonde' },\n      { name: 'Claudia', height: 6, hairColor: 'brunette' }\n    ]\n  }\n*/","metadata":{"source":"examples/src/models/chat/anthropic_functions/extraction.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":61}}}}],["bc475072-da3e-4908-a111-f121f7d2e711",{"pageContent":"import { AnthropicFunctions } from \"langchain/experimental/chat_models/anthropic_functions\";\nimport { HumanMessage } from \"langchain/schema\";\n\nconst model = new AnthropicFunctions({\n  temperature: 0.1,\n}).bind({\n  functions: [\n    {\n      name: \"get_current_weather\",\n      description: \"Get the current weather in a given location\",\n      parameters: {\n        type: \"object\",\n        properties: {\n          location: {\n            type: \"string\",\n            description: \"The city and state, e.g. San Francisco, CA\",\n          },\n          unit: { type: \"string\", enum: [\"celsius\", \"fahrenheit\"] },\n        },\n        required: [\"location\"],\n      },\n    },\n  ],\n  // You can set the `function_call` arg to force the model to use a function\n  function_call: {\n    name: \"get_current_weather\",\n  },\n});\n\nconst response = await model.invoke([\n  new HumanMessage({\n    content: \"What's the weather in Boston?\",\n  }),\n]);\n\nconsole.log(response);\n\n/*\n  AIMessage {\n    content: '',\n    additional_kwargs: {\n      function_call: {\n        name: 'get_current_weather',\n        arguments: '{\"location\":\"Boston, MA\",\"unit\":\"fahrenheit\"}'\n      }\n    }\n  }\n*/","metadata":{"source":"examples/src/models/chat/anthropic_functions/function_calling.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":48}}}}],["dac4d1fe-7d59-46b5-a651-171959a25d2f",{"pageContent":"import { CohereEmbeddings } from \"langchain/embeddings/cohere\";\n\nexport const run = async () => {\n  /* Embed queries */\n  const embeddings = new CohereEmbeddings();\n  const res = await embeddings.embedQuery(\"Hello world\");\n  console.log(res);\n  /* Embed documents */\n  const documentRes = await embeddings.embedDocuments([\n    \"Hello world\",\n    \"Bye bye\",\n  ]);\n  console.log({ documentRes });\n};","metadata":{"source":"examples/src/models/embeddings/cohere.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":14}}}}],["5cf26777-11a1-44a4-9d13-b48ba377d513",{"pageContent":"import { GooglePaLMEmbeddings } from \"langchain/embeddings/googlepalm\";\n\nconst model = new GooglePaLMEmbeddings({\n  apiKey: \"<YOUR API KEY>\", // or set it in environment variable as `GOOGLE_PALM_API_KEY`\n  modelName: \"models/embedding-gecko-001\", // OPTIONAL\n});\n/* Embed queries */\nconst res = await model.embedQuery(\n  \"What would be a good company name for a company that makes colorful socks?\"\n);\nconsole.log({ res });\n/* Embed documents */\nconst documentRes = await model.embedDocuments([\"Hello world\", \"Bye bye\"]);\nconsole.log({ documentRes });","metadata":{"source":"examples/src/models/embeddings/googlepalm.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":14}}}}],["b9aaa4c2-58c5-4e77-ac06-7eed5787dc43",{"pageContent":"import { GoogleVertexAIEmbeddings } from \"langchain/embeddings/googlevertexai\";\n\nexport const run = async () => {\n  const model = new GoogleVertexAIEmbeddings();\n  const res = await model.embedQuery(\n    \"What would be a good company name for a company that makes colorful socks?\"\n  );\n  console.log({ res });\n};","metadata":{"source":"examples/src/models/embeddings/googlevertexai.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":9}}}}],["7f571ce1-911f-4f96-9bb4-5fd77336fd25",{"pageContent":"import fs from \"fs\";\nimport { GoogleVertexAIMultimodalEmbeddings } from \"langchain/experimental/multimodal_embeddings/googlevertexai\";\n\nconst model = new GoogleVertexAIMultimodalEmbeddings();\n\n// Load the image into a buffer to get the embedding of it\nconst img = fs.readFileSync(\"/path/to/file.jpg\");\nconst imgEmbedding = await model.embedImageQuery(img);\nconsole.log({ imgEmbedding });\n\n// You can also get text embeddings\nconst textEmbedding = await model.embedQuery(\n  \"What would be a good company name for a company that makes colorful socks?\"\n);\nconsole.log({ textEmbedding });","metadata":{"source":"examples/src/models/embeddings/googlevertexai_multimodal.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":15}}}}],["433d25ff-4b2e-4b49-9ccc-9e2a4a8088a2",{"pageContent":"import fs from \"fs\";\nimport { GoogleVertexAIMultimodalEmbeddings } from \"langchain/experimental/multimodal_embeddings/googlevertexai\";\nimport { FaissStore } from \"langchain/vectorstores/faiss\";\nimport { Document } from \"langchain/document\";\n\nconst embeddings = new GoogleVertexAIMultimodalEmbeddings();\n\nconst vectorStore = await FaissStore.fromTexts(\n  [\"dog\", \"cat\", \"horse\", \"seagull\"],\n  [{ id: 2 }, { id: 1 }, { id: 3 }, { id: 4 }],\n  embeddings\n);\n\nconst img = fs.readFileSync(\"parrot.jpeg\");\nconst vectors: number[] = await embeddings.embedImageQuery(img);\nconst document = new Document({\n  pageContent: img.toString(\"base64\"),\n  // Metadata is optional but helps track what kind of document is being retrieved\n  metadata: {\n    id: 5,\n    mediaType: \"image\",\n  },\n});\n\n// Add the image embedding vectors to the vector store directly\nawait vectorStore.addVectors([vectors], [document]);\n\n// Use a similar image to the one just added\nconst img2 = fs.readFileSync(\"parrot-icon.png\");\nconst vectors2: number[] = await embeddings.embedImageQuery(img2);\n\n// Use the lower level, direct API\nconst resultTwo = await vectorStore.similaritySearchVectorWithScore(\n  vectors2,\n  2\n);\nconsole.log(JSON.stringify(resultTwo, null, 2));\n\n/*\n  [\n    [\n      Document {\n        pageContent: '<BASE64 ENCODED IMAGE DATA>'\n        metadata: {\n          id: 5,\n          mediaType: \"image\"\n        }\n      },\n      0.8931522965431213\n    ],\n    [\n      Document {\n        pageContent: 'seagull',\n        metadata: {\n          id: 4\n        }\n      },\n      1.9188631772994995\n    ]\n  ]\n*/","metadata":{"source":"examples/src/models/embeddings/googlevertexai_multimodal_advanced.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":61}}}}],["1672a6ef-5ded-4c50-b2fd-adb3d783d092",{"pageContent":"import { HuggingFaceTransformersEmbeddings } from \"langchain/embeddings/hf_transformers\";\n\nconst model = new HuggingFaceTransformersEmbeddings({\n  modelName: \"Xenova/all-MiniLM-L6-v2\",\n});\n\n/* Embed queries */\nconst res = await model.embedQuery(\n  \"What would be a good company name for a company that makes colorful socks?\"\n);\nconsole.log({ res });\n/* Embed documents */\nconst documentRes = await model.embedDocuments([\"Hello world\", \"Bye bye\"]);\nconsole.log({ documentRes });","metadata":{"source":"examples/src/models/embeddings/hf_transformers.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":14}}}}],["bd6dbfec-8c60-4403-9170-4031449f37b9",{"pageContent":"import { MinimaxEmbeddings } from \"langchain/embeddings/minimax\";\n\nexport const run = async () => {\n  /* Embed queries */\n  const embeddings = new MinimaxEmbeddings();\n  const res = await embeddings.embedQuery(\"Hello world\");\n  console.log(res);\n  /* Embed documents */\n  const documentRes = await embeddings.embedDocuments([\n    \"Hello world\",\n    \"Bye bye\",\n  ]);\n  console.log({ documentRes });\n};","metadata":{"source":"examples/src/models/embeddings/minimax.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":14}}}}],["29c3b80b-cfa7-461d-a068-425b92a4355a",{"pageContent":"import { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nexport const run = async () => {\n  /* Embed queries */\n  const embeddings = new OpenAIEmbeddings();\n  const res = await embeddings.embedQuery(\"Hello world\");\n  console.log(res);\n  /* Embed documents */\n  const documentRes = await embeddings.embedDocuments([\n    \"Hello world\",\n    \"Bye bye\",\n  ]);\n  console.log({ documentRes });\n};","metadata":{"source":"examples/src/models/embeddings/openai.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":14}}}}],["e413999e-a840-4570-902a-9ea27bf7bbb4",{"pageContent":"import { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nconst embeddings = new OpenAIEmbeddings({\n  timeout: 1000, // 1s timeout\n});\n/* Embed queries */\nconst res = await embeddings.embedQuery(\"Hello world\");\nconsole.log(res);\n/* Embed documents */\nconst documentRes = await embeddings.embedDocuments([\"Hello world\", \"Bye bye\"]);\nconsole.log({ documentRes });","metadata":{"source":"examples/src/models/embeddings/openai_timeout.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":11}}}}],["921fdc69-8638-4076-92b6-8bab33b79f10",{"pageContent":"import \"@tensorflow/tfjs-backend-cpu\";\nimport { Document } from \"langchain/document\";\nimport { TensorFlowEmbeddings } from \"langchain/embeddings/tensorflow\";\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n\nconst embeddings = new TensorFlowEmbeddings();\nconst store = new MemoryVectorStore(embeddings);\n\nconst documents = [\n  \"A document\",\n  \"Some other piece of text\",\n  \"One more\",\n  \"And another\",\n];\n\nawait store.addDocuments(\n  documents.map((pageContent) => new Document({ pageContent }))\n);","metadata":{"source":"examples/src/models/embeddings/tensorflow.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":18}}}}],["2470b16e-ae1d-4183-ab5d-a3e664c622ff",{"pageContent":"import { AI21 } from \"langchain/llms/ai21\";\n\nconst model = new AI21({\n  ai21ApiKey: \"YOUR_AI21_API_KEY\", // Or set as process.env.AI21_API_KEY\n});\n\nconst res = await model.call(`Translate \"I love programming\" into German.`);\n\nconsole.log({ res });\n\n/*\n  {\n    res: \"\\nIch liebe das Programmieren.\"\n  }\n */","metadata":{"source":"examples/src/models/llm/ai21.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":15}}}}],["6f071e3b-2eed-4d68-b84b-40183c5bd8b9",{"pageContent":"import { AlephAlpha } from \"langchain/llms/aleph_alpha\";\n\nconst model = new AlephAlpha({\n  aleph_alpha_api_key: \"YOUR_ALEPH_ALPHA_API_KEY\", // Or set as process.env.ALEPH_ALPHA_API_KEY\n});\n\nconst res = await model.call(`Is cereal soup?`);\n\nconsole.log({ res });\n\n/*\n  {\n    res: \"\\nIs soup a cereal? I don’t think so, but it is delicious.\"\n  }\n */","metadata":{"source":"examples/src/models/llm/aleph_alpha.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":15}}}}],["4f148aed-554c-4aaf-8484-5fc658197e76",{"pageContent":"import { Bedrock } from \"langchain/llms/bedrock\";\n// Or, from web environments:\n// import { Bedrock } from \"langchain/llms/bedrock/web\";\n\n// If no credentials are provided, the default credentials from\n// @aws-sdk/credential-provider-node will be used.\nconst model = new Bedrock({\n  model: \"ai21.j2-grande-instruct\", // You can also do e.g. \"anthropic.claude-v2\"\n  region: \"us-east-1\",\n  // endpointUrl: \"custom.amazonaws.com\",\n  // credentials: {\n  //   accessKeyId: process.env.BEDROCK_AWS_ACCESS_KEY_ID!,\n  //   secretAccessKey: process.env.BEDROCK_AWS_SECRET_ACCESS_KEY!,\n  // },\n  // modelKwargs: {},\n});\n\nconst res = await model.invoke(\"Tell me a joke\");\nconsole.log(res);\n\n/*\n\n\n  Why was the math book unhappy?\n\n  Because it had too many problems!\n*/","metadata":{"source":"examples/src/models/llm/bedrock.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":27}}}}],["e6b4e6c4-2b9f-4316-88dd-3f0437eea5af",{"pageContent":"import { CloudflareWorkersAI } from \"langchain/llms/cloudflare_workersai\";\n\nconst model = new CloudflareWorkersAI({\n  model: \"@cf/meta/llama-2-7b-chat-int8\", // Default value\n  cloudflareAccountId: process.env.CLOUDFLARE_ACCOUNT_ID,\n  cloudflareApiToken: process.env.CLOUDFLARE_API_TOKEN,\n  // Pass a custom base URL to use Cloudflare AI Gateway\n  // baseUrl: `https://gateway.ai.cloudflare.com/v1/{YOUR_ACCOUNT_ID}/{GATEWAY_NAME}/workers-ai/`,\n});\n\nconst response = await model.invoke(\n  `Translate \"I love programming\" into German.`\n);\n\nconsole.log(response);\n\n/*\n Here are a few options:\n\n1. \"Ich liebe Programmieren\" - This is the most common way to say \"I love programming\" in German. \"Liebe\" means \"love\" in German, and \"Programmieren\" means \"programming\".\n2. \"Programmieren macht mir Spaß\" - This means \"Programming makes me happy\". This is a more casual way to express your love for programming in German.\n3. \"Ich bin ein großer Fan von Programmieren\" - This means \"I'm a big fan of programming\". This is a more formal way to express your love for programming in German.\n4. \"Programmieren ist mein Hobby\" - This means \"Programming is my hobby\". This is a more casual way to express your love for programming in German.\n5. \"Ich liebe es, Programme zu schreiben\" - This means \"I love writing programs\". This is a more formal way to express your love for programming in German.\n*/\n\nconst stream = await model.stream(\n  `Translate \"I love programming\" into German.`\n);\n\nfor await (const chunk of stream) {\n  console.log(chunk);\n}\n\n/*\n  Here\n  are\n  a\n  few\n  options\n  :\n\n\n\n\n  1\n  .\n  \"\n  I\n  ch\n  lie\n  be\n  Program\n  ...\n*/","metadata":{"source":"examples/src/models/llm/cloudflare_workersai.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":55}}}}],["d147cbbb-ded4-4f9e-afd8-fcbef437640c",{"pageContent":"import { FakeListLLM } from \"langchain/llms/fake\";\n\n/**\n * The FakeListLLM can be used to simulate ordered predefined responses.\n */\n\nconst llm = new FakeListLLM({\n  responses: [\"I'll callback later.\", \"You 'console' them!\"],\n});\n\nconst firstResponse = await llm.call(\"You want to hear a JavasSript joke?\");\nconst secondResponse = await llm.call(\n  \"How do you cheer up a JavaScript developer?\"\n);\n\nconsole.log({ firstResponse });\nconsole.log({ secondResponse });\n\n/**\n * The FakeListLLM can also be used to simulate streamed responses.\n */\n\nconst stream = await llm.stream(\"You want to hear a JavasSript joke?\");\nconst chunks = [];\nfor await (const chunk of stream) {\n  chunks.push(chunk);\n}\n\nconsole.log(chunks.join(\"\"));\n\n/**\n * The FakeListLLM can also be used to simulate delays in either either synchronous or streamed responses.\n */\n\nconst slowLLM = new FakeListLLM({\n  responses: [\"Because Oct 31 equals Dec 25\", \"You 'console' them!\"],\n  sleep: 1000,\n});\n\nconst slowResponse = await slowLLM.call(\n  \"Why do programmers always mix up Halloween and Christmas?\"\n);\nconsole.log({ slowResponse });\n\nconst slowStream = await slowLLM.stream(\n  \"How do you cheer up a JavaScript developer?\"\n);\nconst slowChunks = [];\nfor await (const chunk of slowStream) {\n  slowChunks.push(chunk);\n}\n\nconsole.log(slowChunks.join(\"\"));","metadata":{"source":"examples/src/models/llm/fake.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":53}}}}],["994adffb-297d-4250-8b05-0881777d1a6e",{"pageContent":"import { Fireworks } from \"langchain/llms/fireworks\";\n\nconst model = new Fireworks({\n  temperature: 0.9,\n  // In Node.js defaults to process.env.FIREWORKS_API_KEY\n  fireworksApiKey: \"YOUR-API-KEY\",\n});","metadata":{"source":"examples/src/models/llm/fireworks.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":7}}}}],["9f3117c3-fffd-43c2-b063-485647e7d100",{"pageContent":"import { GooglePaLM } from \"langchain/llms/googlepalm\";\n\nexport const run = async () => {\n  const model = new GooglePaLM({\n    apiKey: \"<YOUR API KEY>\", // or set it in environment variable as `GOOGLE_PALM_API_KEY`\n    // other params\n    temperature: 1, // OPTIONAL\n    modelName: \"models/text-bison-001\", // OPTIONAL\n    maxOutputTokens: 1024, // OPTIONAL\n    topK: 40, // OPTIONAL\n    topP: 3, // OPTIONAL\n    safetySettings: [\n      // OPTIONAL\n      {\n        category: \"HARM_CATEGORY_DANGEROUS\",\n        threshold: \"BLOCK_MEDIUM_AND_ABOVE\",\n      },\n    ],\n    stopSequences: [\"stop\"], // OPTIONAL\n  });\n  const res = await model.call(\n    \"What would be a good company name for a company that makes colorful socks?\"\n  );\n  console.log({ res });\n};","metadata":{"source":"examples/src/models/llm/googlepalm.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":25}}}}],["492eb06e-ee6e-4b58-939f-a3e81220ec01",{"pageContent":"import { LlamaCpp } from \"langchain/llms/llama_cpp\";\n\nconst llamaPath = \"/Replace/with/path/to/your/model/gguf-llama2-q4_0.bin\";\nconst question = \"Where do Llamas come from?\";\n\nconst model = new LlamaCpp({ modelPath: llamaPath });\n\nconsole.log(`You: ${question}`);\nconst response = await model.call(question);\nconsole.log(`AI : ${response}`);","metadata":{"source":"examples/src/models/llm/llama_cpp.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":10}}}}],["d69d0cb8-a80c-4f5e-9e60-934d2a4a274a",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\n\nexport const run = async () => {\n  const modelA = new OpenAI();\n  // `call` is a simple string-in, string-out method for interacting with the model.\n  const resA = await modelA.call(\n    \"What would be a good company name a company that makes colorful socks?\"\n  );\n  console.log({ resA });\n  // { resA: '\\n\\nSocktastic Colors' }\n\n  // `generate` allows you to generate multiple completions for multiple prompts (in a single request for some models).\n  const resB = await modelA.generate([\n    \"What would be a good company name a company that makes colorful socks?\",\n    \"What would be a good company name a company that makes colorful sweaters?\",\n  ]);\n\n  // `resB` is a `LLMResult` object with a `generations` field and `llmOutput` field.\n  // `generations` is a `Generation[][]`, each `Generation` having a `text` field.\n  // Each input to the LLM could have multiple generations (depending on the `n` parameter), hence the list of lists.\n  console.log(JSON.stringify(resB, null, 2));\n  /*\n  {\n      \"generations\": [\n          [{\n              \"text\": \"\\n\\nVibrant Socks Co.\",\n              \"generationInfo\": {\n                  \"finishReason\": \"stop\",\n                  \"logprobs\": null\n              }\n          }],\n          [{\n              \"text\": \"\\n\\nRainbow Knitworks.\",\n              \"generationInfo\": {\n                  \"finishReason\": \"stop\",\n                  \"logprobs\": null\n              }\n          }]\n      ],\n      \"llmOutput\": {\n          \"tokenUsage\": {\n              \"completionTokens\": 17,\n              \"promptTokens\": 29,\n              \"totalTokens\": 46\n          }\n      }\n  }\n  */\n\n  // We can specify additional parameters the specific model provider supports, like `temperature`:\n  const modelB = new OpenAI({ temperature: 0.9 });\n  const resC = await modelA.call(\n    \"What would be a good company name a company that makes colorful socks?\"\n  );\n  console.log({ resC });\n  // { resC: '\\n\\nKaleidoSox' }\n\n  // We can get the number of tokens for a given input for a specific model.\n  const numTokens = modelB.getNumTokens(\"How many tokens are in this input?\");\n  console.log({ numTokens });\n  // { numTokens: 8 }\n};","metadata":{"source":"examples/src/models/llm/llm.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":62}}}}],["d90cd747-be24-486d-8c41-0c3b44bbecb0",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\n\nexport const run = async () => {\n  const model = new OpenAI({\n    // customize openai model that's used, `text-davinci-003` is the default\n    modelName: \"text-ada-001\",\n\n    // `max_tokens` supports a magic -1 param where the max token length for the specified modelName\n    //  is calculated and included in the request to OpenAI as the `max_tokens` param\n    maxTokens: -1,\n\n    // use `modelKwargs` to pass params directly to the openai call\n    // note that they use snake_case instead of camelCase\n    modelKwargs: {\n      user: \"me\",\n    },\n\n    // for additional logging for debugging purposes\n    verbose: true,\n  });\n\n  const resA = await model.call(\n    \"What would be a good company name a company that makes colorful socks?\"\n  );\n  console.log({ resA });\n  // { resA: '\\n\\nSocktastic Colors' }\n};","metadata":{"source":"examples/src/models/llm/llm_advanced.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":27}}}}],["0b1032de-d452-4969-b078-525fb9cb9b3a",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\n\nconst model = new OpenAI({ temperature: 1 });\nconst controller = new AbortController();\n\n// Call `controller.abort()` somewhere to cancel the request.\n\nconst res = await model.call(\n  \"What would be a good name for a company that makes colorful socks?\",\n  { signal: controller.signal }\n);\n\nconsole.log(res);\n/*\n'\\n\\nSocktastic Colors'\n*/","metadata":{"source":"examples/src/models/llm/llm_cancellation.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":16}}}}],["9ac55c9d-3c83-45ca-b5e0-458a784d3a16",{"pageContent":"import { LLMResult } from \"langchain/schema\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { Serialized } from \"langchain/load/serializable\";\n\n// We can pass in a list of CallbackHandlers to the LLM constructor to get callbacks for various events.\nconst model = new OpenAI({\n  callbacks: [\n    {\n      handleLLMStart: async (llm: Serialized, prompts: string[]) => {\n        console.log(JSON.stringify(llm, null, 2));\n        console.log(JSON.stringify(prompts, null, 2));\n      },\n      handleLLMEnd: async (output: LLMResult) => {\n        console.log(JSON.stringify(output, null, 2));\n      },\n      handleLLMError: async (err: Error) => {\n        console.error(err);\n      },\n    },\n  ],\n});\n\nawait model.call(\n  \"What would be a good company name a company that makes colorful socks?\"\n);\n// {\n//     \"name\": \"openai\"\n// }\n// [\n//     \"What would be a good company name a company that makes colorful socks?\"\n// ]\n// {\n//   \"generations\": [\n//     [\n//         {\n//             \"text\": \"\\n\\nSocktastic Splashes.\",\n//             \"generationInfo\": {\n//                 \"finishReason\": \"stop\",\n//                 \"logprobs\": null\n//             }\n//         }\n//     ]\n//  ],\n//   \"llmOutput\": {\n//     \"tokenUsage\": {\n//         \"completionTokens\": 9,\n//          \"promptTokens\": 14,\n//          \"totalTokens\": 23\n//     }\n//   }\n// }","metadata":{"source":"examples/src/models/llm/llm_debugging.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":51}}}}],["5cf3295a-101e-4de4-b2d4-b10e2ba8668d",{"pageContent":"import { PromptLayerOpenAI } from \"langchain/llms/openai\";\n\nexport const run = async () => {\n  const model = new PromptLayerOpenAI({ temperature: 0.9 });\n  const res = await model.call(\n    \"What would be a good company name a company that makes colorful socks?\"\n  );\n  console.log({ res });\n};","metadata":{"source":"examples/src/models/llm/llm_promptlayer.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":9}}}}],["666d772a-a68e-4435-a9ae-c2e8527a0a8c",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\n\nexport const run = async () => {\n  const model = new OpenAI();\n  // `call` is a simple string-in, string-out method for interacting with the model.\n  const resA = await model.call(\n    \"What would be a good company name a company that makes colorful socks?\"\n  );\n  console.log({ resA });\n  // { resA: '\\n\\nSocktastic Colors' }\n};","metadata":{"source":"examples/src/models/llm/llm_quick_start.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":11}}}}],["0fb4d744-4254-4d8f-9553-2750815c6e1f",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\n\n// To enable streaming, we pass in `streaming: true` to the LLM constructor.\n// Additionally, we pass in a handler for the `handleLLMNewToken` event.\nconst model = new OpenAI({\n  maxTokens: 25,\n  streaming: true,\n});\n\nconst response = await model.call(\"Tell me a joke.\", {\n  callbacks: [\n    {\n      handleLLMNewToken(token: string) {\n        console.log({ token });\n      },\n    },\n  ],\n});\nconsole.log(response);\n/*\n{ token: '\\n' }\n{ token: '\\n' }\n{ token: 'Q' }\n{ token: ':' }\n{ token: ' Why' }\n{ token: ' did' }\n{ token: ' the' }\n{ token: ' chicken' }\n{ token: ' cross' }\n{ token: ' the' }\n{ token: ' playground' }\n{ token: '?' }\n{ token: '\\n' }\n{ token: 'A' }\n{ token: ':' }\n{ token: ' To' }\n{ token: ' get' }\n{ token: ' to' }\n{ token: ' the' }\n{ token: ' other' }\n{ token: ' slide' }\n{ token: '.' }\n\n\nQ: Why did the chicken cross the playground?\nA: To get to the other slide.\n*/","metadata":{"source":"examples/src/models/llm/llm_streaming.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":47}}}}],["7b69b7d8-6dd2-4c36-9016-172cbd47ab1f",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\n\n// To enable streaming, we pass in `streaming: true` to the LLM constructor.\n// Additionally, we pass in a handler for the `handleLLMNewToken` event.\nconst chat = new OpenAI({\n  streaming: true,\n  callbacks: [\n    {\n      handleLLMNewToken(token: string) {\n        process.stdout.write(token);\n      },\n    },\n  ],\n});\n\nawait chat.call(\"Write me a song about sparkling water.\");\n/*\nVerse 1\nCrystal clear and made with care\nSparkling water on my lips, so refreshing in the air\nFizzy bubbles, light and sweet\nMy favorite beverage I can’t help but repeat\n\nChorus\nA toast to sparkling water, I’m feeling so alive\nLet’s take a sip, and let’s take a drive\nA toast to sparkling water, it’s the best I’ve had in my life\nIt’s the best way to start off the night\n\nVerse 2\nIt’s the perfect drink to quench my thirst\nIt’s the best way to stay hydrated, it’s the first\nA few ice cubes, a splash of lime\nIt will make any day feel sublime\n...\n*/","metadata":{"source":"examples/src/models/llm/llm_streaming_stdout.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":36}}}}],["4693478d-7764-4e4e-9419-ec67eb415d63",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\n\nconst model = new OpenAI({\n  maxTokens: 25,\n});\n\nconst stream = await model.stream(\"Tell me a joke.\");\n\nfor await (const chunk of stream) {\n  console.log(chunk);\n}\n\n/*\n\n\nQ\n:\n What\n did\n the\n fish\n say\n when\n it\n hit\n the\n wall\n?\n\n\nA\n:\n Dam\n!\n*/","metadata":{"source":"examples/src/models/llm/llm_streaming_stream_method.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":35}}}}],["75481c50-b8b9-45b7-a981-cf8b6573856f",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\n\nconst model = new OpenAI({ temperature: 1 });\n\nconst resA = await model.call(\n  \"What would be a good company name a company that makes colorful socks?\",\n  { timeout: 1000 } // 1s timeout\n);\n\nconsole.log({ resA });\n// '\\n\\nSocktastic Colors' }","metadata":{"source":"examples/src/models/llm/llm_timeout.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":11}}}}],["fc8b2ab8-02a2-4329-b4c3-06e40494d55a",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { SystemMessage, HumanMessage } from \"langchain/schema\";\nimport * as process from \"process\";\n\nexport const run = async () => {\n  process.env.LANGCHAIN_HANDLER = \"langchain\";\n  const model = new OpenAI({ temperature: 0.9 });\n  const resA = await model.call(\n    \"What would be a good company name a company that makes colorful socks?\"\n  );\n  console.log({ resA });\n\n  const chat = new ChatOpenAI({ temperature: 0 });\n  const system_message = new SystemMessage(\"You are to chat with a user.\");\n  const message = new HumanMessage(\"Hello!\");\n  const resB = await chat.call([system_message, message]);\n  console.log({ resB });\n};","metadata":{"source":"examples/src/models/llm/llm_with_tracing.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":19}}}}],["8ef71ade-c574-49e0-98d1-46de6183d0e9",{"pageContent":"import { NIBittensorLLM } from \"langchain/experimental/llms/bittensor\";\n\nconst model = new NIBittensorLLM();\n\nconst res = await model.call(`What is Bittensor?`);\n\nconsole.log({ res });\n\n/*\n  {\n    res: \"\\nBittensor is opensource protocol...\"\n  }\n */","metadata":{"source":"examples/src/models/llm/ni_bittensor.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":13}}}}],["01373a58-0b54-4ce1-a24a-535377472a10",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { HumanMessage } from \"langchain/schema\";\nimport process from \"process\";\n\nprocess.env.LANGCHAIN_TRACING_V2 = \"true\";\n\nconst model = new OpenAI({});\n\nconst prompts = [\n  \"Say hello to Bob.\",\n  \"Say hello to Alice.\",\n  \"Say hello to John.\",\n  \"Say hello to Mary.\",\n];\n\nconst res = await model.generate(prompts);\nconsole.log({ res });\n\nconst chat = new ChatOpenAI({\n  modelName: \"gpt-3.5-turbo\",\n});\n\nconst messages = prompts.map((prompt) => [new HumanMessage(prompt)]);\n\nconst res2 = await chat.generate(messages);\nconsole.log({ res2 });","metadata":{"source":"examples/src/models/llm/openai-batch.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":27}}}}],["4f84a6c0-b76a-4155-9e66-5d31cb30a5be",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\n\nconst model = new OpenAI(\n  { temperature: 0 },\n  { baseURL: \"https://oai.hconeai.com/v1\" }\n);\n\nconst res = await model.call(\n  \"What would be a good company name a company that makes colorful socks?\"\n);\nconsole.log(res);","metadata":{"source":"examples/src/models/llm/openai_basePath.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":11}}}}],["224c7ef4-d1b3-48dd-8743-e7e07f089a38",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\n\nconst model = new OpenAI({ temperature: 0 });\n\nconst res = await model.call(\n  \"What would be a good company name a company that makes colorful socks?\",\n  {\n    options: {\n      headers: {\n        \"User-Id\": \"123\",\n      },\n    },\n  }\n);\nconsole.log(res);","metadata":{"source":"examples/src/models/llm/openai_userid.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":15}}}}],["dcd8b730-c324-4619-bb5d-e9f378a67e2e",{"pageContent":"import { RaycastAI } from \"langchain/llms/raycast\";\n\nimport { showHUD } from \"@raycast/api\";\nimport { Tool } from \"langchain/tools\";\nimport { initializeAgentExecutorWithOptions } from \"langchain/agents\";\n\nconst model = new RaycastAI({\n  rateLimitPerMinute: 10, // It is 10 by default so you can omit this line\n  model: \"gpt-3.5-turbo\",\n  creativity: 0, // `creativity` is a term used by Raycast which is equivalent to `temperature` in some other LLMs\n});\n\nconst tools: Tool[] = [\n  // Add your tools here\n];\n\nexport default async function main() {\n  // Initialize the agent executor with RaycastAI model\n  const executor = await initializeAgentExecutorWithOptions(tools, model, {\n    agentType: \"chat-conversational-react-description\",\n  });\n\n  const input = `Describe my today's schedule as Gabriel Garcia Marquez would describe it`;\n\n  const answer = await executor.call({ input });\n\n  await showHUD(answer.output);\n}","metadata":{"source":"examples/src/models/llm/raycast.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":28}}}}],["2e94253a-fc12-44ae-9192-c3b960550d3b",{"pageContent":"import { Replicate } from \"langchain/llms/replicate\";\n\nconst modelA = new Replicate({\n  model:\n    \"a16z-infra/llama13b-v2-chat:df7690f1994d94e96ad9d568eac121aecf50684a0b0963b25a41cc40061269e5\",\n});\n\n// `call` is a simple string-in, string-out method for interacting with the model.\nconst resA = await modelA.call(\n  \"What would be a good company name a company that makes colorful socks?\"\n);\nconsole.log({ resA });\n/*\n{\n  resA: 'Color Box'\n}\n*/\n\n// `generate` allows you to generate multiple completions for multiple prompts (in a single request for some models).\nconst resB = await modelA.generate([\n  \"What would be a good company name a company that makes colorful socks?\",\n  \"What would be a good company name a company that makes colorful sweaters?\",\n]);\n// `resB` is a `LLMResult` object with a `generations` field and `llmOutput` field.\n// `generations` is a `Generation[][]`, each `Generation` having a `text` field.\n// Each input to the LLM could have multiple generations (depending on the `n` parameter), hence the list of lists.\nconsole.log(JSON.stringify(resB, null, 2));\n/*\n{\n  \"generations\": [\n    [\n      {\n        \"text\": \"apron string\"\n      }\n    ],\n    [\n      {\n        \"text\": \"Kulut\"\n      }\n    ]\n  ]\n}\n*/\n\nconst text2image = new Replicate({\n  model:\n    \"stability-ai/stable-diffusion:db21e45d3f7023abc2a46ee38a23973f6dce16bb082a930b0c49861f96d1e5bf\",\n});\n\nconst image = await text2image.call(\"A cat\");\nconsole.log({ image });\n/*\n{\n  \"image\": \"https://replicate.delivery/pbxt/Nc8qkJ8zkdpDPdNSYuMaDErImcXVMUAybFrLk9Kane7IKOWIA/out-0.png\"\n}\n*/","metadata":{"source":"examples/src/models/llm/replicate.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":56}}}}],["4e1b8949-7143-42e6-8322-5ab800b1fd14",{"pageContent":"import { Replicate } from \"langchain/llms/replicate\";\n\nconst model = new Replicate({\n  model:\n    \"a16z-infra/llama13b-v2-chat:df7690f1994d94e96ad9d568eac121aecf50684a0b0963b25a41cc40061269e5\",\n});\n\nconst prompt = `\nUser: How much wood would a woodchuck chuck if a wood chuck could chuck wood?\nAssistant:`;\n\nconst res = await model.call(prompt);\nconsole.log({ res });\n/*\n  {\n    res: \"I'm happy to help! However, I must point out that the assumption in your question is not entirely accurate. \" +\n      + \"Woodchucks, also known as groundhogs, do not actually chuck wood. They are burrowing animals that primarily \" +\n      \"feed on grasses, clover, and other vegetation. They do not have the physical ability to chuck wood.\\n\" +\n      '\\n' +\n      'If you have any other questions or if there is anything else I can assist you with, please feel free to ask!'\n  }\n*/","metadata":{"source":"examples/src/models/llm/replicate_llama2.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":22}}}}],["b5a00b30-5ac7-4646-86e3-fd995e9eeab4",{"pageContent":"import {\n  SageMakerEndpoint,\n  SageMakerLLMContentHandler,\n} from \"langchain/llms/sagemaker_endpoint\";\n\ninterface ResponseJsonInterface {\n  generation: {\n    content: string;\n  };\n}\n\n// Custom for whatever model you'll be using\nclass LLama213BHandler implements SageMakerLLMContentHandler {\n  contentType = \"application/json\";\n\n  accepts = \"application/json\";\n\n  async transformInput(\n    prompt: string,\n    modelKwargs: Record<string, unknown>\n  ): Promise<Uint8Array> {\n    const payload = {\n      inputs: [[{ role: \"user\", content: prompt }]],\n      parameters: modelKwargs,\n    };\n\n    const stringifiedPayload = JSON.stringify(payload);\n\n    return new TextEncoder().encode(stringifiedPayload);\n  }\n\n  async transformOutput(output: Uint8Array): Promise<string> {\n    const response_json = JSON.parse(\n      new TextDecoder(\"utf-8\").decode(output)\n    ) as ResponseJsonInterface[];\n    const content = response_json[0]?.generation.content ?? \"\";\n    return content;\n  }\n}\n\nconst contentHandler = new LLama213BHandler();\n\nconst model = new SageMakerEndpoint({\n  endpointName: \"aws-llama-2-13b-chat\",\n  modelKwargs: {\n    temperature: 0.5,\n    max_new_tokens: 700,\n    top_p: 0.9,\n  },\n  endpointKwargs: {\n    CustomAttributes: \"accept_eula=true\",\n  },\n  contentHandler,\n  clientOptions: {\n    region: \"YOUR AWS ENDPOINT REGION\",\n    credentials: {\n      accessKeyId: \"YOUR AWS ACCESS ID\",\n      secretAccessKey: \"YOUR AWS SECRET ACCESS KEY\",\n    },\n  },\n});\n\nconst res = await model.call(\n  \"Hello, my name is John Doe, tell me a joke about llamas \"\n);\n\nconsole.log(res);\n\n/*\n  [\n    {\n      content: \"Hello, John Doe! Here's a llama joke for you:\n        Why did the llama become a gardener?\n        Because it was great at llama-scaping!\"\n    }\n  ]\n */","metadata":{"source":"examples/src/models/llm/sagemaker_endpoint.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":77}}}}],["ddeed930-cac3-410d-a042-bd425fbca9f9",{"pageContent":"import { Writer } from \"langchain/llms/writer\";\n\nconst model = new Writer({\n  maxTokens: 20,\n  apiKey: \"YOUR-API-KEY\", // In Node.js defaults to process.env.WRITER_API_KEY\n  orgId: \"YOUR-ORGANIZATION-ID\", // In Node.js defaults to process.env.WRITER_ORG_ID\n});\nconst res = await model.invoke(\n  \"What would be a good company name a company that makes colorful socks?\"\n);\nconsole.log({ res });","metadata":{"source":"examples/src/models/llm/writer.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":11}}}}],["7864064e-29d2-4052-ba19-9f2bb5a1b376",{"pageContent":"import { YandexGPT } from \"langchain/llms/yandex\";\n\nconst model = new YandexGPT();\n\nconst res = await model.call('Translate \"I love programming\" into French.');\n\nconsole.log({ res });","metadata":{"source":"examples/src/models/llm/yandex.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":7}}}}],["a6cdae0a-07f6-4c36-8e4c-cb04f3cd4ef6",{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { BytesOutputParser } from \"langchain/schema/output_parser\";\n\nconst handler = async () => {\n  const parser = new BytesOutputParser();\n\n  const model = new ChatOpenAI({ temperature: 0 });\n\n  const stream = await model.pipe(parser).stream(\"Hello there!\");\n\n  const httpResponse = new Response(stream, {\n    headers: {\n      \"Content-Type\": \"text/plain; charset=utf-8\",\n    },\n  });\n\n  return httpResponse;\n};\n\nawait handler();","metadata":{"source":"examples/src/prompts/bytes_output_parser.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":20}}}}],["5d82ebef-9b05-4cb8-9f9e-c9d98b27edf6",{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { BytesOutputParser } from \"langchain/schema/output_parser\";\nimport { RunnableSequence } from \"langchain/schema/runnable\";\n\nconst chain = RunnableSequence.from([\n  new ChatOpenAI({ temperature: 0 }),\n  new BytesOutputParser(),\n]);\n\nconst stream = await chain.stream(\"Hello there!\");\n\nconst decoder = new TextDecoder();\n\nfor await (const chunk of stream) {\n  if (chunk) {\n    console.log(decoder.decode(chunk));\n  }\n}","metadata":{"source":"examples/src/prompts/bytes_output_parser_sequence.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":18}}}}],["33af14e5-1db8-4141-bbaf-22b07d19ae20",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport {\n  StructuredOutputParser,\n  RegexParser,\n  CombiningOutputParser,\n} from \"langchain/output_parsers\";\n\nconst answerParser = StructuredOutputParser.fromNamesAndDescriptions({\n  answer: \"answer to the user's question\",\n  source: \"source used to answer the user's question, should be a website.\",\n});\n\nconst confidenceParser = new RegexParser(\n  /Confidence: (A|B|C), Explanation: (.*)/,\n  [\"confidence\", \"explanation\"],\n  \"noConfidence\"\n);\n\nconst parser = new CombiningOutputParser(answerParser, confidenceParser);\nconst formatInstructions = parser.getFormatInstructions();\n\nconst prompt = new PromptTemplate({\n  template:\n    \"Answer the users question as best as possible.\\n{format_instructions}\\n{question}\",\n  inputVariables: [\"question\"],\n  partialVariables: { format_instructions: formatInstructions },\n});\n\nconst model = new OpenAI({ temperature: 0 });\n\nconst input = await prompt.format({\n  question: \"What is the capital of France?\",\n});\nconst response = await model.call(input);\n\nconsole.log(input);\n/*\nAnswer the users question as best as possible.\nReturn the following outputs, each formatted as described below:\n\nOutput 1:\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\n\nAs an example, for the schema {{\"properties\": {{\"foo\": {{\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {{\"type\": \"string\"}}}}}}, \"required\": [\"foo\"]}}}}\nthe object {{\"foo\": [\"bar\", \"baz\"]}} is a well-formatted instance of the schema. The object {{\"properties\": {{\"foo\": [\"bar\", \"baz\"]}}}} is not well-formatted.\n\nHere is the output schema:\n```\n{\"type\":\"object\",\"properties\":{\"answer\":{\"type\":\"string\",\"description\":\"answer to the user's question\"},\"source\":{\"type\":\"string\",\"description\":\"source used to answer the user's question, should be a website.\"}},\"required\":[\"answer\",\"source\"],\"additionalProperties\":false,\"$schema\":\"http://json-schema.org/draft-07/schema#\"}\n```\n\nOutput 2:\nYour response should match the following regex: /Confidence: (A|B|C), Explanation: (.*)/\n\nWhat is the capital of France?\n*/\n\nconsole.log(response);\n/*\nOutput 1:\n{\"answer\":\"Paris\",\"source\":\"https://www.worldatlas.com/articles/what-is-the-capital-of-france.html\"}\n\nOutput 2:\nConfidence: A, Explanation: The capital of France is Paris.\n*/\n\nconsole.log(await parser.parse(response));\n/*\n{\n  answer: 'Paris',\n  source: 'https://www.worldatlas.com/articles/what-is-the-capital-of-france.html',\n  confidence: 'A',\n  explanation: 'The capital of France is Paris.'\n}\n*/","metadata":{"source":"examples/src/prompts/combining_parser.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":76}}}}],["28df93eb-b2ac-41fe-8e8b-61278ac69cef",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport {\n  StructuredOutputParser,\n  RegexParser,\n  CombiningOutputParser,\n} from \"langchain/output_parsers\";\nimport { RunnableSequence } from \"langchain/schema/runnable\";\n\nconst answerParser = StructuredOutputParser.fromNamesAndDescriptions({\n  answer: \"answer to the user's question\",\n  source: \"source used to answer the user's question, should be a website.\",\n});\n\nconst confidenceParser = new RegexParser(\n  /Confidence: (A|B|C), Explanation: (.*)/,\n  [\"confidence\", \"explanation\"],\n  \"noConfidence\"\n);\n\nconst parser = new CombiningOutputParser(answerParser, confidenceParser);\n\nconst chain = RunnableSequence.from([\n  PromptTemplate.fromTemplate(\n    \"Answer the users question as best as possible.\\n{format_instructions}\\n{question}\"\n  ),\n  new OpenAI({ temperature: 0 }),\n  parser,\n]);\n\n/*\nAnswer the users question as best as possible.\nReturn the following outputs, each formatted as described below:\n\nOutput 1:\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\n\nAs an example, for the schema {{\"properties\": {{\"foo\": {{\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {{\"type\": \"string\"}}}}}}, \"required\": [\"foo\"]}}}}\nthe object {{\"foo\": [\"bar\", \"baz\"]}} is a well-formatted instance of the schema. The object {{\"properties\": {{\"foo\": [\"bar\", \"baz\"]}}}} is not well-formatted.\n\nHere is the output schema:\n```\n{\"type\":\"object\",\"properties\":{\"answer\":{\"type\":\"string\",\"description\":\"answer to the user's question\"},\"source\":{\"type\":\"string\",\"description\":\"source used to answer the user's question, should be a website.\"}},\"required\":[\"answer\",\"source\"],\"additionalProperties\":false,\"$schema\":\"http://json-schema.org/draft-07/schema#\"}\n```\n\nOutput 2:\nYour response should match the following regex: /Confidence: (A|B|C), Explanation: (.*)/\n\nWhat is the capital of France?\n*/\n\nconst response = await chain.invoke({\n  question: \"What is the capital of France?\",\n  format_instructions: parser.getFormatInstructions(),\n});\n\nconsole.log(response);\n/*\n{\n  answer: 'Paris',\n  source: 'https://www.worldatlas.com/articles/what-is-the-capital-of-france.html',\n  confidence: 'A',\n  explanation: 'The capital of France is Paris.'\n}\n*/","metadata":{"source":"examples/src/prompts/combining_parser_sequence.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":65}}}}],["5febb36f-dcd4-49a6-ae9e-32202c91b14f",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport { CommaSeparatedListOutputParser } from \"langchain/output_parsers\";\n\nexport const run = async () => {\n  // With a `CommaSeparatedListOutputParser`, we can parse a comma separated list.\n  const parser = new CommaSeparatedListOutputParser();\n\n  const formatInstructions = parser.getFormatInstructions();\n\n  const prompt = new PromptTemplate({\n    template: \"List five {subject}.\\n{format_instructions}\",\n    inputVariables: [\"subject\"],\n    partialVariables: { format_instructions: formatInstructions },\n  });\n\n  const model = new OpenAI({ temperature: 0 });\n\n  const input = await prompt.format({ subject: \"ice cream flavors\" });\n  const response = await model.call(input);\n\n  console.log(input);\n  /*\n   List five ice cream flavors.\n   Your response should be a list of comma separated values, eg: `foo, bar, baz`\n  */\n\n  console.log(response);\n  // Vanilla, Chocolate, Strawberry, Mint Chocolate Chip, Cookies and Cream\n\n  console.log(await parser.parse(response));\n  /*\n  [\n    'Vanilla',\n    'Chocolate',\n    'Strawberry',\n    'Mint Chocolate Chip',\n    'Cookies and Cream'\n  ]\n  */\n};","metadata":{"source":"examples/src/prompts/comma_list_parser.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":41}}}}],["9772c60c-6a71-4458-bda5-ae3aab6c7a01",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport { CommaSeparatedListOutputParser } from \"langchain/output_parsers\";\nimport { RunnableSequence } from \"langchain/schema/runnable\";\n\nexport const run = async () => {\n  // With a `CommaSeparatedListOutputParser`, we can parse a comma separated list.\n  const parser = new CommaSeparatedListOutputParser();\n\n  const chain = RunnableSequence.from([\n    PromptTemplate.fromTemplate(\"List five {subject}.\\n{format_instructions}\"),\n    new OpenAI({ temperature: 0 }),\n    parser,\n  ]);\n\n  /*\n   List five ice cream flavors.\n   Your response should be a list of comma separated values, eg: `foo, bar, baz`\n  */\n  const response = await chain.invoke({\n    subject: \"ice cream flavors\",\n    format_instructions: parser.getFormatInstructions(),\n  });\n\n  console.log(response);\n  /*\n\t\t\t[\n\t\t\t'Vanilla',\n\t\t\t'Chocolate',\n\t\t\t'Strawberry',\n\t\t\t'Mint Chocolate Chip',\n\t\t\t'Cookies and Cream'\n\t\t\t]\n\t\t*/\n};","metadata":{"source":"examples/src/prompts/comma_list_parser_sequence.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":35}}}}],["19800d40-0eff-4990-8ef9-86aa4a9e52e5",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport { CustomListOutputParser } from \"langchain/output_parsers\";\n\n// With a `CustomListOutputParser`, we can parse a list with a specific length and separator.\nconst parser = new CustomListOutputParser({ length: 3, separator: \"\\n\" });\n\nconst formatInstructions = parser.getFormatInstructions();\n\nconst prompt = new PromptTemplate({\n  template: \"Provide a list of {subject}.\\n{format_instructions}\",\n  inputVariables: [\"subject\"],\n  partialVariables: { format_instructions: formatInstructions },\n});\n\nconst model = new OpenAI({ temperature: 0 });\n\nconst input = await prompt.format({\n  subject: \"great fiction books (book, author)\",\n});\n\nconst response = await model.call(input);\n\nconsole.log(input);\n/*\nProvide a list of great fiction books (book, author).\nYour response should be a list of 3 items separated by \"\\n\" (eg: `foo\\n bar\\n baz`)\n*/\n\nconsole.log(response);\n/*\nThe Catcher in the Rye, J.D. Salinger\nTo Kill a Mockingbird, Harper Lee\nThe Great Gatsby, F. Scott Fitzgerald\n*/\n\nconsole.log(await parser.parse(response));\n/*\n[\n  'The Catcher in the Rye, J.D. Salinger',\n  'To Kill a Mockingbird, Harper Lee',\n  'The Great Gatsby, F. Scott Fitzgerald'\n]\n*/","metadata":{"source":"examples/src/prompts/custom_list_parser.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":44}}}}],["d6f84e51-14e2-45f7-86f8-c22c8aed13f8",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport { CustomListOutputParser } from \"langchain/output_parsers\";\nimport { RunnableSequence } from \"langchain/schema/runnable\";\n\n// With a `CustomListOutputParser`, we can parse a list with a specific length and separator.\nconst parser = new CustomListOutputParser({ length: 3, separator: \"\\n\" });\n\nconst chain = RunnableSequence.from([\n  PromptTemplate.fromTemplate(\n    \"Provide a list of {subject}.\\n{format_instructions}\"\n  ),\n  new OpenAI({ temperature: 0 }),\n  parser,\n]);\n\n/*\nProvide a list of great fiction books (book, author).\nYour response should be a list of 3 items separated by \"\\n\" (eg: `foo\\n bar\\n baz`)\n*/\nconst response = await chain.invoke({\n  subject: \"great fiction books (book, author)\",\n  format_instructions: parser.getFormatInstructions(),\n});\n\nconsole.log(response);\n/*\n[\n  'The Catcher in the Rye, J.D. Salinger',\n  'To Kill a Mockingbird, Harper Lee',\n  'The Great Gatsby, F. Scott Fitzgerald'\n]\n*/","metadata":{"source":"examples/src/prompts/custom_list_parser_sequence.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":33}}}}],["54481986-2be9-4ea8-9bb4-dbc9a05c1770",{"pageContent":"import { FewShotPromptTemplate, PromptTemplate } from \"langchain/prompts\";\n\nexport const run = async () => {\n  // First, create a list of few-shot examples.\n  const examples = [\n    { word: \"happy\", antonym: \"sad\" },\n    { word: \"tall\", antonym: \"short\" },\n  ];\n\n  // Next, we specify the template to format the examples we have provided.\n  const exampleFormatterTemplate = \"Word: {word}\\nAntonym: {antonym}\\n\";\n  const examplePrompt = new PromptTemplate({\n    inputVariables: [\"word\", \"antonym\"],\n    template: exampleFormatterTemplate,\n  });\n  // Finally, we create the `FewShotPromptTemplate`\n  const fewShotPrompt = new FewShotPromptTemplate({\n    /* These are the examples we want to insert into the prompt. */\n    examples,\n    /* This is how we want to format the examples when we insert them into the prompt. */\n    examplePrompt,\n    /* The prefix is some text that goes before the examples in the prompt. Usually, this consists of instructions. */\n    prefix: \"Give the antonym of every input\",\n    /* The suffix is some text that goes after the examples in the prompt. Usually, this is where the user input will go */\n    suffix: \"Word: {input}\\nAntonym:\",\n    /* The input variables are the variables that the overall prompt expects. */\n    inputVariables: [\"input\"],\n    /* The example_separator is the string we will use to join the prefix, examples, and suffix together with. */\n    exampleSeparator: \"\\n\\n\",\n    /* The template format is the formatting method to use for the template. Should usually be f-string. */\n    templateFormat: \"f-string\",\n  });\n\n  // We can now generate a prompt using the `format` method.\n  console.log(await fewShotPrompt.format({ input: \"big\" }));\n  /*\n  Give the antonym of every input\n\n  Word: happy\n  Antonym: sad\n\n\n  Word: tall\n  Antonym: short\n\n\n  Word: big\n  Antonym:\n  */\n};","metadata":{"source":"examples/src/prompts/few_shot.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":50}}}}],["a1b1fcdb-1481-4a38-947d-66423691ffd2",{"pageContent":"import { z } from \"zod\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport {\n  StructuredOutputParser,\n  OutputFixingParser,\n} from \"langchain/output_parsers\";\n\nexport const run = async () => {\n  const parser = StructuredOutputParser.fromZodSchema(\n    z.object({\n      answer: z.string().describe(\"answer to the user's question\"),\n      sources: z\n        .array(z.string())\n        .describe(\"sources used to answer the question, should be websites.\"),\n    })\n  );\n  /** This is a bad output because sources is a string, not a list */\n  const badOutput = `\\`\\`\\`json\n  {\n    \"answer\": \"foo\",\n    \"sources\": \"foo.com\"\n  }\n  \\`\\`\\``;\n  try {\n    await parser.parse(badOutput);\n  } catch (e) {\n    console.log(\"Failed to parse bad output: \", e);\n    /*\n    Failed to parse bad output:  OutputParserException [Error]: Failed to parse. Text: ```json\n      {\n        \"answer\": \"foo\",\n        \"sources\": \"foo.com\"\n      }\n      ```. Error: [\n      {\n        \"code\": \"invalid_type\",\n        \"expected\": \"array\",\n        \"received\": \"string\",\n        \"path\": [\n          \"sources\"\n        ],\n        \"message\": \"Expected array, received string\"\n      }\n    ]\n    at StructuredOutputParser.parse (/Users/ankushgola/Code/langchainjs/langchain/src/output_parsers/structured.ts:71:13)\n    at run (/Users/ankushgola/Code/langchainjs/examples/src/prompts/fix_parser.ts:25:18)\n    at <anonymous> (/Users/ankushgola/Code/langchainjs/examples/src/index.ts:33:22)\n   */\n  }\n  const fixParser = OutputFixingParser.fromLLM(\n    new ChatOpenAI({ temperature: 0 }),\n    parser\n  );\n  const output = await fixParser.parse(badOutput);\n  console.log(\"Fixed output: \", output);\n  // Fixed output:  { answer: 'foo', sources: [ 'foo.com' ] }\n};","metadata":{"source":"examples/src/prompts/fix_parser.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":57}}}}],["ded83938-2113-4565-b502-a9cf7aed932e",{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { JsonOutputFunctionsParser } from \"langchain/output_parsers\";\nimport { HumanMessage } from \"langchain/schema\";\n\n// Instantiate the parser\nconst parser = new JsonOutputFunctionsParser();\n\n// Define the function schema\nconst extractionFunctionSchema = {\n  name: \"extractor\",\n  description: \"Extracts fields from the input.\",\n  parameters: {\n    type: \"object\",\n    properties: {\n      tone: {\n        type: \"string\",\n        enum: [\"positive\", \"negative\"],\n        description: \"The overall tone of the input\",\n      },\n      word_count: {\n        type: \"number\",\n        description: \"The number of words in the input\",\n      },\n      chat_response: {\n        type: \"string\",\n        description: \"A response to the human's input\",\n      },\n    },\n    required: [\"tone\", \"word_count\", \"chat_response\"],\n  },\n};\n\n// Instantiate the ChatOpenAI class\nconst model = new ChatOpenAI({ modelName: \"gpt-4\" });\n\n// Create a new runnable, bind the function to the model, and pipe the output through the parser\nconst runnable = model\n  .bind({\n    functions: [extractionFunctionSchema],\n    function_call: { name: \"extractor\" },\n  })\n  .pipe(parser);\n\n// Invoke the runnable with an input\nconst result = await runnable.invoke([\n  new HumanMessage(\"What a beautiful day!\"),\n]);\n\nconsole.log({ result });\n\n/**\n{\n  result: {\n    tone: 'positive',\n    word_count: 4,\n    chat_response: \"Indeed, it's a lovely day!\"\n  }\n}\n */","metadata":{"source":"examples/src/prompts/json_structured_output_parser.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":59}}}}],["c87d0ebc-fefc-4754-87c8-ff748deca767",{"pageContent":"import { z } from \"zod\";\nimport { zodToJsonSchema } from \"zod-to-json-schema\";\n\nimport { ChatPromptTemplate } from \"langchain/prompts\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { JsonOutputFunctionsParser } from \"langchain/output_parsers\";\n\nconst schema = z.object({\n  setup: z.string().describe(\"The setup for the joke\"),\n  punchline: z.string().describe(\"The punchline to the joke\"),\n});\n\nconst modelParams = {\n  functions: [\n    {\n      name: \"joke\",\n      description: \"A joke\",\n      parameters: zodToJsonSchema(schema),\n    },\n  ],\n  function_call: { name: \"joke\" },\n};\n\nconst prompt = ChatPromptTemplate.fromTemplate(\n  `tell me a long joke about {foo}`\n);\nconst model = new ChatOpenAI({\n  temperature: 0,\n}).bind(modelParams);\n\nconst chain = prompt\n  .pipe(model)\n  .pipe(new JsonOutputFunctionsParser({ diff: true }));\n\nconst stream = await chain.stream({\n  foo: \"bears\",\n});\n\n// Stream a diff as JSON patch operations\nfor await (const chunk of stream) {\n  console.log(chunk);\n}\n\n/*\n  []\n  [ { op: 'add', path: '/setup', value: '' } ]\n  [ { op: 'replace', path: '/setup', value: 'Why' } ]\n  [ { op: 'replace', path: '/setup', value: 'Why don' } ]\n  [ { op: 'replace', path: '/setup', value: \"Why don't\" } ]\n  [ { op: 'replace', path: '/setup', value: \"Why don't bears\" } ]\n  [ { op: 'replace', path: '/setup', value: \"Why don't bears wear\" } ]\n  [\n    {\n      op: 'replace',\n      path: '/setup',\n      value: \"Why don't bears wear shoes\"\n    }\n  ]\n  [\n    {\n      op: 'replace',\n      path: '/setup',\n      value: \"Why don't bears wear shoes?\"\n    },\n    { op: 'add', path: '/punchline', value: '' }\n  ]\n  [ { op: 'replace', path: '/punchline', value: 'Because' } ]\n  [ { op: 'replace', path: '/punchline', value: 'Because they' } ]\n  [ { op: 'replace', path: '/punchline', value: 'Because they have' } ]\n  [\n    {\n      op: 'replace',\n      path: '/punchline',\n      value: 'Because they have bear'\n    }\n  ]\n  [\n    {\n      op: 'replace',\n      path: '/punchline',\n      value: 'Because they have bear feet'\n    }\n  ]\n  [\n    {\n      op: 'replace',\n      path: '/punchline',\n      value: 'Because they have bear feet!'\n    }\n  ]\n*/\n\nconst chain2 = prompt.pipe(model).pipe(new JsonOutputFunctionsParser());\n\nconst stream2 = await chain2.stream({\n  foo: \"beets\",\n});\n\n// Stream the entire aggregated JSON object\nfor await (const chunk of stream2) {\n  console.log(chunk);\n}\n\n/*\n  {}\n  { setup: '' }\n  { setup: 'Why' }\n  { setup: 'Why did' }\n  { setup: 'Why did the' }\n  { setup: 'Why did the beet' }\n  { setup: 'Why did the beet go' }\n  { setup: 'Why did the beet go to' }\n  { setup: 'Why did the beet go to therapy' }\n  { setup: 'Why did the beet go to therapy?', punchline: '' }\n  { setup: 'Why did the beet go to therapy?', punchline: 'Because' }\n  { setup: 'Why did the beet go to therapy?', punchline: 'Because it' }\n  {\n    setup: 'Why did the beet go to therapy?',\n    punchline: 'Because it had'\n  }\n  {\n    setup: 'Why did the beet go to therapy?',\n    punchline: 'Because it had a'\n  }\n  {\n    setup: 'Why did the beet go to therapy?',\n    punchline: 'Because it had a lot'\n  }\n  {\n    setup: 'Why did the beet go to therapy?',\n    punchline: 'Because it had a lot of'\n  }\n  {\n    setup: 'Why did the beet go to therapy?',\n    punchline: 'Because it had a lot of unresolved'\n  }\n  {\n    setup: 'Why did the beet go to therapy?',\n    punchline: 'Because it had a lot of unresolved issues'\n  }\n  {\n    setup: 'Why did the beet go to therapy?',\n    punchline: 'Because it had a lot of unresolved issues!'\n  }\n*/","metadata":{"source":"examples/src/prompts/json_structured_output_parser_streaming.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":145}}}}],["822d9f30-78b1-4f16-96ca-7abbed5df3c7",{"pageContent":"import {\n  LengthBasedExampleSelector,\n  PromptTemplate,\n  FewShotPromptTemplate,\n} from \"langchain/prompts\";\n\nexport async function run() {\n  // Create a prompt template that will be used to format the examples.\n  const examplePrompt = new PromptTemplate({\n    inputVariables: [\"input\", \"output\"],\n    template: \"Input: {input}\\nOutput: {output}\",\n  });\n\n  // Create a LengthBasedExampleSelector that will be used to select the examples.\n  const exampleSelector = await LengthBasedExampleSelector.fromExamples(\n    [\n      { input: \"happy\", output: \"sad\" },\n      { input: \"tall\", output: \"short\" },\n      { input: \"energetic\", output: \"lethargic\" },\n      { input: \"sunny\", output: \"gloomy\" },\n      { input: \"windy\", output: \"calm\" },\n    ],\n    {\n      examplePrompt,\n      maxLength: 25,\n    }\n  );\n\n  // Create a FewShotPromptTemplate that will use the example selector.\n  const dynamicPrompt = new FewShotPromptTemplate({\n    // We provide an ExampleSelector instead of examples.\n    exampleSelector,\n    examplePrompt,\n    prefix: \"Give the antonym of every input\",\n    suffix: \"Input: {adjective}\\nOutput:\",\n    inputVariables: [\"adjective\"],\n  });\n\n  // An example with small input, so it selects all examples.\n  console.log(await dynamicPrompt.format({ adjective: \"big\" }));\n  /*\n   Give the antonym of every input\n\n   Input: happy\n   Output: sad\n\n   Input: tall\n   Output: short\n\n   Input: energetic\n   Output: lethargic\n\n   Input: sunny\n   Output: gloomy\n\n   Input: windy\n   Output: calm\n\n   Input: big\n   Output:\n   */\n\n  // An example with long input, so it selects only one example.\n  const longString =\n    \"big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else\";\n  console.log(await dynamicPrompt.format({ adjective: longString }));\n  /*\n   Give the antonym of every input\n\n   Input: happy\n   Output: sad\n\n   Input: big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else\n   Output:\n   */\n}","metadata":{"source":"examples/src/prompts/length_based_example_selector.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":76}}}}],["2eeb9bff-73fe-4af9-9079-33f59e130d9a",{"pageContent":"import { loadPrompt } from \"langchain/prompts/load\";\n\nexport const run = async () => {\n  const prompt = await loadPrompt(\"lc://prompts/hello-world/prompt.yaml\");\n  const res = await prompt.format({});\n  console.log({ res });\n};","metadata":{"source":"examples/src/prompts/load_from_hub.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":7}}}}],["24568417-45c6-4748-a5ed-17acb83dd3b3",{"pageContent":"import { PromptTemplate } from \"langchain/prompts\";\n\nexport const run = async () => {\n  // The `partial` method returns a new `PromptTemplate` object that can be used to format the prompt with only some of the input variables.\n  const promptA = new PromptTemplate({\n    template: \"{foo}{bar}\",\n    inputVariables: [\"foo\", \"bar\"],\n  });\n  const partialPromptA = await promptA.partial({ foo: \"foo\" });\n  console.log(await partialPromptA.format({ bar: \"bar\" }));\n  // foobar\n\n  // You can also explicitly specify the partial variables when creating the `PromptTemplate` object.\n  const promptB = new PromptTemplate({\n    template: \"{foo}{bar}\",\n    inputVariables: [\"foo\"],\n    partialVariables: { bar: \"bar\" },\n  });\n  console.log(await promptB.format({ foo: \"foo\" }));\n  // foobar\n\n  // You can also use partial formatting with function inputs instead of string inputs.\n  const promptC = new PromptTemplate({\n    template: \"Tell me a {adjective} joke about the day {date}\",\n    inputVariables: [\"adjective\", \"date\"],\n  });\n  const partialPromptC = await promptC.partial({\n    date: () => new Date().toLocaleDateString(),\n  });\n  console.log(await partialPromptC.format({ adjective: \"funny\" }));\n  // Tell me a funny joke about the day 3/22/2023\n\n  const promptD = new PromptTemplate({\n    template: \"Tell me a {adjective} joke about the day {date}\",\n    inputVariables: [\"adjective\"],\n    partialVariables: { date: () => new Date().toLocaleDateString() },\n  });\n  console.log(await promptD.format({ adjective: \"funny\" }));\n  // Tell me a funny joke about the day 3/22/2023\n};","metadata":{"source":"examples/src/prompts/partial.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":40}}}}],["169bde47-f301-47e9-9fbf-d677c21a9f1c",{"pageContent":"import { PromptTemplate, PipelinePromptTemplate } from \"langchain/prompts\";\n\nconst fullPrompt = PromptTemplate.fromTemplate(`{introduction}\n\n{example}\n\n{start}`);\n\nconst introductionPrompt = PromptTemplate.fromTemplate(\n  `You are impersonating {person}.`\n);\n\nconst examplePrompt =\n  PromptTemplate.fromTemplate(`Here's an example of an interaction:\nQ: {example_q}\nA: {example_a}`);\n\nconst startPrompt = PromptTemplate.fromTemplate(`Now, do this for real!\nQ: {input}\nA:`);\n\nconst composedPrompt = new PipelinePromptTemplate({\n  pipelinePrompts: [\n    {\n      name: \"introduction\",\n      prompt: introductionPrompt,\n    },\n    {\n      name: \"example\",\n      prompt: examplePrompt,\n    },\n    {\n      name: \"start\",\n      prompt: startPrompt,\n    },\n  ],\n  finalPrompt: fullPrompt,\n});\n\nconst formattedPrompt = await composedPrompt.format({\n  person: \"Elon Musk\",\n  example_q: `What's your favorite car?`,\n  example_a: \"Telsa\",\n  input: `What's your favorite social media site?`,\n});\n\nconsole.log(formattedPrompt);\n\n/*\n  You are impersonating Elon Musk.\n\n  Here's an example of an interaction:\n  Q: What's your favorite car?\n  A: Telsa\n\n  Now, do this for real!\n  Q: What's your favorite social media site?\n  A:\n*/","metadata":{"source":"examples/src/prompts/pipeline_prompt.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":59}}}}],["069e39a9-ebd9-4637-875c-6ddfe10eb6ce",{"pageContent":"import {\n  ChatPromptTemplate,\n  HumanMessagePromptTemplate,\n  PromptTemplate,\n  SystemMessagePromptTemplate,\n} from \"langchain/prompts\";\n\nexport const run = async () => {\n  const template = \"What is a good name for a company that makes {product}?\";\n  const promptA = new PromptTemplate({ template, inputVariables: [\"product\"] });\n\n  // The `formatPromptValue` method returns a `PromptValue` object that can be used to format the prompt as a string or a list of `ChatMessage` objects.\n  const responseA = await promptA.formatPromptValue({\n    product: \"colorful socks\",\n  });\n  const responseAString = responseA.toString();\n  console.log({ responseAString });\n  /*\n    {\n        responseAString: 'What is a good name for a company that makes colorful socks?'\n    }\n    */\n\n  const responseAMessages = responseA.toChatMessages();\n  console.log({ responseAMessages });\n  /*\n    {\n        responseAMessages: [\n            HumanMessage {\n                text: 'What is a good name for a company that makes colorful socks?'\n            }\n        ]\n    }\n    */\n\n  const chatPrompt = ChatPromptTemplate.fromMessages([\n    SystemMessagePromptTemplate.fromTemplate(\n      \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n    ),\n    HumanMessagePromptTemplate.fromTemplate(\"{text}\"),\n  ]);\n\n  // `formatPromptValue` also works with `ChatPromptTemplate`.\n  const responseB = await chatPrompt.formatPromptValue({\n    input_language: \"English\",\n    output_language: \"French\",\n    text: \"I love programming.\",\n  });\n  const responseBString = responseB.toString();\n  console.log({ responseBString });\n  /*\n    {\n        responseBString: '[{\"text\":\"You are a helpful assistant that translates English to French.\"},{\"text\":\"I love programming.\"}]'\n    }\n    */\n\n  const responseBMessages = responseB.toChatMessages();\n  console.log({ responseBMessages });\n  /*\n    {\n        responseBMessages: [\n            SystemMessage {\n                text: 'You are a helpful assistant that translates English to French.'\n            },\n            HumanMessage { text: 'I love programming.' }\n        ]\n    }\n    */\n};","metadata":{"source":"examples/src/prompts/prompt_value.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":69}}}}],["074b4914-3c95-4edd-b35c-e72ce3999e67",{"pageContent":"import {\n  ChatPromptTemplate,\n  HumanMessagePromptTemplate,\n  PromptTemplate,\n  SystemMessagePromptTemplate,\n} from \"langchain/prompts\";\n\nexport const run = async () => {\n  // A `PromptTemplate` consists of a template string and a list of input variables.\n  const template = \"What is a good name for a company that makes {product}?\";\n  const promptA = new PromptTemplate({ template, inputVariables: [\"product\"] });\n\n  // We can use the `format` method to format the template with the given input values.\n  const responseA = await promptA.format({ product: \"colorful socks\" });\n  console.log({ responseA });\n  /*\n  {\n    responseA: 'What is a good name for a company that makes colorful socks?'\n  }\n  */\n\n  // We can also use the `fromTemplate` method to create a `PromptTemplate` object.\n  const promptB = PromptTemplate.fromTemplate(\n    \"What is a good name for a company that makes {product}?\"\n  );\n  const responseB = await promptB.format({ product: \"colorful socks\" });\n  console.log({ responseB });\n  /*\n  {\n    responseB: 'What is a good name for a company that makes colorful socks?'\n  }\n  */\n\n  // For chat models, we provide a `ChatPromptTemplate` class that can be used to format chat prompts.\n  const chatPrompt = ChatPromptTemplate.fromMessages([\n    SystemMessagePromptTemplate.fromTemplate(\n      \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n    ),\n    HumanMessagePromptTemplate.fromTemplate(\"{text}\"),\n  ]);\n\n  // The result can be formatted as a string using the `format` method.\n  const responseC = await chatPrompt.format({\n    input_language: \"English\",\n    output_language: \"French\",\n    text: \"I love programming.\",\n  });\n  console.log({ responseC });\n  /*\n  {\n    responseC: '[{\"text\":\"You are a helpful assistant that translates English to French.\"},{\"text\":\"I love programming.\"}]'\n  }\n  */\n\n  // The result can also be formatted as a list of `ChatMessage` objects by returning a `PromptValue` object and calling the `toChatMessages` method.\n  // More on this below.\n  const responseD = await chatPrompt.formatPromptValue({\n    input_language: \"English\",\n    output_language: \"French\",\n    text: \"I love programming.\",\n  });\n  const messages = responseD.toChatMessages();\n  console.log({ messages });\n  /*\n  {\n    messages: [\n        SystemMessage {\n          text: 'You are a helpful assistant that translates English to French.'\n        },\n        HumanMessage { text: 'I love programming.' }\n      ]\n  }\n  */\n};","metadata":{"source":"examples/src/prompts/prompts.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":74}}}}],["4f5b7cf6-4f46-4d8c-b291-9ecbb1c2229f",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { RegexParser } from \"langchain/output_parsers\";\nimport { PromptTemplate } from \"langchain/prompts\";\n\nexport const run = async () => {\n  const parser = new RegexParser(\n    /Humor: ([0-9]+), Sophistication: (A|B|C|D|E)/,\n    [\"mark\", \"grade\"],\n    \"noConfidence\"\n  );\n  const formatInstructions = parser.getFormatInstructions();\n\n  const prompt = new PromptTemplate({\n    template: \"Grade the joke.\\n\\n{format_instructions}\\n\\nJoke: {joke}\",\n    inputVariables: [\"joke\"],\n    partialVariables: { format_instructions: formatInstructions },\n  });\n\n  const model = new OpenAI({ temperature: 0 });\n\n  const input = await prompt.format({\n    joke: \"What time is the appointment? Tooth hurt-y.\",\n  });\n  console.log(input);\n  /*\n  Grade the joke.\n\n  Your response should match the following regex: /Humor: ([0-9]+), Sophistication: (A|B|C|D|E)/\n\n  Joke: What time is the appointment? Tooth hurt-y.\n  */\n\n  const response = await model.call(input);\n  console.log(response);\n  /*\n  Humor: 8, Sophistication: D\n  */\n};","metadata":{"source":"examples/src/prompts/regex_parser.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":38}}}}],["6b297e1f-3618-427e-9c61-9764824bff33",{"pageContent":"import { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport {\n  SemanticSimilarityExampleSelector,\n  PromptTemplate,\n  FewShotPromptTemplate,\n} from \"langchain/prompts\";\nimport { HNSWLib } from \"langchain/vectorstores/hnswlib\";\n\n// Create a prompt template that will be used to format the examples.\nconst examplePrompt = PromptTemplate.fromTemplate(\n  \"Input: {input}\\nOutput: {output}\"\n);\n\n// Create a SemanticSimilarityExampleSelector that will be used to select the examples.\nconst exampleSelector = await SemanticSimilarityExampleSelector.fromExamples(\n  [\n    { input: \"happy\", output: \"sad\" },\n    { input: \"tall\", output: \"short\" },\n    { input: \"energetic\", output: \"lethargic\" },\n    { input: \"sunny\", output: \"gloomy\" },\n    { input: \"windy\", output: \"calm\" },\n  ],\n  new OpenAIEmbeddings(),\n  HNSWLib,\n  { k: 1 }\n);\n\n// Create a FewShotPromptTemplate that will use the example selector.\nconst dynamicPrompt = new FewShotPromptTemplate({\n  // We provide an ExampleSelector instead of examples.\n  exampleSelector,\n  examplePrompt,\n  prefix: \"Give the antonym of every input\",\n  suffix: \"Input: {adjective}\\nOutput:\",\n  inputVariables: [\"adjective\"],\n});\n\n// Input is about the weather, so should select eg. the sunny/gloomy example\nconsole.log(await dynamicPrompt.format({ adjective: \"rainy\" }));\n/*\n  Give the antonym of every input\n\n  Input: sunny\n  Output: gloomy\n\n  Input: rainy\n  Output:\n*/\n\n// Input is a measurement, so should select the tall/short example\nconsole.log(await dynamicPrompt.format({ adjective: \"large\" }));\n/*\n  Give the antonym of every input\n\n  Input: tall\n  Output: short\n\n  Input: large\n  Output:\n*/","metadata":{"source":"examples/src/prompts/semantic_similarity_example_selector.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":60}}}}],["a4a7d4f4-1d3c-4ca5-bf8e-b66c5f3340f5",{"pageContent":"/* eslint-disable @typescript-eslint/no-non-null-assertion */\n\n// Requires a vectorstore that supports maximal marginal relevance search\nimport { Pinecone } from \"@pinecone-database/pinecone\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { PineconeStore } from \"langchain/vectorstores/pinecone\";\nimport {\n  SemanticSimilarityExampleSelector,\n  PromptTemplate,\n  FewShotPromptTemplate,\n} from \"langchain/prompts\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\n\nconst pinecone = new Pinecone();\n\nconst pineconeIndex = pinecone.Index(process.env.PINECONE_INDEX!);\n\nconst pineconeVectorstore = await PineconeStore.fromExistingIndex(\n  new OpenAIEmbeddings(),\n  { pineconeIndex }\n);\n\nconst pineconeMmrRetriever = pineconeVectorstore.asRetriever({\n  searchType: \"mmr\",\n  k: 2,\n});\n\nconst examples = [\n  {\n    query: \"healthy food\",\n    output: `lettuce`,\n    food_type: \"vegetable\",\n  },\n  {\n    query: \"healthy food\",\n    output: `schnitzel`,\n    food_type: \"veal\",\n  },\n  {\n    query: \"foo\",\n    output: `bar`,\n    food_type: \"baz\",\n  },\n];\n\nconst exampleSelector = new SemanticSimilarityExampleSelector({\n  vectorStoreRetriever: pineconeMmrRetriever,\n  // Only embed the \"query\" key of each example\n  inputKeys: [\"query\"],\n});\n\nfor (const example of examples) {\n  // Format and add an example to the underlying vector store\n  await exampleSelector.addExample(example);\n}\n\n// Create a prompt template that will be used to format the examples.\nconst examplePrompt = PromptTemplate.fromTemplate(`<example>\n  <user_input>\n    {query}\n  </user_input>\n  <output>\n    {output}\n  </output>\n</example>`);\n\n// Create a FewShotPromptTemplate that will use the example selector.\nconst dynamicPrompt = new FewShotPromptTemplate({\n  // We provide an ExampleSelector instead of examples.\n  exampleSelector,\n  examplePrompt,\n  prefix: `Answer the user's question, using the below examples as reference:`,\n  suffix: \"User question:\\n{query}\",\n  inputVariables: [\"query\"],\n});\n\nconst model = new ChatOpenAI({});\n\nconst chain = dynamicPrompt.pipe(model);\n\nconst result = await chain.invoke({\n  query: \"What is exactly one type of healthy food?\",\n});\n\nconsole.log(result);\n\n/*\n  AIMessage {\n    content: 'lettuce.',\n    additional_kwargs: { function_call: undefined }\n  }\n*/","metadata":{"source":"examples/src/prompts/semantic_similarity_example_selector_custom_retriever.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":92}}}}],["05f1c411-ccdc-4927-8551-c78fe2570805",{"pageContent":"// Ephemeral, in-memory vector store for demo purposes\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\nimport {\n  SemanticSimilarityExampleSelector,\n  PromptTemplate,\n  FewShotPromptTemplate,\n} from \"langchain/prompts\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\n\nconst embeddings = new OpenAIEmbeddings();\n\nconst memoryVectorStore = new MemoryVectorStore(embeddings);\n\nconst examples = [\n  {\n    query: \"healthy food\",\n    output: `galbi`,\n  },\n  {\n    query: \"healthy food\",\n    output: `schnitzel`,\n  },\n  {\n    query: \"foo\",\n    output: `bar`,\n  },\n];\n\nconst exampleSelector = new SemanticSimilarityExampleSelector({\n  vectorStore: memoryVectorStore,\n  k: 2,\n  // Only embed the \"query\" key of each example\n  inputKeys: [\"query\"],\n});\n\nfor (const example of examples) {\n  // Format and add an example to the underlying vector store\n  await exampleSelector.addExample(example);\n}\n\n// Create a prompt template that will be used to format the examples.\nconst examplePrompt = PromptTemplate.fromTemplate(`<example>\n  <user_input>\n    {query}\n  </user_input>\n  <output>\n    {output}\n  </output>\n</example>`);\n\n// Create a FewShotPromptTemplate that will use the example selector.\nconst dynamicPrompt = new FewShotPromptTemplate({\n  // We provide an ExampleSelector instead of examples.\n  exampleSelector,\n  examplePrompt,\n  prefix: `Answer the user's question, using the below examples as reference:`,\n  suffix: \"User question: {query}\",\n  inputVariables: [\"query\"],\n});\n\nconst formattedValue = await dynamicPrompt.format({\n  query: \"What is a healthy food?\",\n});\nconsole.log(formattedValue);\n\n/*\nAnswer the user's question, using the below examples as reference:\n\n<example>\n  <user_input>\n    healthy\n  </user_input>\n  <output>\n    galbi\n  </output>\n</example>\n\n<example>\n  <user_input>\n    healthy\n  </user_input>\n  <output>\n    schnitzel\n  </output>\n</example>\n\nUser question: What is a healthy food?\n*/\n\nconst model = new ChatOpenAI({});\n\nconst chain = dynamicPrompt.pipe(model);\n\nconst result = await chain.invoke({ query: \"What is a healthy food?\" });\nconsole.log(result);\n/*\n  AIMessage {\n    content: 'A healthy food can be galbi or schnitzel.',\n    additional_kwargs: { function_call: undefined }\n  }\n*/","metadata":{"source":"examples/src/prompts/semantic_similarity_example_selector_from_existing.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":102}}}}],["b461db72-6c56-4258-97e4-bf6ab0f2f897",{"pageContent":"// Ephemeral, in-memory vector store for demo purposes\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\nimport {\n  SemanticSimilarityExampleSelector,\n  PromptTemplate,\n  FewShotPromptTemplate,\n} from \"langchain/prompts\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { Document } from \"langchain/document\";\n\nconst embeddings = new OpenAIEmbeddings();\n\nconst memoryVectorStore = new MemoryVectorStore(embeddings);\n\nconst examples = [\n  {\n    query: \"healthy food\",\n    output: `lettuce`,\n    food_type: \"vegetable\",\n  },\n  {\n    query: \"healthy food\",\n    output: `schnitzel`,\n    food_type: \"veal\",\n  },\n  {\n    query: \"foo\",\n    output: `bar`,\n    food_type: \"baz\",\n  },\n];\n\nconst exampleSelector = new SemanticSimilarityExampleSelector({\n  vectorStore: memoryVectorStore,\n  k: 2,\n  // Only embed the \"query\" key of each example\n  inputKeys: [\"query\"],\n  // Filter type will depend on your specific vector store.\n  // See the section of the docs for the specific vector store you are using.\n  filter: (doc: Document) => doc.metadata.food_type === \"vegetable\",\n});\n\nfor (const example of examples) {\n  // Format and add an example to the underlying vector store\n  await exampleSelector.addExample(example);\n}\n\n// Create a prompt template that will be used to format the examples.\nconst examplePrompt = PromptTemplate.fromTemplate(`<example>\n  <user_input>\n    {query}\n  </user_input>\n  <output>\n    {output}\n  </output>\n</example>`);\n\n// Create a FewShotPromptTemplate that will use the example selector.\nconst dynamicPrompt = new FewShotPromptTemplate({\n  // We provide an ExampleSelector instead of examples.\n  exampleSelector,\n  examplePrompt,\n  prefix: `Answer the user's question, using the below examples as reference:`,\n  suffix: \"User question:\\n{query}\",\n  inputVariables: [\"query\"],\n});\n\nconst model = new ChatOpenAI({});\n\nconst chain = dynamicPrompt.pipe(model);\n\nconst result = await chain.invoke({\n  query: \"What is exactly one type of healthy food?\",\n});\nconsole.log(result);\n/*\n  AIMessage {\n    content: 'One type of healthy food is lettuce.',\n    additional_kwargs: { function_call: undefined }\n  }\n*/","metadata":{"source":"examples/src/prompts/semantic_similarity_example_selector_metadata_filtering.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":82}}}}],["5075c9b0-305c-4def-9c62-ed2b1349355c",{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { StringOutputParser } from \"langchain/schema/output_parser\";\n\nconst parser = new StringOutputParser();\n\nconst model = new ChatOpenAI({ temperature: 0 });\n\nconst stream = await model.pipe(parser).stream(\"Hello there!\");\n\nfor await (const chunk of stream) {\n  console.log(chunk);\n}\n\n/*\n  Hello\n  !\n  How\n  can\n  I\n  assist\n  you\n  today\n  ?\n*/","metadata":{"source":"examples/src/prompts/string_output_parser.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":24}}}}],["7ea45d4e-b599-41d4-85d5-57c1941ebb14",{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { StringOutputParser } from \"langchain/schema/output_parser\";\nimport { RunnableSequence } from \"langchain/schema/runnable\";\n\nconst chain = RunnableSequence.from([\n  new ChatOpenAI({ temperature: 0 }),\n  new StringOutputParser(),\n]);\n\nconst stream = await chain.stream(\"Hello there!\");\n\nfor await (const chunk of stream) {\n  console.log(chunk);\n}","metadata":{"source":"examples/src/prompts/string_output_parser_sequence.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":14}}}}],["ce1c1092-9206-4384-a739-e3118613399b",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport { StructuredOutputParser } from \"langchain/output_parsers\";\n\n// With a `StructuredOutputParser` we can define a schema for the output.\nconst parser = StructuredOutputParser.fromNamesAndDescriptions({\n  answer: \"answer to the user's question\",\n  source: \"source used to answer the user's question, should be a website.\",\n});\n\nconst formatInstructions = parser.getFormatInstructions();\n\nconst prompt = new PromptTemplate({\n  template:\n    \"Answer the users question as best as possible.\\n{format_instructions}\\n{question}\",\n  inputVariables: [\"question\"],\n  partialVariables: { format_instructions: formatInstructions },\n});\n\nconst model = new OpenAI({ temperature: 0 });\n\nconst input = await prompt.format({\n  question: \"What is the capital of France?\",\n});\nconst response = await model.call(input);\n\nconsole.log(input);\n/*\nAnswer the users question as best as possible.\nYou must format your output as a JSON value that adheres to a given \"JSON Schema\" instance.\n\n\"JSON Schema\" is a declarative language that allows you to annotate and validate JSON documents.\n\nFor example, the example \"JSON Schema\" instance {{\"properties\": {{\"foo\": {{\"description\": \"a list of test words\", \"type\": \"array\", \"items\": {{\"type\": \"string\"}}}}}}, \"required\": [\"foo\"]}}}}\nwould match an object with one required property, \"foo\". The \"type\" property specifies \"foo\" must be an \"array\", and the \"description\" property semantically describes it as \"a list of test words\". The items within \"foo\" must be strings.\nThus, the object {{\"foo\": [\"bar\", \"baz\"]}} is a well-formatted instance of this example \"JSON Schema\". The object {{\"properties\": {{\"foo\": [\"bar\", \"baz\"]}}}} is not well-formatted.\n\nYour output will be parsed and type-checked according to the provided schema instance, so make sure all fields in your output match the schema exactly and there are no trailing commas!\n\nHere is the JSON Schema instance your output must adhere to. Include the enclosing markdown codeblock:\n```json\n{\"type\":\"object\",\"properties\":{\"answer\":{\"type\":\"string\",\"description\":\"answer to the user's question\"},\"source\":{\"type\":\"string\",\"description\":\"source used to answer the user's question, should be a website.\"}},\"required\":[\"answer\",\"source\"],\"additionalProperties\":false,\"$schema\":\"http://json-schema.org/draft-07/schema#\"}\n```\n\nWhat is the capital of France?\n*/\n\nconsole.log(response);\n/*\n{\"answer\": \"Paris\", \"source\": \"https://en.wikipedia.org/wiki/Paris\"}\n*/\n\nconsole.log(await parser.parse(response));\n// { answer: 'Paris', source: 'https://en.wikipedia.org/wiki/Paris' }","metadata":{"source":"examples/src/prompts/structured_parser.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":54}}}}],["31965ef3-4d37-4f44-b8fa-b9cecec81e2b",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport { StructuredOutputParser } from \"langchain/output_parsers\";\nimport { RunnableSequence } from \"langchain/schema/runnable\";\n\nconst parser = StructuredOutputParser.fromNamesAndDescriptions({\n  answer: \"answer to the user's question\",\n  source: \"source used to answer the user's question, should be a website.\",\n});\n\nconst chain = RunnableSequence.from([\n  PromptTemplate.fromTemplate(\n    \"Answer the users question as best as possible.\\n{format_instructions}\\n{question}\"\n  ),\n  new OpenAI({ temperature: 0 }),\n  parser,\n]);\n\nconsole.log(parser.getFormatInstructions());\n\n/*\nAnswer the users question as best as possible.\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\n\nAs an example, for the schema {{\"properties\": {{\"foo\": {{\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {{\"type\": \"string\"}}}}}}, \"required\": [\"foo\"]}}}}\nthe object {{\"foo\": [\"bar\", \"baz\"]}} is a well-formatted instance of the schema. The object {{\"properties\": {{\"foo\": [\"bar\", \"baz\"]}}}} is not well-formatted.\n\nHere is the output schema:\n```\n{\"type\":\"object\",\"properties\":{\"answer\":{\"type\":\"string\",\"description\":\"answer to the user's question\"},\"sources\":{\"type\":\"array\",\"items\":{\"type\":\"string\"},\"description\":\"sources used to answer the question, should be websites.\"}},\"required\":[\"answer\",\"sources\"],\"additionalProperties\":false,\"$schema\":\"http://json-schema.org/draft-07/schema#\"}\n```\n\nWhat is the capital of France?\n*/\n\nconst response = await chain.invoke({\n  question: \"What is the capital of France?\",\n  format_instructions: parser.getFormatInstructions(),\n});\n\nconsole.log(response);\n// { answer: 'Paris', source: 'https://en.wikipedia.org/wiki/Paris' }","metadata":{"source":"examples/src/prompts/structured_parser_sequence.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":42}}}}],["6b7f27cd-a515-4e95-b32c-0be1d8e7d5b9",{"pageContent":"import { z } from \"zod\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport { StructuredOutputParser } from \"langchain/output_parsers\";\n\n// We can use zod to define a schema for the output using the `fromZodSchema` method of `StructuredOutputParser`.\nconst parser = StructuredOutputParser.fromZodSchema(\n  z.object({\n    answer: z.string().describe(\"answer to the user's question\"),\n    sources: z\n      .array(z.string())\n      .describe(\"sources used to answer the question, should be websites.\"),\n  })\n);\n\nconst formatInstructions = parser.getFormatInstructions();\n\nconst prompt = new PromptTemplate({\n  template:\n    \"Answer the users question as best as possible.\\n{format_instructions}\\n{question}\",\n  inputVariables: [\"question\"],\n  partialVariables: { format_instructions: formatInstructions },\n});\n\nconst model = new OpenAI({ temperature: 0 });\n\nconst input = await prompt.format({\n  question: \"What is the capital of France?\",\n});\nconst response = await model.call(input);\n\nconsole.log(input);\n/*\nAnswer the users question as best as possible.\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\n\nAs an example, for the schema {{\"properties\": {{\"foo\": {{\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {{\"type\": \"string\"}}}}}}, \"required\": [\"foo\"]}}}}\nthe object {{\"foo\": [\"bar\", \"baz\"]}} is a well-formatted instance of the schema. The object {{\"properties\": {{\"foo\": [\"bar\", \"baz\"]}}}} is not well-formatted.\n\nHere is the output schema:\n```\n{\"type\":\"object\",\"properties\":{\"answer\":{\"type\":\"string\",\"description\":\"answer to the user's question\"},\"sources\":{\"type\":\"array\",\"items\":{\"type\":\"string\"},\"description\":\"sources used to answer the question, should be websites.\"}},\"required\":[\"answer\",\"sources\"],\"additionalProperties\":false,\"$schema\":\"http://json-schema.org/draft-07/schema#\"}\n```\n\nWhat is the capital of France?\n*/\n\nconsole.log(response);\n/*\n{\"answer\": \"Paris\", \"sources\": [\"https://en.wikipedia.org/wiki/Paris\"]}\n*/\n\nconsole.log(await parser.parse(response));\n/*\n{ answer: 'Paris', sources: [ 'https://en.wikipedia.org/wiki/Paris' ] }\n*/","metadata":{"source":"examples/src/prompts/structured_parser_zod.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":56}}}}],["28e3b8f8-3c30-463f-baa6-c08ffd572ea3",{"pageContent":"import { z } from \"zod\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport { StructuredOutputParser } from \"langchain/output_parsers\";\nimport { RunnableSequence } from \"langchain/schema/runnable\";\n\n// We can use zod to define a schema for the output using the `fromZodSchema` method of `StructuredOutputParser`.\nconst parser = StructuredOutputParser.fromZodSchema(\n  z.object({\n    answer: z.string().describe(\"answer to the user's question\"),\n    sources: z\n      .array(z.string())\n      .describe(\"sources used to answer the question, should be websites.\"),\n  })\n);\n\nconst chain = RunnableSequence.from([\n  PromptTemplate.fromTemplate(\n    \"Answer the users question as best as possible.\\n{format_instructions}\\n{question}\"\n  ),\n  new OpenAI({ temperature: 0 }),\n  parser,\n]);\n\nconsole.log(parser.getFormatInstructions());\n\n/*\nAnswer the users question as best as possible.\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\n\nAs an example, for the schema {{\"properties\": {{\"foo\": {{\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {{\"type\": \"string\"}}}}}}, \"required\": [\"foo\"]}}}}\nthe object {{\"foo\": [\"bar\", \"baz\"]}} is a well-formatted instance of the schema. The object {{\"properties\": {{\"foo\": [\"bar\", \"baz\"]}}}} is not well-formatted.\n\nHere is the output schema:\n```\n{\"type\":\"object\",\"properties\":{\"answer\":{\"type\":\"string\",\"description\":\"answer to the user's question\"},\"sources\":{\"type\":\"array\",\"items\":{\"type\":\"string\"},\"description\":\"sources used to answer the question, should be websites.\"}},\"required\":[\"answer\",\"sources\"],\"additionalProperties\":false,\"$schema\":\"http://json-schema.org/draft-07/schema#\"}\n```\n\nWhat is the capital of France?\n*/\n\nconst response = await chain.invoke({\n  question: \"What is the capital of France?\",\n  format_instructions: parser.getFormatInstructions(),\n});\n\nconsole.log(response);\n/*\n{ answer: 'Paris', sources: [ 'https://en.wikipedia.org/wiki/Paris' ] }\n*/","metadata":{"source":"examples/src/prompts/structured_parser_zod_sequence.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":50}}}}],["c66be175-2eb3-4ffc-a482-2f358775054a",{"pageContent":"import { z } from \"zod\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport { LLMChain } from \"langchain/chains\";\nimport {\n  StructuredOutputParser,\n  OutputFixingParser,\n} from \"langchain/output_parsers\";\n\nconst outputParser = StructuredOutputParser.fromZodSchema(\n  z\n    .array(\n      z.object({\n        fields: z.object({\n          Name: z.string().describe(\"The name of the country\"),\n          Capital: z.string().describe(\"The country's capital\"),\n        }),\n      })\n    )\n    .describe(\"An array of Airtable records, each representing a country\")\n);\n\nconst chatModel = new ChatOpenAI({\n  modelName: \"gpt-4\", // Or gpt-3.5-turbo\n  temperature: 0, // For best results with the output fixing parser\n});\n\nconst outputFixingParser = OutputFixingParser.fromLLM(chatModel, outputParser);\n\n// Don't forget to include formatting instructions in the prompt!\nconst prompt = new PromptTemplate({\n  template: `Answer the user's question as best you can:\\n{format_instructions}\\n{query}`,\n  inputVariables: [\"query\"],\n  partialVariables: {\n    format_instructions: outputFixingParser.getFormatInstructions(),\n  },\n});\n\nconst answerFormattingChain = new LLMChain({\n  llm: chatModel,\n  prompt,\n  outputKey: \"records\", // For readability - otherwise the chain output will default to a property named \"text\"\n  outputParser: outputFixingParser,\n});\n\nconst result = await answerFormattingChain.call({\n  query: \"List 5 countries.\",\n});\n\nconsole.log(JSON.stringify(result.records, null, 2));\n\n/*\n[\n  {\n    \"fields\": {\n      \"Name\": \"United States\",\n      \"Capital\": \"Washington, D.C.\"\n    }\n  },\n  {\n    \"fields\": {\n      \"Name\": \"Canada\",\n      \"Capital\": \"Ottawa\"\n    }\n  },\n  {\n    \"fields\": {\n      \"Name\": \"Germany\",\n      \"Capital\": \"Berlin\"\n    }\n  },\n  {\n    \"fields\": {\n      \"Name\": \"Japan\",\n      \"Capital\": \"Tokyo\"\n    }\n  },\n  {\n    \"fields\": {\n      \"Name\": \"Australia\",\n      \"Capital\": \"Canberra\"\n    }\n  }\n]\n*/","metadata":{"source":"examples/src/prompts/use_with_llm_chain.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":85}}}}],["0800f1cf-eac2-4a5c-a6e2-5923a1991825",{"pageContent":"import { ChaindeskRetriever } from \"langchain/retrievers/chaindesk\";\n\nconst retriever = new ChaindeskRetriever({\n  datastoreId: \"DATASTORE_ID\",\n  apiKey: \"CHAINDESK_API_KEY\", // optional: needed for private datastores\n  topK: 8, // optional: default value is 3\n});\n\nconst docs = await retriever.getRelevantDocuments(\"hello\");\n\nconsole.log(docs);","metadata":{"source":"examples/src/retrievers/chaindesk.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":11}}}}],["f3be4f4a-2b61-4e9b-94ad-c212a3d39b08",{"pageContent":"import { ChatGPTPluginRetriever } from \"langchain/retrievers/remote\";\n\nexport const run = async () => {\n  const retriever = new ChatGPTPluginRetriever({\n    url: \"http://0.0.0.0:8000\",\n    auth: {\n      bearer: \"super-secret-jwt-token-with-at-least-32-characters-long\",\n    },\n  });\n\n  const docs = await retriever.getRelevantDocuments(\"hello world\");\n\n  console.log(docs);\n};","metadata":{"source":"examples/src/retrievers/chatgpt-plugin.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":14}}}}],["a56a767e-2803-446c-a9b1-62f8cfd472fe",{"pageContent":"import { AttributeInfo } from \"langchain/schema/query_constructor\";\nimport { Document } from \"langchain/document\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { SelfQueryRetriever } from \"langchain/retrievers/self_query\";\nimport { ChromaTranslator } from \"langchain/retrievers/self_query/chroma\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { Chroma } from \"langchain/vectorstores/chroma\";\n\n/**\n * First, we create a bunch of documents. You can load your own documents here instead.\n * Each document has a pageContent and a metadata field. Make sure your metadata matches the AttributeInfo below.\n */\nconst docs = [\n  new Document({\n    pageContent:\n      \"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n    metadata: { year: 1993, rating: 7.7, genre: \"science fiction\" },\n  }),\n  new Document({\n    pageContent:\n      \"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",\n    metadata: { year: 2010, director: \"Christopher Nolan\", rating: 8.2 },\n  }),\n  new Document({\n    pageContent:\n      \"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",\n    metadata: { year: 2006, director: \"Satoshi Kon\", rating: 8.6 },\n  }),\n  new Document({\n    pageContent:\n      \"A bunch of normal-sized women are supremely wholesome and some men pine after them\",\n    metadata: { year: 2019, director: \"Greta Gerwig\", rating: 8.3 },\n  }),\n  new Document({\n    pageContent: \"Toys come alive and have a blast doing so\",\n    metadata: { year: 1995, genre: \"animated\" },\n  }),\n  new Document({\n    pageContent: \"Three men walk into the Zone, three men walk out of the Zone\",\n    metadata: {\n      year: 1979,\n      director: \"Andrei Tarkovsky\",\n      genre: \"science fiction\",\n      rating: 9.9,\n    },\n  }),\n];\n\n/**\n * Next, we define the attributes we want to be able to query on.\n * in this case, we want to be able to query on the genre, year, director, rating, and length of the movie.\n * We also provide a description of each attribute and the type of the attribute.\n * This is used to generate the query prompts.\n */\nconst attributeInfo: AttributeInfo[] = [\n  {\n    name: \"genre\",\n    description: \"The genre of the movie\",\n    type: \"string or array of strings\",\n  },\n  {\n    name: \"year\",\n    description: \"The year the movie was released\",\n    type: \"number\",\n  },\n  {\n    name: \"director\",\n    description: \"The director of the movie\",\n    type: \"string\",\n  },\n  {\n    name: \"rating\",\n    description: \"The rating of the movie (1-10)\",\n    type: \"number\",\n  },\n  {\n    name: \"length\",\n    description: \"The length of the movie in minutes\",\n    type: \"number\",\n  },\n];\n\n/**\n * Next, we instantiate a vector store. This is where we store the embeddings of the documents.\n * We also need to provide an embeddings object. This is used to embed the documents.\n */\nconst embeddings = new OpenAIEmbeddings();\nconst llm = new OpenAI();\nconst documentContents = \"Brief summary of a movie\";\nconst vectorStore = await Chroma.fromDocuments(docs, embeddings, {\n  collectionName: \"a-movie-collection\",\n});\nconst selfQueryRetriever = await SelfQueryRetriever.fromLLM({\n  llm,\n  vectorStore,\n  documentContents,\n  attributeInfo,\n  /**\n   * We need to create a basic translator that translates the queries into a\n   * filter format that the vector store can understand. We provide a basic translator\n   * translator here, but you can create your own translator by extending BaseTranslator\n   * abstract class. Note that the vector store needs to support filtering on the metadata\n   * attributes you want to query on.\n   */\n  structuredQueryTranslator: new ChromaTranslator(),\n});\n\n/**\n * Now we can query the vector store.\n * We can ask questions like \"Which movies are less than 90 minutes?\" or \"Which movies are rated higher than 8.5?\".\n * We can also ask questions like \"Which movies are either comedy or drama and are less than 90 minutes?\".\n * The retriever will automatically convert these questions into queries that can be used to retrieve documents.\n */","metadata":{"source":"examples/src/retrievers/chroma_self_query.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":113}}}}],["2e491f64-d4fa-40cd-b1ad-c0ee56e49b27",{"pageContent":"const query1 = await selfQueryRetriever.getRelevantDocuments(\n  \"Which movies are less than 90 minutes?\"\n);\nconst query2 = await selfQueryRetriever.getRelevantDocuments(\n  \"Which movies are rated higher than 8.5?\"\n);\nconst query3 = await selfQueryRetriever.getRelevantDocuments(\n  \"Which movies are directed by Greta Gerwig?\"\n);\nconst query4 = await selfQueryRetriever.getRelevantDocuments(\n  \"Which movies are either comedy or drama and are less than 90 minutes?\"\n);\nconsole.log(query1, query2, query3, query4);","metadata":{"source":"examples/src/retrievers/chroma_self_query.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":114,"to":126}}}}],["43d5844c-03ca-4735-a59f-7539404f2721",{"pageContent":"import * as fs from \"fs\";\n\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { ContextualCompressionRetriever } from \"langchain/retrievers/contextual_compression\";\nimport { LLMChainExtractor } from \"langchain/retrievers/document_compressors/chain_extract\";\n\nconst model = new OpenAI({\n  modelName: \"gpt-3.5-turbo-instruct\",\n});\nconst baseCompressor = LLMChainExtractor.fromLLM(model);\n\nconst text = fs.readFileSync(\"state_of_the_union.txt\", \"utf8\");\n\nconst textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });\nconst docs = await textSplitter.createDocuments([text]);\n\n// Create a vector store from the documents.\nconst vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());\n\nconst retriever = new ContextualCompressionRetriever({\n  baseCompressor,\n  baseRetriever: vectorStore.asRetriever(),\n});\n\nconst retrievedDocs = await retriever.getRelevantDocuments(\n  \"What did the speaker say about Justice Breyer?\"\n);\n\nconsole.log({ retrievedDocs });\n\n/*\n  {\n    retrievedDocs: [\n      Document {\n        pageContent: 'One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.',\n        metadata: [Object]\n      },\n      Document {\n        pageContent: '\"Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.\"',\n        metadata: [Object]\n      },\n      Document {\n        pageContent: 'The onslaught of state laws targeting transgender Americans and their families is wrong.',\n        metadata: [Object]\n      }\n    ]\n  }\n*/","metadata":{"source":"examples/src/retrievers/contextual_compression.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":51}}}}],["cfe260c2-e5d6-45bb-b229-21a6f7fcc08b",{"pageContent":"import { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { ContextualCompressionRetriever } from \"langchain/retrievers/contextual_compression\";\nimport { EmbeddingsFilter } from \"langchain/retrievers/document_compressors/embeddings_filter\";\nimport { TavilySearchAPIRetriever } from \"langchain/retrievers/tavily_search_api\";\nimport { DocumentCompressorPipeline } from \"langchain/retrievers/document_compressors\";\n\nconst embeddingsFilter = new EmbeddingsFilter({\n  embeddings: new OpenAIEmbeddings(),\n  similarityThreshold: 0.8,\n  k: 5,\n});\n\nconst textSplitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 200,\n  chunkOverlap: 0,\n});\n\nconst compressorPipeline = new DocumentCompressorPipeline({\n  transformers: [textSplitter, embeddingsFilter],\n});\n\nconst baseRetriever = new TavilySearchAPIRetriever({\n  includeRawContent: true,\n});\n\nconst retriever = new ContextualCompressionRetriever({\n  baseCompressor: compressorPipeline,\n  baseRetriever,\n});\n\nconst retrievedDocs = await retriever.getRelevantDocuments(\n  \"What did the speaker say about Justice Breyer in the 2022 State of the Union?\"\n);\nconsole.log({ retrievedDocs });\n\n/*\n  {\n    retrievedDocs: [\n      Document {\n        pageContent: 'Justice Stephen Breyer talks to President Joe Biden ahead of the State of the Union address on Tuesday. (jabin botsford/Agence France-Presse/Getty Images)',\n        metadata: [Object]\n      },\n      Document {\n        pageContent: 'President Biden recognized outgoing US Supreme Court Justice Stephen Breyer during his State of the Union on Tuesday.',\n        metadata: [Object]\n      },\n      Document {\n        pageContent: 'What we covered here\\n' +\n          'Biden recognized outgoing Supreme Court Justice Breyer during his speech',\n        metadata: [Object]\n      },\n      Document {\n        pageContent: 'States Supreme Court. Justice Breyer, thank you for your service,” the president said.',\n        metadata: [Object]\n      },\n      Document {\n        pageContent: 'Court,\" Biden said. \"Justice Breyer, thank you for your service.\"',\n        metadata: [Object]\n      }\n    ]\n  }\n*/","metadata":{"source":"examples/src/retrievers/document_compressor_pipeline.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":63}}}}],["c468f849-e2c0-467f-bbbf-3cb7051b9d44",{"pageContent":"import * as fs from \"fs\";\n\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { ContextualCompressionRetriever } from \"langchain/retrievers/contextual_compression\";\nimport { EmbeddingsFilter } from \"langchain/retrievers/document_compressors/embeddings_filter\";\n\nconst baseCompressor = new EmbeddingsFilter({\n  embeddings: new OpenAIEmbeddings(),\n  similarityThreshold: 0.8,\n});\n\nconst text = fs.readFileSync(\"state_of_the_union.txt\", \"utf8\");\n\nconst textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });\nconst docs = await textSplitter.createDocuments([text]);\n\n// Create a vector store from the documents.\nconst vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());\n\nconst retriever = new ContextualCompressionRetriever({\n  baseCompressor,\n  baseRetriever: vectorStore.asRetriever(),\n});\n\nconst retrievedDocs = await retriever.getRelevantDocuments(\n  \"What did the speaker say about Justice Breyer?\"\n);\nconsole.log({ retrievedDocs });\n\n/*\n  {\n    retrievedDocs: [\n      Document {\n        pageContent: 'And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence. \\n' +\n          '\\n' +\n          'A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she’s been nominated, she’s received a broad range of support—from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. \\n' +\n          '\\n' +\n          'And if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. \\n' +\n          '\\n' +\n          'We can do both. At our border, we’ve installed new technology like cutting-edge scanners to better detect drug smuggling.  \\n' +\n          '\\n' +\n          'We’ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  \\n' +\n          '\\n' +\n          'We’re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster.',\n        metadata: [Object]\n      },\n      Document {\n        pageContent: 'In state after state, new laws have been passed, not only to suppress the vote, but to subvert entire elections. \\n' +\n          '\\n' +\n          'We cannot let this happen. \\n' +\n          '\\n' +\n          'Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \\n' +\n          '\\n' +\n          'Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\n' +\n          '\\n' +\n          'One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\n' +\n          '\\n' +\n          'And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.',\n        metadata: [Object]\n      }\n    ]\n  }\n*/","metadata":{"source":"examples/src/retrievers/embeddings_filter.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":65}}}}],["87c1c1e3-1ca6-4e0f-b0fd-ab0a3d70dbdf",{"pageContent":"import { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { AttributeInfo } from \"langchain/schema/query_constructor\";\nimport { Document } from \"langchain/document\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { SelfQueryRetriever } from \"langchain/retrievers/self_query\";\nimport { FunctionalTranslator } from \"langchain/retrievers/self_query/functional\";\nimport { OpenAI } from \"langchain/llms/openai\";\n\n/**\n * First, we create a bunch of documents. You can load your own documents here instead.\n * Each document has a pageContent and a metadata field. Make sure your metadata matches the AttributeInfo below.\n */\nconst docs = [\n  new Document({\n    pageContent:\n      \"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n    metadata: { year: 1993, rating: 7.7, genre: \"science fiction\" },\n  }),\n  new Document({\n    pageContent:\n      \"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",\n    metadata: { year: 2010, director: \"Christopher Nolan\", rating: 8.2 },\n  }),\n  new Document({\n    pageContent:\n      \"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",\n    metadata: { year: 2006, director: \"Satoshi Kon\", rating: 8.6 },\n  }),\n  new Document({\n    pageContent:\n      \"A bunch of normal-sized women are supremely wholesome and some men pine after them\",\n    metadata: { year: 2019, director: \"Greta Gerwig\", rating: 8.3 },\n  }),\n  new Document({\n    pageContent: \"Toys come alive and have a blast doing so\",\n    metadata: { year: 1995, genre: \"animated\" },\n  }),\n  new Document({\n    pageContent: \"Three men walk into the Zone, three men walk out of the Zone\",\n    metadata: {\n      year: 1979,\n      director: \"Andrei Tarkovsky\",\n      genre: \"science fiction\",\n      rating: 9.9,\n    },\n  }),\n];\n\n/**\n * Next, we define the attributes we want to be able to query on.\n * in this case, we want to be able to query on the genre, year, director, rating, and length of the movie.\n * We also provide a description of each attribute and the type of the attribute.\n * This is used to generate the query prompts.\n */\nconst attributeInfo: AttributeInfo[] = [\n  {\n    name: \"genre\",\n    description: \"The genre of the movie\",\n    type: \"string or array of strings\",\n  },\n  {\n    name: \"year\",\n    description: \"The year the movie was released\",\n    type: \"number\",\n  },\n  {\n    name: \"director\",\n    description: \"The director of the movie\",\n    type: \"string\",\n  },\n  {\n    name: \"rating\",\n    description: \"The rating of the movie (1-10)\",\n    type: \"number\",\n  },\n  {\n    name: \"length\",\n    description: \"The length of the movie in minutes\",\n    type: \"number\",\n  },\n];\n\n/**\n * Next, we instantiate a vector store. This is where we store the embeddings of the documents.\n * We also need to provide an embeddings object. This is used to embed the documents.\n */\nconst embeddings = new OpenAIEmbeddings();\nconst llm = new OpenAI();\nconst documentContents = \"Brief summary of a movie\";\nconst vectorStore = await HNSWLib.fromDocuments(docs, embeddings);\nconst selfQueryRetriever = await SelfQueryRetriever.fromLLM({\n  llm,\n  vectorStore,\n  documentContents,\n  attributeInfo,\n  /**\n   * We need to use a translator that translates the queries into a\n   * filter format that the vector store can understand. We provide a basic translator\n   * translator here, but you can create your own translator by extending BaseTranslator\n   * abstract class. Note that the vector store needs to support filtering on the metadata\n   * attributes you want to query on.\n   */\n  structuredQueryTranslator: new FunctionalTranslator(),\n});\n\n/**\n * Now we can query the vector store.\n * We can ask questions like \"Which movies are less than 90 minutes?\" or \"Which movies are rated higher than 8.5?\".\n * We can also ask questions like \"Which movies are either comedy or drama and are less than 90 minutes?\".\n * The retriever will automatically convert these questions into queries that can be used to retrieve documents.\n */","metadata":{"source":"examples/src/retrievers/hnswlib_self_query.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":111}}}}],["6705ac36-bea3-4faa-a201-5e67cfaa29dd",{"pageContent":"const query1 = await selfQueryRetriever.getRelevantDocuments(\n  \"Which movies are less than 90 minutes?\"\n);\nconst query2 = await selfQueryRetriever.getRelevantDocuments(\n  \"Which movies are rated higher than 8.5?\"\n);\nconst query3 = await selfQueryRetriever.getRelevantDocuments(\n  \"Which movies are directed by Greta Gerwig?\"\n);\nconst query4 = await selfQueryRetriever.getRelevantDocuments(\n  \"Which movies are either comedy or drama and are less than 90 minutes?\"\n);\nconsole.log(query1, query2, query3, query4);","metadata":{"source":"examples/src/retrievers/hnswlib_self_query.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":112,"to":124}}}}],["27790a50-e56f-4fcb-971f-1598e36f7d2b",{"pageContent":"import { OpenAI } from \"langchain/llms/openai\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\nimport { HydeRetriever } from \"langchain/retrievers/hyde\";\nimport { Document } from \"langchain/document\";\n\nconst embeddings = new OpenAIEmbeddings();\nconst vectorStore = new MemoryVectorStore(embeddings);\nconst llm = new OpenAI();\nconst retriever = new HydeRetriever({\n  vectorStore,\n  llm,\n  k: 1,\n});\n\nawait vectorStore.addDocuments(\n  [\n    \"My name is John.\",\n    \"My name is Bob.\",\n    \"My favourite food is pizza.\",\n    \"My favourite food is pasta.\",\n  ].map((pageContent) => new Document({ pageContent }))\n);\n\nconst results = await retriever.getRelevantDocuments(\n  \"What is my favourite food?\"\n);\n\nconsole.log(results);\n/*\n[\n  Document { pageContent: 'My favourite food is pasta.', metadata: {} }\n]\n*/","metadata":{"source":"examples/src/retrievers/hyde.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":34}}}}],["45b0af90-1152-4bcc-9364-9759fcf9531d",{"pageContent":"import { AmazonKendraRetriever } from \"langchain/retrievers/amazon_kendra\";\n\nconst retriever = new AmazonKendraRetriever({\n  topK: 10,\n  indexId: \"YOUR_INDEX_ID\",\n  region: \"us-east-2\", // Your region\n  clientOptions: {\n    credentials: {\n      accessKeyId: \"YOUR_ACCESS_KEY_ID\",\n      secretAccessKey: \"YOUR_SECRET_ACCESS_KEY\",\n    },\n  },\n});\n\nconst docs = await retriever.getRelevantDocuments(\"How are clouds formed?\");\n\nconsole.log(docs);","metadata":{"source":"examples/src/retrievers/kendra.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":17}}}}],["1f93f6d7-0853-4de4-b575-2d49c7736238",{"pageContent":"import { MemoryVectorStore } from \"langchain/vectorstores/memory\";\nimport { AttributeInfo } from \"langchain/schema/query_constructor\";\nimport { Document } from \"langchain/document\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { SelfQueryRetriever } from \"langchain/retrievers/self_query\";\nimport { FunctionalTranslator } from \"langchain/retrievers/self_query/functional\";\nimport { OpenAI } from \"langchain/llms/openai\";\n\n/**\n * First, we create a bunch of documents. You can load your own documents here instead.\n * Each document has a pageContent and a metadata field. Make sure your metadata matches the AttributeInfo below.\n */\nconst docs = [\n  new Document({\n    pageContent:\n      \"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n    metadata: { year: 1993, rating: 7.7, genre: \"science fiction\" },\n  }),\n  new Document({\n    pageContent:\n      \"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",\n    metadata: { year: 2010, director: \"Christopher Nolan\", rating: 8.2 },\n  }),\n  new Document({\n    pageContent:\n      \"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",\n    metadata: { year: 2006, director: \"Satoshi Kon\", rating: 8.6 },\n  }),\n  new Document({\n    pageContent:\n      \"A bunch of normal-sized women are supremely wholesome and some men pine after them\",\n    metadata: { year: 2019, director: \"Greta Gerwig\", rating: 8.3 },\n  }),\n  new Document({\n    pageContent: \"Toys come alive and have a blast doing so\",\n    metadata: { year: 1995, genre: \"animated\" },\n  }),\n  new Document({\n    pageContent: \"Three men walk into the Zone, three men walk out of the Zone\",\n    metadata: {\n      year: 1979,\n      director: \"Andrei Tarkovsky\",\n      genre: \"science fiction\",\n      rating: 9.9,\n    },\n  }),\n];\n\n/**\n * Next, we define the attributes we want to be able to query on.\n * in this case, we want to be able to query on the genre, year, director, rating, and length of the movie.\n * We also provide a description of each attribute and the type of the attribute.\n * This is used to generate the query prompts.\n */\nconst attributeInfo: AttributeInfo[] = [\n  {\n    name: \"genre\",\n    description: \"The genre of the movie\",\n    type: \"string or array of strings\",\n  },\n  {\n    name: \"year\",\n    description: \"The year the movie was released\",\n    type: \"number\",\n  },\n  {\n    name: \"director\",\n    description: \"The director of the movie\",\n    type: \"string\",\n  },\n  {\n    name: \"rating\",\n    description: \"The rating of the movie (1-10)\",\n    type: \"number\",\n  },\n  {\n    name: \"length\",\n    description: \"The length of the movie in minutes\",\n    type: \"number\",\n  },\n];\n\n/**\n * Next, we instantiate a vector store. This is where we store the embeddings of the documents.\n * We also need to provide an embeddings object. This is used to embed the documents.\n */\nconst embeddings = new OpenAIEmbeddings();\nconst llm = new OpenAI();\nconst documentContents = \"Brief summary of a movie\";\nconst vectorStore = await MemoryVectorStore.fromDocuments(docs, embeddings);\nconst selfQueryRetriever = await SelfQueryRetriever.fromLLM({\n  llm,\n  vectorStore,\n  documentContents,\n  attributeInfo,\n  /**\n   * We need to use a translator that translates the queries into a\n   * filter format that the vector store can understand. We provide a basic translator\n   * translator here, but you can create your own translator by extending BaseTranslator\n   * abstract class. Note that the vector store needs to support filtering on the metadata\n   * attributes you want to query on.\n   */\n  structuredQueryTranslator: new FunctionalTranslator(),\n});\n\n/**\n * Now we can query the vector store.\n * We can ask questions like \"Which movies are less than 90 minutes?\" or \"Which movies are rated higher than 8.5?\".\n * We can also ask questions like \"Which movies are either comedy or drama and are less than 90 minutes?\".\n * The retriever will automatically convert these questions into queries that can be used to retrieve documents.\n */","metadata":{"source":"examples/src/retrievers/memory_self_query.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":111}}}}],["f793822c-423e-4a61-b521-8e83903e1953",{"pageContent":"const query1 = await selfQueryRetriever.getRelevantDocuments(\n  \"Which movies are less than 90 minutes?\"\n);\nconst query2 = await selfQueryRetriever.getRelevantDocuments(\n  \"Which movies are rated higher than 8.5?\"\n);\nconst query3 = await selfQueryRetriever.getRelevantDocuments(\n  \"Which movies are directed by Greta Gerwig?\"\n);\nconst query4 = await selfQueryRetriever.getRelevantDocuments(\n  \"Which movies are either comedy or drama and are less than 90 minutes?\"\n);\nconsole.log(query1, query2, query3, query4);","metadata":{"source":"examples/src/retrievers/memory_self_query.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":112,"to":124}}}}],["c0f39c8e-7bb8-4948-be68-f4a874189eb0",{"pageContent":"/* eslint-disable @typescript-eslint/no-non-null-assertion */\nimport Metal from \"@getmetal/metal-sdk\";\nimport { MetalRetriever } from \"langchain/retrievers/metal\";\n\nexport const run = async () => {\n  const MetalSDK = Metal;\n\n  const client = new MetalSDK(\n    process.env.METAL_API_KEY!,\n    process.env.METAL_CLIENT_ID!,\n    process.env.METAL_INDEX_ID\n  );\n  const retriever = new MetalRetriever({ client });\n\n  const docs = await retriever.getRelevantDocuments(\"hello\");\n\n  console.log(docs);\n};","metadata":{"source":"examples/src/retrievers/metal.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":18}}}}],["22224a2b-9b87-4f26-a389-bb80a19b0d31",{"pageContent":"import { MemoryVectorStore } from \"langchain/vectorstores/memory\";\nimport { CohereEmbeddings } from \"langchain/embeddings/cohere\";\nimport { ChatAnthropic } from \"langchain/chat_models/anthropic\";\nimport { MultiQueryRetriever } from \"langchain/retrievers/multi_query\";\n\nconst vectorstore = await MemoryVectorStore.fromTexts(\n  [\n    \"Buildings are made out of brick\",\n    \"Buildings are made out of wood\",\n    \"Buildings are made out of stone\",\n    \"Cars are made out of metal\",\n    \"Cars are made out of plastic\",\n    \"mitochondria is the powerhouse of the cell\",\n    \"mitochondria is made of lipids\",\n  ],\n  [{ id: 1 }, { id: 2 }, { id: 3 }, { id: 4 }, { id: 5 }],\n  new CohereEmbeddings()\n);\nconst model = new ChatAnthropic({});\nconst retriever = MultiQueryRetriever.fromLLM({\n  llm: model,\n  retriever: vectorstore.asRetriever(),\n  verbose: true,\n});\n\nconst query = \"What are mitochondria made of?\";\nconst retrievedDocs = await retriever.getRelevantDocuments(query);\n\n/*\n  Generated queries: What are the components of mitochondria?,What substances comprise the mitochondria organelle?  ,What is the molecular composition of mitochondria?\n*/\n\nconsole.log(retrievedDocs);\n\n/*\n  [\n    Document {\n      pageContent: 'mitochondria is the powerhouse of the cell',\n      metadata: {}\n    },\n    Document {\n      pageContent: 'mitochondria is made of lipids',\n      metadata: {}\n    },\n    Document {\n      pageContent: 'Buildings are made out of brick',\n      metadata: { id: 1 }\n    },\n    Document {\n      pageContent: 'Buildings are made out of wood',\n      metadata: { id: 2 }\n    }\n  ]\n*/","metadata":{"source":"examples/src/retrievers/multi_query.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":54}}}}],["7b174de6-10bd-4967-a1a6-91af7c31ee2d",{"pageContent":"import { MemoryVectorStore } from \"langchain/vectorstores/memory\";\nimport { CohereEmbeddings } from \"langchain/embeddings/cohere\";\nimport { ChatAnthropic } from \"langchain/chat_models/anthropic\";\nimport { MultiQueryRetriever } from \"langchain/retrievers/multi_query\";\nimport { BaseOutputParser } from \"langchain/schema/output_parser\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport { LLMChain } from \"langchain/chains\";\nimport { pull } from \"langchain/hub\";\n\ntype LineList = {\n  lines: string[];\n};\n\nclass LineListOutputParser extends BaseOutputParser<LineList> {\n  static lc_name() {\n    return \"LineListOutputParser\";\n  }\n\n  lc_namespace = [\"langchain\", \"retrievers\", \"multiquery\"];\n\n  async parse(text: string): Promise<LineList> {\n    const startKeyIndex = text.indexOf(\"<questions>\");\n    const endKeyIndex = text.indexOf(\"</questions>\");\n    const questionsStartIndex =\n      startKeyIndex === -1 ? 0 : startKeyIndex + \"<questions>\".length;\n    const questionsEndIndex = endKeyIndex === -1 ? text.length : endKeyIndex;\n    const lines = text\n      .slice(questionsStartIndex, questionsEndIndex)\n      .trim()\n      .split(\"\\n\")\n      .filter((line) => line.trim() !== \"\");\n    return { lines };\n  }\n\n  getFormatInstructions(): string {\n    throw new Error(\"Not implemented.\");\n  }\n}\n\n// Default prompt is available at: https://smith.langchain.com/hub/jacob/multi-vector-retriever\nconst prompt: PromptTemplate = await pull(\n  \"jacob/multi-vector-retriever-german\"\n);\n\nconst vectorstore = await MemoryVectorStore.fromTexts(\n  [\n    \"Gebäude werden aus Ziegelsteinen hergestellt\",\n    \"Gebäude werden aus Holz hergestellt\",\n    \"Gebäude werden aus Stein hergestellt\",\n    \"Autos werden aus Metall hergestellt\",\n    \"Autos werden aus Kunststoff hergestellt\",\n    \"Mitochondrien sind die Energiekraftwerke der Zelle\",\n    \"Mitochondrien bestehen aus Lipiden\",\n  ],\n  [{ id: 1 }, { id: 2 }, { id: 3 }, { id: 4 }, { id: 5 }],\n  new CohereEmbeddings()\n);\nconst model = new ChatAnthropic({});\nconst llmChain = new LLMChain({\n  llm: model,\n  prompt,\n  outputParser: new LineListOutputParser(),\n});\nconst retriever = new MultiQueryRetriever({\n  retriever: vectorstore.asRetriever(),\n  llmChain,\n  verbose: true,\n});\n\nconst query = \"What are mitochondria made of?\";\nconst retrievedDocs = await retriever.getRelevantDocuments(query);\n\n/*\n  Generated queries: Was besteht ein Mitochondrium?,Aus welchen Komponenten setzt sich ein Mitochondrium zusammen?  ,Welche Moleküle finden sich in einem Mitochondrium?\n*/\n\nconsole.log(retrievedDocs);\n\n/*\n  [\n    Document {\n      pageContent: 'Mitochondrien bestehen aus Lipiden',\n      metadata: {}\n    },\n    Document {\n      pageContent: 'Mitochondrien sind die Energiekraftwerke der Zelle',\n      metadata: {}\n    },\n    Document {\n      pageContent: 'Autos werden aus Metall hergestellt',\n      metadata: { id: 4 }\n    },\n    Document {\n      pageContent: 'Gebäude werden aus Holz hergestellt',\n      metadata: { id: 2 }\n    },\n    Document {\n      pageContent: 'Gebäude werden aus Ziegelsteinen hergestellt',\n      metadata: { id: 1 }\n    }\n  ]\n*/","metadata":{"source":"examples/src/retrievers/multi_query_custom.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":102}}}}],["330ea531-709d-4ebb-842a-7e7490503952",{"pageContent":"import * as uuid from \"uuid\";\n\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport { RunnableSequence } from \"langchain/schema/runnable\";\n\nimport { MultiVectorRetriever } from \"langchain/retrievers/multi_vector\";\nimport { FaissStore } from \"langchain/vectorstores/faiss\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport { InMemoryStore } from \"langchain/storage/in_memory\";\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\nimport { Document } from \"langchain/document\";\nimport { JsonKeyOutputFunctionsParser } from \"langchain/output_parsers\";\n\nconst textLoader = new TextLoader(\"../examples/state_of_the_union.txt\");\nconst parentDocuments = await textLoader.load();\n\nconst splitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 10000,\n  chunkOverlap: 20,\n});\n\nconst docs = await splitter.splitDocuments(parentDocuments);\n\nconst functionsSchema = [\n  {\n    name: \"hypothetical_questions\",\n    description: \"Generate hypothetical questions\",\n    parameters: {\n      type: \"object\",\n      properties: {\n        questions: {\n          type: \"array\",\n          items: {\n            type: \"string\",\n          },\n        },\n      },\n      required: [\"questions\"],\n    },\n  },\n];\n\nconst functionCallingModel = new ChatOpenAI({\n  maxRetries: 0,\n  modelName: \"gpt-4\",\n}).bind({\n  functions: functionsSchema,\n  function_call: { name: \"hypothetical_questions\" },\n});\n\nconst chain = RunnableSequence.from([\n  { content: (doc: Document) => doc.pageContent },\n  PromptTemplate.fromTemplate(\n    `Generate a list of 3 hypothetical questions that the below document could be used to answer:\\n\\n{content}`\n  ),\n  functionCallingModel,\n  new JsonKeyOutputFunctionsParser<string[]>({ attrName: \"questions\" }),\n]);\n\nconst hypotheticalQuestions = await chain.batch(\n  docs,\n  {},\n  {\n    maxConcurrency: 5,\n  }\n);\n\nconst idKey = \"doc_id\";\nconst docIds = docs.map((_) => uuid.v4());\nconst hypotheticalQuestionDocs = hypotheticalQuestions\n  .map((questionArray, i) => {\n    const questionDocuments = questionArray.map((question) => {\n      const questionDocument = new Document({\n        pageContent: question,\n        metadata: {\n          [idKey]: docIds[i],\n        },\n      });\n      return questionDocument;\n    });\n    return questionDocuments;\n  })\n  .flat();\n\nconst keyValuePairs: [string, Document][] = docs.map((originalDoc, i) => [\n  docIds[i],\n  originalDoc,\n]);\n\n// The docstore to use to store the original chunks\nconst docstore = new InMemoryStore();\nawait docstore.mset(keyValuePairs);\n\n// The vectorstore to use to index the child chunks\nconst vectorstore = await FaissStore.fromDocuments(\n  hypotheticalQuestionDocs,\n  new OpenAIEmbeddings()\n);\n\nconst retriever = new MultiVectorRetriever({\n  vectorstore,\n  docstore,\n  idKey,\n});\n\n// We could also add the original chunks to the vectorstore if we wish\n// const taggedOriginalDocs = docs.map((doc, i) => {\n//   doc.metadata[idKey] = docIds[i];\n//   return doc;\n// });\n// retriever.vectorstore.addDocuments(taggedOriginalDocs);\n\n// Vectorstore alone retrieves the small chunks\nconst vectorstoreResult = await retriever.vectorstore.similaritySearch(\n  \"justice breyer\"\n);\nconsole.log(vectorstoreResult[0].pageContent);\n/*\n  \"What measures will be taken to crack down on corporations overcharging American businesses and consumers?\"\n*/\n\n// Retriever returns larger result\nconst retrieverResult = await retriever.getRelevantDocuments(\"justice breyer\");\nconsole.log(retrieverResult[0].pageContent.length);\n/*\n  9770\n*/","metadata":{"source":"examples/src/retrievers/multi_vector_hypothetical.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":129}}}}],["4437fa29-4054-4763-9001-0b52fd1ef67e",{"pageContent":"import * as uuid from \"uuid\";\n\nimport { MultiVectorRetriever } from \"langchain/retrievers/multi_vector\";\nimport { FaissStore } from \"langchain/vectorstores/faiss\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport { InMemoryStore } from \"langchain/storage/in_memory\";\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\nimport { Document } from \"langchain/document\";\n\nconst textLoader = new TextLoader(\"../examples/state_of_the_union.txt\");\nconst parentDocuments = await textLoader.load();\n\nconst splitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 10000,\n  chunkOverlap: 20,\n});\n\nconst docs = await splitter.splitDocuments(parentDocuments);\n\nconst idKey = \"doc_id\";\nconst docIds = docs.map((_) => uuid.v4());\n\nconst childSplitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 400,\n  chunkOverlap: 0,\n});\n\nconst subDocs = [];\nfor (let i = 0; i < docs.length; i += 1) {\n  const childDocs = await childSplitter.splitDocuments([docs[i]]);\n  const taggedChildDocs = childDocs.map((childDoc) => {\n    // eslint-disable-next-line no-param-reassign\n    childDoc.metadata[idKey] = docIds[i];\n    return childDoc;\n  });\n  subDocs.push(...taggedChildDocs);\n}\n\nconst keyValuePairs: [string, Document][] = docs.map((doc, i) => [\n  docIds[i],\n  doc,\n]);\n\n// The docstore to use to store the original chunks\nconst docstore = new InMemoryStore();\nawait docstore.mset(keyValuePairs);\n\n// The vectorstore to use to index the child chunks\nconst vectorstore = await FaissStore.fromDocuments(\n  subDocs,\n  new OpenAIEmbeddings()\n);\n\nconst retriever = new MultiVectorRetriever({\n  vectorstore,\n  docstore,\n  idKey,\n  // Optional `k` parameter to search for more child documents in VectorStore.\n  // Note that this does not exactly correspond to the number of final (parent) documents\n  // retrieved, as multiple child documents can point to the same parent.\n  childK: 20,\n  // Optional `k` parameter to limit number of final, parent documents returned from this\n  // retriever and sent to LLM. This is an upper-bound, and the final count may be lower than this.\n  parentK: 5,\n});\n\n// Vectorstore alone retrieves the small chunks\nconst vectorstoreResult = await retriever.vectorstore.similaritySearch(\n  \"justice breyer\"\n);\nconsole.log(vectorstoreResult[0].pageContent.length);\n/*\n  390\n*/\n\n// Retriever returns larger result\nconst retrieverResult = await retriever.getRelevantDocuments(\"justice breyer\");\nconsole.log(retrieverResult[0].pageContent.length);\n/*\n  9770\n*/","metadata":{"source":"examples/src/retrievers/multi_vector_small_chunks.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":82}}}}],["029b838f-b6ab-4b74-9b7e-6af947dc4954",{"pageContent":"import * as uuid from \"uuid\";\n\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport { StringOutputParser } from \"langchain/schema/output_parser\";\nimport { RunnableSequence } from \"langchain/schema/runnable\";\n\nimport { MultiVectorRetriever } from \"langchain/retrievers/multi_vector\";\nimport { FaissStore } from \"langchain/vectorstores/faiss\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport { InMemoryStore } from \"langchain/storage/in_memory\";\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\nimport { Document } from \"langchain/document\";\n\nconst textLoader = new TextLoader(\"../examples/state_of_the_union.txt\");\nconst parentDocuments = await textLoader.load();\n\nconst splitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 10000,\n  chunkOverlap: 20,\n});\n\nconst docs = await splitter.splitDocuments(parentDocuments);\n\nconst chain = RunnableSequence.from([\n  { content: (doc: Document) => doc.pageContent },\n  PromptTemplate.fromTemplate(`Summarize the following document:\\n\\n{content}`),\n  new ChatOpenAI({\n    maxRetries: 0,\n  }),\n  new StringOutputParser(),\n]);\n\nconst summaries = await chain.batch(\n  docs,\n  {},\n  {\n    maxConcurrency: 5,\n  }\n);\n\nconst idKey = \"doc_id\";\nconst docIds = docs.map((_) => uuid.v4());\nconst summaryDocs = summaries.map((summary, i) => {\n  const summaryDoc = new Document({\n    pageContent: summary,\n    metadata: {\n      [idKey]: docIds[i],\n    },\n  });\n  return summaryDoc;\n});\n\nconst keyValuePairs: [string, Document][] = docs.map((originalDoc, i) => [\n  docIds[i],\n  originalDoc,\n]);\n\n// The docstore to use to store the original chunks\nconst docstore = new InMemoryStore();\nawait docstore.mset(keyValuePairs);\n\n// The vectorstore to use to index the child chunks\nconst vectorstore = await FaissStore.fromDocuments(\n  summaryDocs,\n  new OpenAIEmbeddings()\n);\n\nconst retriever = new MultiVectorRetriever({\n  vectorstore,\n  docstore,\n  idKey,\n});\n\n// We could also add the original chunks to the vectorstore if we wish\n// const taggedOriginalDocs = docs.map((doc, i) => {\n//   doc.metadata[idKey] = docIds[i];\n//   return doc;\n// });\n// retriever.vectorstore.addDocuments(taggedOriginalDocs);\n\n// Vectorstore alone retrieves the small chunks\nconst vectorstoreResult = await retriever.vectorstore.similaritySearch(\n  \"justice breyer\"\n);\nconsole.log(vectorstoreResult[0].pageContent.length);\n/*\n  1118\n*/\n\n// Retriever returns larger result\nconst retrieverResult = await retriever.getRelevantDocuments(\"justice breyer\");\nconsole.log(retrieverResult[0].pageContent.length);\n/*\n  9770\n*/","metadata":{"source":"examples/src/retrievers/multi_vector_summary.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":97}}}}],["345b71d9-b22e-4c04-bf6d-5bb477f7223b",{"pageContent":"import { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\nimport { InMemoryStore } from \"langchain/storage/in_memory\";\nimport { ParentDocumentRetriever } from \"langchain/retrievers/parent_document\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\n\nconst vectorstore = new MemoryVectorStore(new OpenAIEmbeddings());\nconst docstore = new InMemoryStore();\nconst retriever = new ParentDocumentRetriever({\n  vectorstore,\n  docstore,\n  // Optional, not required if you're already passing in split documents\n  parentSplitter: new RecursiveCharacterTextSplitter({\n    chunkOverlap: 0,\n    chunkSize: 500,\n  }),\n  childSplitter: new RecursiveCharacterTextSplitter({\n    chunkOverlap: 0,\n    chunkSize: 50,\n  }),\n  // Optional `k` parameter to search for more child documents in VectorStore.\n  // Note that this does not exactly correspond to the number of final (parent) documents\n  // retrieved, as multiple child documents can point to the same parent.\n  childK: 20,\n  // Optional `k` parameter to limit number of final, parent documents returned from this\n  // retriever and sent to LLM. This is an upper-bound, and the final count may be lower than this.\n  parentK: 5,\n});\nconst textLoader = new TextLoader(\"../examples/state_of_the_union.txt\");\nconst parentDocuments = await textLoader.load();\n\n// We must add the parent documents via the retriever's addDocuments method\nawait retriever.addDocuments(parentDocuments);\n\nconst retrievedDocs = await retriever.getRelevantDocuments(\"justice breyer\");\n\n// Retrieved chunks are the larger parent chunks\nconsole.log(retrievedDocs);\n/*\n  [\n    Document {\n      pageContent: 'Tonight, I call on the Senate to pass — pass the Freedom to Vote Act. Pass the John Lewis Act — Voting Rights Act. And while you’re at it, pass the DISCLOSE Act so Americans know who is funding our elections.\\n' +\n        '\\n' +\n        'Look, tonight, I’d — I’d like to honor someone who has dedicated his life to serve this country: Justice Breyer — an Army veteran, Constitutional scholar, retiring Justice of the United States Supreme Court.',\n      metadata: { source: '../examples/state_of_the_union.txt', loc: [Object] }\n    },\n    Document {\n      pageContent: 'As I did four days ago, I’ve nominated a Circuit Court of Appeals — Ketanji Brown Jackson. One of our nation’s top legal minds who will continue in just Brey- — Justice Breyer’s legacy of excellence. A former top litigator in private practice, a former federal public defender from a family of public-school educators and police officers — she’s a consensus builder.',\n      metadata: { source: '../examples/state_of_the_union.txt', loc: [Object] }\n    },\n    Document {\n      pageContent: 'Justice Breyer, thank you for your service. Thank you, thank you, thank you. I mean it. Get up. Stand — let me see you. Thank you.\\n' +\n        '\\n' +\n        'And we all know — no matter what your ideology, we all know one of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.',\n      metadata: { source: '../examples/state_of_the_union.txt', loc: [Object] }\n    }\n  ]\n*/","metadata":{"source":"examples/src/retrievers/parent_document_retriever.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":59}}}}],["ebe2849a-1e4e-4a62-9e28-f525879aec97",{"pageContent":"import { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\nimport { InMemoryStore } from \"langchain/storage/in_memory\";\nimport { ParentDocumentRetriever } from \"langchain/retrievers/parent_document\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\nimport { ScoreThresholdRetriever } from \"langchain/retrievers/score_threshold\";\n\nconst vectorstore = new MemoryVectorStore(new OpenAIEmbeddings());\nconst docstore = new InMemoryStore();\nconst childDocumentRetriever = ScoreThresholdRetriever.fromVectorStore(\n  vectorstore,\n  {\n    minSimilarityScore: 0.01, // Essentially no threshold\n    maxK: 1, // Only return the top result\n  }\n);\nconst retriever = new ParentDocumentRetriever({\n  vectorstore,\n  docstore,\n  childDocumentRetriever,\n  // Optional, not required if you're already passing in split documents\n  parentSplitter: new RecursiveCharacterTextSplitter({\n    chunkOverlap: 0,\n    chunkSize: 500,\n  }),\n  childSplitter: new RecursiveCharacterTextSplitter({\n    chunkOverlap: 0,\n    chunkSize: 50,\n  }),\n});\nconst textLoader = new TextLoader(\"../examples/state_of_the_union.txt\");\nconst parentDocuments = await textLoader.load();\n\n// We must add the parent documents via the retriever's addDocuments method\nawait retriever.addDocuments(parentDocuments);\n\nconst retrievedDocs = await retriever.getRelevantDocuments(\"justice breyer\");\n\n// Retrieved chunk is the larger parent chunk\nconsole.log(retrievedDocs);\n/*\n  [\n    Document {\n      pageContent: 'Tonight, I call on the Senate to pass — pass the Freedom to Vote Act. Pass the John Lewis Act — Voting Rights Act. And while you’re at it, pass the DISCLOSE Act so Americans know who is funding our elections.\\n' +\n        '\\n' +\n        'Look, tonight, I’d — I’d like to honor someone who has dedicated his life to serve this country: Justice Breyer — an Army veteran, Constitutional scholar, retiring Justice of the United States Supreme Court.',\n      metadata: { source: '../examples/state_of_the_union.txt', loc: [Object] }\n    },\n  ]\n*/","metadata":{"source":"examples/src/retrievers/parent_document_retriever_score_threshold.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":51}}}}],["d24516d2-9468-4b86-82bd-fc5ef9817d12",{"pageContent":"import { Pinecone } from \"@pinecone-database/pinecone\";\nimport { AttributeInfo } from \"langchain/schema/query_constructor\";\nimport { Document } from \"langchain/document\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { SelfQueryRetriever } from \"langchain/retrievers/self_query\";\nimport { PineconeTranslator } from \"langchain/retrievers/self_query/pinecone\";\nimport { PineconeStore } from \"langchain/vectorstores/pinecone\";\nimport { OpenAI } from \"langchain/llms/openai\";\n\n/**\n * First, we create a bunch of documents. You can load your own documents here instead.\n * Each document has a pageContent and a metadata field. Make sure your metadata matches the AttributeInfo below.\n */\nconst docs = [\n  new Document({\n    pageContent:\n      \"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n    metadata: { year: 1993, rating: 7.7, genre: \"science fiction\" },\n  }),\n  new Document({\n    pageContent:\n      \"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",\n    metadata: { year: 2010, director: \"Christopher Nolan\", rating: 8.2 },\n  }),\n  new Document({\n    pageContent:\n      \"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",\n    metadata: { year: 2006, director: \"Satoshi Kon\", rating: 8.6 },\n  }),\n  new Document({\n    pageContent:\n      \"A bunch of normal-sized women are supremely wholesome and some men pine after them\",\n    metadata: { year: 2019, director: \"Greta Gerwig\", rating: 8.3 },\n  }),\n  new Document({\n    pageContent: \"Toys come alive and have a blast doing so\",\n    metadata: { year: 1995, genre: \"animated\" },\n  }),\n  new Document({\n    pageContent: \"Three men walk into the Zone, three men walk out of the Zone\",\n    metadata: {\n      year: 1979,\n      director: \"Andrei Tarkovsky\",\n      genre: \"science fiction\",\n      rating: 9.9,\n    },\n  }),\n];\n\n/**\n * Next, we define the attributes we want to be able to query on.\n * in this case, we want to be able to query on the genre, year, director, rating, and length of the movie.\n * We also provide a description of each attribute and the type of the attribute.\n * This is used to generate the query prompts.\n */\nconst attributeInfo: AttributeInfo[] = [\n  {\n    name: \"genre\",\n    description: \"The genre of the movie\",\n    type: \"string or array of strings\",\n  },\n  {\n    name: \"year\",\n    description: \"The year the movie was released\",\n    type: \"number\",\n  },\n  {\n    name: \"director\",\n    description: \"The director of the movie\",\n    type: \"string\",\n  },\n  {\n    name: \"rating\",\n    description: \"The rating of the movie (1-10)\",\n    type: \"number\",\n  },\n  {\n    name: \"length\",\n    description: \"The length of the movie in minutes\",\n    type: \"number\",\n  },\n];\n\n/**\n * Next, we instantiate a vector store. This is where we store the embeddings of the documents.\n * We also need to provide an embeddings object. This is used to embed the documents.\n */\nif (\n  !process.env.PINECONE_API_KEY ||\n  !process.env.PINECONE_ENVIRONMENT ||\n  !process.env.PINECONE_INDEX\n) {\n  throw new Error(\n    \"PINECONE_ENVIRONMENT and PINECONE_API_KEY and PINECONE_INDEX must be set\"\n  );\n}\n\nconst pinecone = new Pinecone();\n\nconst index = pinecone.Index(process.env.PINECONE_INDEX);\n\nconst embeddings = new OpenAIEmbeddings();\nconst llm = new OpenAI();\nconst documentContents = \"Brief summary of a movie\";\nconst vectorStore = await PineconeStore.fromDocuments(docs, embeddings, {\n  pineconeIndex: index,\n});","metadata":{"source":"examples/src/retrievers/pinecone_self_query.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":107}}}}],["3f88bc12-2851-4f98-bb1e-7d333353ea6c",{"pageContent":"const selfQueryRetriever = await SelfQueryRetriever.fromLLM({\n  llm,\n  vectorStore,\n  documentContents,\n  attributeInfo,\n  /**\n   * We need to create a basic translator that translates the queries into a\n   * filter format that the vector store can understand. We provide a basic translator\n   * translator here, but you can create your own translator by extending BaseTranslator\n   * abstract class. Note that the vector store needs to support filtering on the metadata\n   * attributes you want to query on.\n   */\n  structuredQueryTranslator: new PineconeTranslator(),\n});\n\n/**\n * Now we can query the vector store.\n * We can ask questions like \"Which movies are less than 90 minutes?\" or \"Which movies are rated higher than 8.5?\".\n * We can also ask questions like \"Which movies are either comedy or drama and are less than 90 minutes?\".\n * The retriever will automatically convert these questions into queries that can be used to retrieve documents.\n */\nconst query1 = await selfQueryRetriever.getRelevantDocuments(\n  \"Which movies are less than 90 minutes?\"\n);\nconst query2 = await selfQueryRetriever.getRelevantDocuments(\n  \"Which movies are rated higher than 8.5?\"\n);\nconst query3 = await selfQueryRetriever.getRelevantDocuments(\n  \"Which movies are directed by Greta Gerwig?\"\n);\nconst query4 = await selfQueryRetriever.getRelevantDocuments(\n  \"Which movies are either comedy or drama and are less than 90 minutes?\"\n);\nconsole.log(query1, query2, query3, query4);","metadata":{"source":"examples/src/retrievers/pinecone_self_query.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":108,"to":141}}}}],["5d046db7-2eae-46ee-88dd-5f5ec1c4c48b",{"pageContent":"import { MemoryVectorStore } from \"langchain/vectorstores/memory\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { ScoreThresholdRetriever } from \"langchain/retrievers/score_threshold\";\n\nconst vectorStore = await MemoryVectorStore.fromTexts(\n  [\n    \"Buildings are made out of brick\",\n    \"Buildings are made out of wood\",\n    \"Buildings are made out of stone\",\n    \"Buildings are made out of atoms\",\n    \"Buildings are made out of building materials\",\n    \"Cars are made out of metal\",\n    \"Cars are made out of plastic\",\n  ],\n  [{ id: 1 }, { id: 2 }, { id: 3 }, { id: 4 }, { id: 5 }],\n  new OpenAIEmbeddings()\n);\n\nconst retriever = ScoreThresholdRetriever.fromVectorStore(vectorStore, {\n  minSimilarityScore: 0.9, // Finds results with at least this similarity score\n  maxK: 100, // The maximum K value to use. Use it based to your chunk size to make sure you don't run out of tokens\n  kIncrement: 2, // How much to increase K by each time. It'll fetch N results, then N + kIncrement, then N + kIncrement * 2, etc.\n});\n\nconst result = await retriever.getRelevantDocuments(\n  \"What are buildings made out of?\"\n);\n\nconsole.log(result);\n\n/*\n  [\n    Document {\n      pageContent: 'Buildings are made out of building materials',\n      metadata: { id: 5 }\n    },\n    Document {\n      pageContent: 'Buildings are made out of wood',\n      metadata: { id: 2 }\n    },\n    Document {\n      pageContent: 'Buildings are made out of brick',\n      metadata: { id: 1 }\n    },\n    Document {\n      pageContent: 'Buildings are made out of stone',\n      metadata: { id: 3 }\n    },\n    Document {\n      pageContent: 'Buildings are made out of atoms',\n      metadata: { id: 4 }\n    }\n  ]\n*/","metadata":{"source":"examples/src/retrievers/similarity_score_threshold.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":54}}}}],["68ff5d39-934b-4375-9e2c-dee4876af756",{"pageContent":"import { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { createClient } from \"@supabase/supabase-js\";\nimport { SupabaseHybridSearch } from \"langchain/retrievers/supabase\";\n\nexport const run = async () => {\n  const client = createClient(\n    process.env.SUPABASE_URL || \"\",\n    process.env.SUPABASE_PRIVATE_KEY || \"\"\n  );\n\n  const embeddings = new OpenAIEmbeddings();\n\n  const retriever = new SupabaseHybridSearch(embeddings, {\n    client,\n    //  Below are the defaults, expecting that you set up your supabase table and functions according to the guide above. Please change if necessary.\n    similarityK: 2,\n    keywordK: 2,\n    tableName: \"documents\",\n    similarityQueryName: \"match_documents\",\n    keywordQueryName: \"kw_match_documents\",\n  });\n\n  const results = await retriever.getRelevantDocuments(\"hello bye\");\n\n  console.log(results);\n};","metadata":{"source":"examples/src/retrievers/supabase_hybrid.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":26}}}}],["aa9a4fc3-07e6-4d37-9f70-096aa14f0fbd",{"pageContent":"import { createClient } from \"@supabase/supabase-js\";\n\nimport { AttributeInfo } from \"langchain/schema/query_constructor\";\nimport { Document } from \"langchain/document\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { SelfQueryRetriever } from \"langchain/retrievers/self_query\";\nimport { SupabaseTranslator } from \"langchain/retrievers/self_query/supabase\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { SupabaseVectorStore } from \"langchain/vectorstores/supabase\";\n\n/**\n * First, we create a bunch of documents. You can load your own documents here instead.\n * Each document has a pageContent and a metadata field. Make sure your metadata matches the AttributeInfo below.\n */\nconst docs = [\n  new Document({\n    pageContent:\n      \"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n    metadata: { year: 1993, rating: 7.7, genre: \"science fiction\" },\n  }),\n  new Document({\n    pageContent:\n      \"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",\n    metadata: { year: 2010, director: \"Christopher Nolan\", rating: 8.2 },\n  }),\n  new Document({\n    pageContent:\n      \"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",\n    metadata: { year: 2006, director: \"Satoshi Kon\", rating: 8.6 },\n  }),\n  new Document({\n    pageContent:\n      \"A bunch of normal-sized women are supremely wholesome and some men pine after them\",\n    metadata: { year: 2019, director: \"Greta Gerwig\", rating: 8.3 },\n  }),\n  new Document({\n    pageContent: \"Toys come alive and have a blast doing so\",\n    metadata: { year: 1995, genre: \"animated\" },\n  }),\n  new Document({\n    pageContent: \"Three men walk into the Zone, three men walk out of the Zone\",\n    metadata: {\n      year: 1979,\n      director: \"Andrei Tarkovsky\",\n      genre: \"science fiction\",\n      rating: 9.9,\n    },\n  }),\n];\n\n/**\n * Next, we define the attributes we want to be able to query on.\n * in this case, we want to be able to query on the genre, year, director, rating, and length of the movie.\n * We also provide a description of each attribute and the type of the attribute.\n * This is used to generate the query prompts.\n */\nconst attributeInfo: AttributeInfo[] = [\n  {\n    name: \"genre\",\n    description: \"The genre of the movie\",\n    type: \"string or array of strings\",\n  },\n  {\n    name: \"year\",\n    description: \"The year the movie was released\",\n    type: \"number\",\n  },\n  {\n    name: \"director\",\n    description: \"The director of the movie\",\n    type: \"string\",\n  },\n  {\n    name: \"rating\",\n    description: \"The rating of the movie (1-10)\",\n    type: \"number\",\n  },\n  {\n    name: \"length\",\n    description: \"The length of the movie in minutes\",\n    type: \"number\",\n  },\n];\n\n/**\n * Next, we instantiate a vector store. This is where we store the embeddings of the documents.\n */\nif (!process.env.SUPABASE_URL || !process.env.SUPABASE_PRIVATE_KEY) {\n  throw new Error(\n    \"Supabase URL or private key not set. Please set it in the .env file\"\n  );\n}\n\nconst embeddings = new OpenAIEmbeddings();\nconst llm = new OpenAI();\nconst documentContents = \"Brief summary of a movie\";\nconst client = createClient(\n  process.env.SUPABASE_URL,\n  process.env.SUPABASE_PRIVATE_KEY\n);\nconst vectorStore = await SupabaseVectorStore.fromDocuments(docs, embeddings, {\n  client,\n});","metadata":{"source":"examples/src/retrievers/supabase_self_query.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":103}}}}],["3beba20c-5fc9-4268-8b2e-26fdf7671e56",{"pageContent":"const selfQueryRetriever = await SelfQueryRetriever.fromLLM({\n  llm,\n  vectorStore,\n  documentContents,\n  attributeInfo,\n  /**\n   * We need to use a translator that translates the queries into a\n   * filter format that the vector store can understand. LangChain provides one here.\n   */\n  structuredQueryTranslator: new SupabaseTranslator(),\n});\n\n/**\n * Now we can query the vector store.\n * We can ask questions like \"Which movies are less than 90 minutes?\" or \"Which movies are rated higher than 8.5?\".\n * We can also ask questions like \"Which movies are either comedy or drama and are less than 90 minutes?\".\n * The retriever will automatically convert these questions into queries that can be used to retrieve documents.\n */\nconst query1 = await selfQueryRetriever.getRelevantDocuments(\n  \"Which movies are less than 90 minutes?\"\n);\nconst query2 = await selfQueryRetriever.getRelevantDocuments(\n  \"Which movies are rated higher than 8.5?\"\n);\nconst query3 = await selfQueryRetriever.getRelevantDocuments(\n  \"Which movies are directed by Greta Gerwig?\"\n);\nconst query4 = await selfQueryRetriever.getRelevantDocuments(\n  \"Which movies are either comedy or drama and are less than 90 minutes?\"\n);\nconsole.log(query1, query2, query3, query4);","metadata":{"source":"examples/src/retrievers/supabase_self_query.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":104,"to":134}}}}],["5a7eb2fb-8845-4861-b47f-5f6c7b1be43c",{"pageContent":"import { TavilySearchAPIRetriever } from \"langchain/retrievers/tavily_search_api\";\n\nconst retriever = new TavilySearchAPIRetriever({\n  k: 3,\n});\n\nconst retrievedDocs = await retriever.getRelevantDocuments(\n  \"What did the speaker say about Justice Breyer in the 2022 State of the Union?\"\n);\nconsole.log({ retrievedDocs });\n\n/*\n  {\n    retrievedDocs: [\n      Document {\n        pageContent: `Shy Justice Br eyer. During his remarks, the president paid tribute to retiring Supreme Court Justice Stephen Breyer. \"Tonight, I'd like to honor someone who dedicated his life to...`,\n        metadata: [Object]\n      },\n      Document {\n        pageContent: 'Fact Check. Ukraine. 56 Posts. Sort by. 10:16 p.m. ET, March 1, 2022. Biden recognized outgoing Supreme Court Justice Breyer during his speech. President Biden recognized outgoing...',\n        metadata: [Object]\n      },\n      Document {\n        pageContent: `In his State of the Union address on March 1, Biden thanked Breyer for his service. \"I'd like to honor someone who has dedicated his life to serve this country: Justice Breyer — an Army...`,\n        metadata: [Object]\n      }\n    ]\n  }\n*/","metadata":{"source":"examples/src/retrievers/tavily.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":29}}}}],["54311d2f-8510-4b12-bc44-47ce65c8cfd2",{"pageContent":"import { TimeWeightedVectorStoreRetriever } from \"langchain/retrievers/time_weighted\";\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nconst vectorStore = new MemoryVectorStore(new OpenAIEmbeddings());\n\nconst retriever = new TimeWeightedVectorStoreRetriever({\n  vectorStore,\n  memoryStream: [],\n  searchKwargs: 2,\n});\n\nconst documents = [\n  \"My name is John.\",\n  \"My name is Bob.\",\n  \"My favourite food is pizza.\",\n  \"My favourite food is pasta.\",\n  \"My favourite food is sushi.\",\n].map((pageContent) => ({ pageContent, metadata: {} }));\n\n// All documents must be added using this method on the retriever (not the vector store!)\n// so that the correct access history metadata is populated\nawait retriever.addDocuments(documents);\n\nconst results1 = await retriever.getRelevantDocuments(\n  \"What is my favourite food?\"\n);\n\nconsole.log(results1);\n\n/*\n[\n  Document { pageContent: 'My favourite food is pasta.', metadata: {} }\n]\n */\n\nconst results2 = await retriever.getRelevantDocuments(\n  \"What is my favourite food?\"\n);\n\nconsole.log(results2);\n\n/*\n[\n  Document { pageContent: 'My favourite food is pasta.', metadata: {} }\n]\n */","metadata":{"source":"examples/src/retrievers/time-weighted-retriever.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":47}}}}],["e6000e60-93e1-4f26-9369-51d3c97029fe",{"pageContent":"import { VespaRetriever } from \"langchain/retrievers/vespa\";\n\nexport const run = async () => {\n  const url = \"https://doc-search.vespa.oath.cloud\";\n  const query_body = {\n    yql: \"select content from paragraph where userQuery()\",\n    hits: 5,\n    ranking: \"documentation\",\n    locale: \"en-us\",\n  };\n  const content_field = \"content\";\n\n  const retriever = new VespaRetriever({\n    url,\n    auth: false,\n    query_body,\n    content_field,\n  });\n\n  const result = await retriever.getRelevantDocuments(\"what is vespa?\");\n  console.log(result);\n};","metadata":{"source":"examples/src/retrievers/vespa.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":22}}}}],["f68ab22d-dd46-4ed1-bd64-0a26567cbc82",{"pageContent":"import weaviate from \"weaviate-ts-client\";\n\nimport { AttributeInfo } from \"langchain/schema/query_constructor\";\nimport { Document } from \"langchain/document\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { SelfQueryRetriever } from \"langchain/retrievers/self_query\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { WeaviateStore } from \"langchain/vectorstores/weaviate\";\nimport { WeaviateTranslator } from \"langchain/retrievers/self_query/weaviate\";\n\n/**\n * First, we create a bunch of documents. You can load your own documents here instead.\n * Each document has a pageContent and a metadata field. Make sure your metadata matches the AttributeInfo below.\n */\nconst docs = [\n  new Document({\n    pageContent:\n      \"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n    metadata: { year: 1993, rating: 7.7, genre: \"science fiction\" },\n  }),\n  new Document({\n    pageContent:\n      \"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",\n    metadata: { year: 2010, director: \"Christopher Nolan\", rating: 8.2 },\n  }),\n  new Document({\n    pageContent:\n      \"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",\n    metadata: { year: 2006, director: \"Satoshi Kon\", rating: 8.6 },\n  }),\n  new Document({\n    pageContent:\n      \"A bunch of normal-sized women are supremely wholesome and some men pine after them\",\n    metadata: { year: 2019, director: \"Greta Gerwig\", rating: 8.3 },\n  }),\n  new Document({\n    pageContent: \"Toys come alive and have a blast doing so\",\n    metadata: { year: 1995, genre: \"animated\" },\n  }),\n  new Document({\n    pageContent: \"Three men walk into the Zone, three men walk out of the Zone\",\n    metadata: {\n      year: 1979,\n      director: \"Andrei Tarkovsky\",\n      genre: \"science fiction\",\n      rating: 9.9,\n    },\n  }),\n];\n\n/**\n * Next, we define the attributes we want to be able to query on.\n * in this case, we want to be able to query on the genre, year, director, rating, and length of the movie.\n * We also provide a description of each attribute and the type of the attribute.\n * This is used to generate the query prompts.\n */\nconst attributeInfo: AttributeInfo[] = [\n  {\n    name: \"genre\",\n    description: \"The genre of the movie\",\n    type: \"string or array of strings\",\n  },\n  {\n    name: \"year\",\n    description: \"The year the movie was released\",\n    type: \"number\",\n  },\n  {\n    name: \"director\",\n    description: \"The director of the movie\",\n    type: \"string\",\n  },\n  {\n    name: \"rating\",\n    description: \"The rating of the movie (1-10)\",\n    type: \"number\",\n  },\n  {\n    name: \"length\",\n    description: \"The length of the movie in minutes\",\n    type: \"number\",\n  },\n];\n\n/**\n * Next, we instantiate a vector store. This is where we store the embeddings of the documents.\n */\nconst embeddings = new OpenAIEmbeddings();\nconst llm = new OpenAI();\nconst documentContents = \"Brief summary of a movie\";\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nconst client = (weaviate as any).client({\n  scheme: process.env.WEAVIATE_SCHEME || \"https\",\n  host: process.env.WEAVIATE_HOST || \"localhost\",\n  apiKey: process.env.WEAVIATE_API_KEY\n    ? // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      new (weaviate as any).ApiKey(process.env.WEAVIATE_API_KEY)\n    : undefined,\n});\n\nconst vectorStore = await WeaviateStore.fromDocuments(docs, embeddings, {\n  client,\n  indexName: \"Test\",\n  textKey: \"text\",\n  metadataKeys: [\"year\", \"director\", \"rating\", \"genre\"],\n});","metadata":{"source":"examples/src/retrievers/weaviate_self_query.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":106}}}}],["82ca539f-020b-4d2f-8553-5c0e1f0bdc79",{"pageContent":"const selfQueryRetriever = await SelfQueryRetriever.fromLLM({\n  llm,\n  vectorStore,\n  documentContents,\n  attributeInfo,\n  /**\n   * We need to use a translator that translates the queries into a\n   * filter format that the vector store can understand. LangChain provides one here.\n   */\n  structuredQueryTranslator: new WeaviateTranslator(),\n});\n\n/**\n * Now we can query the vector store.\n * We can ask questions like \"Which movies are less than 90 minutes?\" or \"Which movies are rated higher than 8.5?\".\n * We can also ask questions like \"Which movies are either comedy or drama and are less than 90 minutes?\".\n * The retriever will automatically convert these questions into queries that can be used to retrieve documents.\n *\n * Note that unlike other vector stores, you have to make sure each metadata keys are actually presnt in the database,\n * meaning that Weaviate will throw an error if the self query chain generate a query with a metadata key that does\n * not exist in your Weaviate database.\n */\nconst query1 = await selfQueryRetriever.getRelevantDocuments(\n  \"Which movies are rated higher than 8.5?\"\n);\nconst query2 = await selfQueryRetriever.getRelevantDocuments(\n  \"Which movies are directed by Greta Gerwig?\"\n);\nconsole.log(query1, query2);","metadata":{"source":"examples/src/retrievers/weaviate_self_query.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":107,"to":135}}}}],["5a007e2f-112b-4861-90ca-e54cd4ee4f4e",{"pageContent":"import { ZepRetriever } from \"langchain/retrievers/zep\";\nimport { ZepMemory } from \"langchain/memory/zep\";\nimport { Memory as MemoryModel, Message } from \"@getzep/zep-js\";\nimport { randomUUID } from \"crypto\";","metadata":{"source":"examples/src/retrievers/zep.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":4}}}}],["c8dee76a-a619-4614-aa5f-2c4a475f329a",{"pageContent":"function sleep(ms: number) {\n  // eslint-disable-next-line no-promise-executor-return\n  return new Promise((resolve) => setTimeout(resolve, ms));\n}\n\nexport const run = async () => {\n  const zepConfig = {\n    url: process.env.ZEP_URL || \"http://localhost:8000\",\n    sessionId: `session_${randomUUID()}`,\n  };\n\n  console.log(`Zep Config: ${JSON.stringify(zepConfig)}`);\n\n  const memory = new ZepMemory({\n    baseURL: zepConfig.url,\n    sessionId: zepConfig.sessionId,\n  });\n\n  // Generate chat messages about traveling to France\n  const chatMessages = [\n    {\n      role: \"AI\",\n      message: \"Bonjour! How can I assist you with your travel plans today?\",\n    },\n    { role: \"User\", message: \"I'm planning a trip to France.\" },\n    {\n      role: \"AI\",\n      message: \"That sounds exciting! What cities are you planning to visit?\",\n    },\n    { role: \"User\", message: \"I'm thinking of visiting Paris and Nice.\" },\n    {\n      role: \"AI\",\n      message: \"Great choices! Are you interested in any specific activities?\",\n    },\n    { role: \"User\", message: \"I would love to visit some vineyards.\" },\n    {\n      role: \"AI\",\n      message:\n        \"France has some of the best vineyards in the world. I can help you find some.\",\n    },\n    { role: \"User\", message: \"That would be great!\" },\n    { role: \"AI\", message: \"Do you prefer red or white wine?\" },\n    { role: \"User\", message: \"I prefer red wine.\" },\n    {\n      role: \"AI\",\n      message:\n        \"Perfect! I'll find some vineyards that are known for their red wines.\",\n    },\n    { role: \"User\", message: \"Thank you, that would be very helpful.\" },\n    {\n      role: \"AI\",\n      message:\n        \"You're welcome! I'll also look up some French wine etiquette for you.\",\n    },\n    {\n      role: \"User\",\n      message: \"That sounds great. I can't wait to start my trip!\",\n    },\n    {\n      role: \"AI\",\n      message:\n        \"I'm sure you'll have a fantastic time. Do you have any other questions about your trip?\",\n    },\n    { role: \"User\", message: \"Not at the moment, thank you for your help!\" },\n  ];\n\n  const zepClient = await memory.zepClientPromise;\n  if (!zepClient) {\n    throw new Error(\"ZepClient is not initialized\");\n  }\n\n  // Add chat messages to memory\n  for (const chatMessage of chatMessages) {\n    let m: MemoryModel;\n    if (chatMessage.role === \"AI\") {\n      m = new MemoryModel({\n        messages: [new Message({ role: \"ai\", content: chatMessage.message })],\n      });\n    } else {\n      m = new MemoryModel({\n        messages: [\n          new Message({ role: \"human\", content: chatMessage.message }),\n        ],\n      });\n    }\n\n    await zepClient.memory.addMemory(zepConfig.sessionId, m);\n  }\n\n  // Wait for messages to be summarized, enriched, embedded and indexed.\n  await sleep(10000);\n\n  // Simple similarity search\n  const query = \"Can I drive red cars in France?\";\n  const retriever = new ZepRetriever({ ...zepConfig, topK: 3 });\n  const docs = await retriever.getRelevantDocuments(query);\n  console.log(\"Simple similarity search\");\n  console.log(JSON.stringify(docs, null, 2));\n\n  // mmr reranking search\n  const mmrRetriever = new ZepRetriever({\n    ...zepConfig,\n    topK: 3,\n    searchType: \"mmr\",\n    mmrLambda: 0.5,\n  });\n  const mmrDocs = await mmrRetriever.getRelevantDocuments(query);\n  console.log(\"MMR reranking search\");\n  console.log(JSON.stringify(mmrDocs, null, 2));\n\n  // summary search with mmr reranking\n  const mmrSummaryRetriever = new ZepRetriever({\n    ...zepConfig,\n    topK: 3,\n    searchScope: \"summary\",\n    searchType: \"mmr\",\n    mmrLambda: 0.5,\n  });\n  const mmrSummaryDocs = await mmrSummaryRetriever.getRelevantDocuments(query);\n  console.log(\"Summary search with MMR reranking\");\n  console.log(JSON.stringify(mmrSummaryDocs, null, 2));","metadata":{"source":"examples/src/retrievers/zep.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":6,"to":126}}}}],["7195f605-69d7-48b3-a334-d2016c269ac6",{"pageContent":"// Filtered search\n  const filteredRetriever = new ZepRetriever({\n    ...zepConfig,\n    topK: 3,\n    filter: {\n      where: { jsonpath: '$.system.entities[*] ? (@.Label == \"GPE\")' },\n    },\n  });\n  const filteredDocs = await filteredRetriever.getRelevantDocuments(query);\n  console.log(\"Filtered search\");\n  console.log(JSON.stringify(filteredDocs, null, 2));\n};","metadata":{"source":"examples/src/retrievers/zep.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":128,"to":139}}}}],["d4899df8-93ff-4a48-aa9b-dbd71c7c49e6",{"pageContent":"import fs from \"fs\";\nimport { AIMessage, HumanMessage } from \"langchain/schema\";\nimport { LocalFileStore } from \"langchain/storage/file_system\";\n\n// Instantiate the store using the `fromPath` method.\nconst store = await LocalFileStore.fromPath(\"./messages\");\n// Define our encoder/decoder for converting between strings and Uint8Arrays\nconst encoder = new TextEncoder();\nconst decoder = new TextDecoder();\n/**\n * Here you would define your LLM and chat chain, call\n * the LLM and eventually get a list of messages.\n * For this example, we'll assume we already have a list.\n */\nconst messages = Array.from({ length: 5 }).map((_, index) => {\n  if (index % 2 === 0) {\n    return new AIMessage(\"ai stuff...\");\n  }\n  return new HumanMessage(\"human stuff...\");\n});\n// Set your messages in the store\n// The key will be prefixed with `message:id:` and end\n// with the index.\nawait store.mset(\n  messages.map((message, index) => [\n    `message:id:${index}`,\n    encoder.encode(JSON.stringify(message)),\n  ])\n);\n// Now you can get your messages from the store\nconst retrievedMessages = await store.mget([\"message:id:0\", \"message:id:1\"]);\n// Make sure to decode the values\nconsole.log(retrievedMessages.map((v) => decoder.decode(v)));\n/**\n[\n  '{\"id\":[\"langchain\",\"AIMessage\"],\"kwargs\":{\"content\":\"ai stuff...\"}}',\n  '{\"id\":[\"langchain\",\"HumanMessage\"],\"kwargs\":{\"content\":\"human stuff...\"}}'\n]\n */\n// Or, if you want to get back all the keys you can call\n// the `yieldKeys` method.\n// Optionally, you can pass a key prefix to only get back\n// keys which match that prefix.\nconst yieldedKeys = [];\nfor await (const key of store.yieldKeys(\"message:id:\")) {\n  yieldedKeys.push(key);\n}\n// The keys are not encoded, so no decoding is necessary\nconsole.log(yieldedKeys);\n/**\n[\n  'message:id:2',\n  'message:id:1',\n  'message:id:3',\n  'message:id:0',\n  'message:id:4'\n]\n */\n// Finally, let's delete the keys from the store\n// and delete the file.\nawait store.mdelete(yieldedKeys);\nawait fs.promises.rm(\"./messages\", { recursive: true, force: true });","metadata":{"source":"examples/src/stores/file_system_storage.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":62}}}}],["21ccb9da-08e1-4e34-99d4-ba0667ff7366",{"pageContent":"import { AIMessage, BaseMessage, HumanMessage } from \"langchain/schema\";\nimport { InMemoryStore } from \"langchain/storage/in_memory\";\n\n// Instantiate the store using the `fromPath` method.\nconst store = new InMemoryStore<BaseMessage>();\n/**\n * Here you would define your LLM and chat chain, call\n * the LLM and eventually get a list of messages.\n * For this example, we'll assume we already have a list.\n */\nconst messages = Array.from({ length: 5 }).map((_, index) => {\n  if (index % 2 === 0) {\n    return new AIMessage(\"ai stuff...\");\n  }\n  return new HumanMessage(\"human stuff...\");\n});\n// Set your messages in the store\n// The key will be prefixed with `message:id:` and end\n// with the index.\nawait store.mset(\n  messages.map((message, index) => [`message:id:${index}`, message])\n);\n// Now you can get your messages from the store\nconst retrievedMessages = await store.mget([\"message:id:0\", \"message:id:1\"]);\nconsole.log(retrievedMessages.map((v) => v));\n/**\n[\n  AIMessage {\n    lc_kwargs: { content: 'ai stuff...', additional_kwargs: {} },\n    content: 'ai stuff...',\n    ...\n  },\n  HumanMessage {\n    lc_kwargs: { content: 'human stuff...', additional_kwargs: {} },\n    content: 'human stuff...',\n    ...\n  }\n]\n */\n// Or, if you want to get back all the keys you can call\n// the `yieldKeys` method.\n// Optionally, you can pass a key prefix to only get back\n// keys which match that prefix.\nconst yieldedKeys = [];\nfor await (const key of store.yieldKeys(\"message:id:\")) {\n  yieldedKeys.push(key);\n}\n// The keys are not encoded, so no decoding is necessary\nconsole.log(yieldedKeys);\n/**\n[\n  'message:id:0',\n  'message:id:1',\n  'message:id:2',\n  'message:id:3',\n  'message:id:4'\n]\n */\n// Finally, let's delete the keys from the store\nawait store.mdelete(yieldedKeys);","metadata":{"source":"examples/src/stores/in_memory_storage.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":60}}}}],["3a0e1662-c8b8-4e1c-b898-41e0bc192f07",{"pageContent":"import { Redis } from \"ioredis\";\nimport { AIMessage, HumanMessage } from \"langchain/schema\";\nimport { RedisByteStore } from \"langchain/storage/ioredis\";\n\n// Define the client and store\nconst client = new Redis({});\nconst store = new RedisByteStore({\n  client,\n});\n// Define our encoder/decoder for converting between strings and Uint8Arrays\nconst encoder = new TextEncoder();\nconst decoder = new TextDecoder();\n/**\n * Here you would define your LLM and chat chain, call\n * the LLM and eventually get a list of messages.\n * For this example, we'll assume we already have a list.\n */\nconst messages = Array.from({ length: 5 }).map((_, index) => {\n  if (index % 2 === 0) {\n    return new AIMessage(\"ai stuff...\");\n  }\n  return new HumanMessage(\"human stuff...\");\n});\n// Set your messages in the store\n// The key will be prefixed with `message:id:` and end\n// with the index.\nawait store.mset(\n  messages.map((message, index) => [\n    `message:id:${index}`,\n    encoder.encode(JSON.stringify(message)),\n  ])\n);\n// Now you can get your messages from the store\nconst retrievedMessages = await store.mget([\"message:id:0\", \"message:id:1\"]);\n// Make sure to decode the values\nconsole.log(retrievedMessages.map((v) => decoder.decode(v)));\n/**\n[\n  '{\"id\":[\"langchain\",\"AIMessage\"],\"kwargs\":{\"content\":\"ai stuff...\"}}',\n  '{\"id\":[\"langchain\",\"HumanMessage\"],\"kwargs\":{\"content\":\"human stuff...\"}}'\n]\n */\n// Or, if you want to get back all the keys you can call\n// the `yieldKeys` method.\n// Optionally, you can pass a key prefix to only get back\n// keys which match that prefix.\nconst yieldedKeys = [];\nfor await (const key of store.yieldKeys(\"message:id:\")) {\n  yieldedKeys.push(key);\n}\n// The keys are not encoded, so no decoding is necessary\nconsole.log(yieldedKeys);\n/**\n[\n  'message:id:2',\n  'message:id:1',\n  'message:id:3',\n  'message:id:0',\n  'message:id:4'\n]\n */\n// Finally, let's delete the keys from the store\n// and close the Redis connection.\nawait store.mdelete(yieldedKeys);\nclient.disconnect();","metadata":{"source":"examples/src/stores/ioredis_storage.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":65}}}}],["519e2b08-832c-4c48-8358-5fdf187f75ee",{"pageContent":"import { Redis } from \"@upstash/redis\";\nimport { AIMessage, HumanMessage } from \"langchain/schema\";\nimport { UpstashRedisStore } from \"langchain/storage/upstash_redis\";\n\n// Pro tip: define a helper function for getting your client\n// along with handling the case where your environment variables\n// are not set.\nconst getClient = () => {\n  if (\n    !process.env.UPSTASH_REDIS_REST_URL ||\n    !process.env.UPSTASH_REDIS_REST_TOKEN\n  ) {\n    throw new Error(\n      \"UPSTASH_REDIS_REST_URL and UPSTASH_REDIS_REST_TOKEN must be set in the environment\"\n    );\n  }\n  const client = new Redis({\n    url: process.env.UPSTASH_REDIS_REST_URL,\n    token: process.env.UPSTASH_REDIS_REST_TOKEN,\n  });\n  return client;\n};\n\n// Define the client and store\nconst client = getClient();\nconst store = new UpstashRedisStore({\n  client,\n});\n// Define our encoder/decoder for converting between strings and Uint8Arrays\nconst encoder = new TextEncoder();\nconst decoder = new TextDecoder();\n/**\n * Here you would define your LLM and chat chain, call\n * the LLM and eventually get a list of messages.\n * For this example, we'll assume we already have a list.\n */\nconst messages = Array.from({ length: 5 }).map((_, index) => {\n  if (index % 2 === 0) {\n    return new AIMessage(\"ai stuff...\");\n  }\n  return new HumanMessage(\"human stuff...\");\n});\n// Set your messages in the store\n// The key will be prefixed with `message:id:` and end\n// with the index.\nawait store.mset(\n  messages.map((message, index) => [\n    `message:id:${index}`,\n    encoder.encode(JSON.stringify(message)),\n  ])\n);\n// Now you can get your messages from the store\nconst retrievedMessages = await store.mget([\"message:id:0\", \"message:id:1\"]);\n// Make sure to decode the values\nconsole.log(retrievedMessages.map((v) => decoder.decode(v)));\n/**\n[\n  '{\"id\":[\"langchain\",\"AIMessage\"],\"kwargs\":{\"content\":\"ai stuff...\"}}',\n  '{\"id\":[\"langchain\",\"HumanMessage\"],\"kwargs\":{\"content\":\"human stuff...\"}}'\n]\n */\n// Or, if you want to get back all the keys you can call\n// the `yieldKeys` method.\n// Optionally, you can pass a key prefix to only get back\n// keys which match that prefix.\nconst yieldedKeys = [];\nfor await (const key of store.yieldKeys(\"message:id\")) {\n  yieldedKeys.push(key);\n}\n// The keys are not encoded, so no decoding is necessary\nconsole.log(yieldedKeys);\n/**\n[\n  'message:id:2',\n  'message:id:1',\n  'message:id:3',\n  'message:id:0',\n  'message:id:4'\n]\n */\n// Finally, let's delete the keys from the store\nawait store.mdelete(yieldedKeys);","metadata":{"source":"examples/src/stores/upstash_redis_storage.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":82}}}}],["9d5f38e4-6f8c-4f23-a2a8-721809bcf637",{"pageContent":"import { createClient } from \"@vercel/kv\";\nimport { AIMessage, HumanMessage } from \"langchain/schema\";\nimport { VercelKVStore } from \"langchain/storage/vercel_kv\";\n\n// Pro tip: define a helper function for getting your client\n// along with handling the case where your environment variables\n// are not set.\nconst getClient = () => {\n  if (!process.env.VERCEL_KV_API_URL || !process.env.VERCEL_KV_API_TOKEN) {\n    throw new Error(\n      \"VERCEL_KV_API_URL and VERCEL_KV_API_TOKEN must be set in the environment\"\n    );\n  }\n  const client = createClient({\n    url: process.env.VERCEL_KV_API_URL,\n    token: process.env.VERCEL_KV_API_TOKEN,\n  });\n  return client;\n};\n\n// Define the client and store\nconst client = getClient();\nconst store = new VercelKVStore({\n  client,\n});\n// Define our encoder/decoder for converting between strings and Uint8Arrays\nconst encoder = new TextEncoder();\nconst decoder = new TextDecoder();\n/**\n * Here you would define your LLM and chat chain, call\n * the LLM and eventually get a list of messages.\n * For this example, we'll assume we already have a list.\n */\nconst messages = Array.from({ length: 5 }).map((_, index) => {\n  if (index % 2 === 0) {\n    return new AIMessage(\"ai stuff...\");\n  }\n  return new HumanMessage(\"human stuff...\");\n});\n// Set your messages in the store\n// The key will be prefixed with `message:id:` and end\n// with the index.\nawait store.mset(\n  messages.map((message, index) => [\n    `message:id:${index}`,\n    encoder.encode(JSON.stringify(message)),\n  ])\n);\n// Now you can get your messages from the store\nconst retrievedMessages = await store.mget([\"message:id:0\", \"message:id:1\"]);\n// Make sure to decode the values\nconsole.log(retrievedMessages.map((v) => decoder.decode(v)));\n/**\n[\n  '{\"id\":[\"langchain\",\"AIMessage\"],\"kwargs\":{\"content\":\"ai stuff...\"}}',\n  '{\"id\":[\"langchain\",\"HumanMessage\"],\"kwargs\":{\"content\":\"human stuff...\"}}'\n]\n */\n// Or, if you want to get back all the keys you can call\n// the `yieldKeys` method.\n// Optionally, you can pass a key prefix to only get back\n// keys which match that prefix.\nconst yieldedKeys = [];\nfor await (const key of store.yieldKeys(\"message:id:\")) {\n  yieldedKeys.push(key);\n}\n// The keys are not encoded, so no decoding is necessary\nconsole.log(yieldedKeys);\n/**\n[\n  'message:id:2',\n  'message:id:1',\n  'message:id:3',\n  'message:id:0',\n  'message:id:4'\n]\n */\n// Finally, let's delete the keys from the store\nawait store.mdelete(yieldedKeys);","metadata":{"source":"examples/src/stores/vercel_kv_storage.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":79}}}}],["70581cc9-8923-4867-9c7b-6625c9695b72",{"pageContent":"import { initializeAgentExecutorWithOptions } from \"langchain/agents\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { Calculator } from \"langchain/tools/calculator\";\nimport {\n  GoogleCalendarCreateTool,\n  GoogleCalendarViewTool,\n} from \"langchain/tools/google_calendar\";\n\nexport async function run() {\n  const model = new OpenAI({\n    temperature: 0,\n    openAIApiKey: process.env.OPENAI_API_KEY,\n  });\n\n  const googleCalendarParams = {\n    credentials: {\n      clientEmail: process.env.GOOGLE_CALENDAR_CLIENT_EMAIL,\n      privateKey: process.env.GOOGLE_CALENDAR_PRIVATE_KEY,\n      calendarId: process.env.GOOGLE_CALENDAR_CALENDAR_ID,\n    },\n    scopes: [\n      \"https://www.googleapis.com/auth/calendar\",\n      \"https://www.googleapis.com/auth/calendar.events\",\n    ],\n    model,\n  };\n\n  const tools = [\n    new Calculator(),\n    new GoogleCalendarCreateTool(googleCalendarParams),\n    new GoogleCalendarViewTool(googleCalendarParams),\n  ];\n\n  const calendarAgent = await initializeAgentExecutorWithOptions(tools, model, {\n    agentType: \"zero-shot-react-description\",\n    verbose: true,\n  });\n\n  const createInput = `Create a meeting with John Doe next Friday at 4pm - adding to the agenda of it the result of 99 + 99`;\n\n  const createResult = await calendarAgent.call({ input: createInput });\n  //   Create Result {\n  //     output: 'A meeting with John Doe on 29th September at 4pm has been created and the result of 99 + 99 has been added to the agenda.'\n  //   }\n  console.log(\"Create Result\", createResult);\n\n  const viewInput = `What meetings do I have this week?`;\n\n  const viewResult = await calendarAgent.call({ input: viewInput });\n  //   View Result {\n  //     output: \"You have no meetings this week between 8am and 8pm.\"\n  //   }\n  console.log(\"View Result\", viewResult);\n}","metadata":{"source":"examples/src/tools/google_calendar.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":54}}}}],["35dddebf-234f-4432-829f-ca0793de39f9",{"pageContent":"import { SearchApi } from \"langchain/tools\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { initializeAgentExecutorWithOptions } from \"langchain/agents\";\n\nexport async function run() {\n  const model = new OpenAI({\n    temperature: 0,\n  });\n\n  const tools = [\n    new SearchApi(process.env.SEARCHAPI_API_KEY, {\n      engine: \"google_news\",\n    }),\n  ];\n  const prefix =\n    \"Answer the following questions as best you can. In your final answer, use a bulleted list markdown format. You have access to the following tools:\";\n\n  const executor = await initializeAgentExecutorWithOptions(tools, model, {\n    agentType: \"zero-shot-react-description\",\n    agentArgs: {\n      prefix,\n    },\n  });\n\n  const res = await executor.call({\n    input: \"What's happening in Ukraine today?\",\n  });\n\n  console.log(res.output);\n}","metadata":{"source":"examples/src/tools/searchapi_google_news.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":30}}}}],["de8d4cfa-05a3-45f8-9a49-1a3aa51a57a5",{"pageContent":"import { SearxngSearch } from \"langchain/tools\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { initializeAgentExecutorWithOptions } from \"langchain/agents\";\n\nexport async function run() {\n  const model = new ChatOpenAI({\n    maxTokens: 1000,\n    modelName: \"gpt-4\",\n  });\n\n  // `apiBase` will be automatically parsed from .env file, set \"SEARXNG_API_BASE\" in .env,\n  const tools = [\n    new SearxngSearch({\n      params: {\n        format: \"json\", // Do not change this, format other than \"json\" is will throw error\n        engines: \"google\",\n      },\n      // Custom Headers to support rapidAPI authentication Or any instance that requires custom headers\n      headers: {},\n    }),\n  ];\n\n  const executor = await initializeAgentExecutorWithOptions(tools, model, {\n    agentType: \"structured-chat-zero-shot-react-description\",\n    verbose: false,\n  });\n\n  console.log(\"Loaded agent.\");\n\n  const input = `What is Langchain? Describe in 50 words`;\n\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await executor.call({ input });\n\n  console.log(result.output);\n  /**\n   * Langchain is a framework for developing applications powered by language models, such as chatbots, Generative Question-Answering, summarization, and more. It provides a standard interface, integrations with other tools, and end-to-end chains for common applications. Langchain enables data-aware and powerful applications.\n   */\n}","metadata":{"source":"examples/src/tools/searxng_search.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":40}}}}],["964a30ad-3e6e-4810-98a1-d6d7284d33ae",{"pageContent":"import { WebBrowser } from \"langchain/tools/webbrowser\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nexport async function run() {\n  // this will not work with Azure OpenAI API yet\n  // Azure OpenAI API does not support embedding with multiple inputs yet\n  // Too many inputs. The max number of inputs is 1.  We hope to increase the number of inputs per request soon. Please contact us through an Azure support request at: https://go.microsoft.com/fwlink/?linkid=2213926 for further questions.\n  // So we will fail fast, when Azure OpenAI API is used\n  if (process.env.AZURE_OPENAI_API_KEY) {\n    throw new Error(\n      \"Azure OpenAI API does not support embedding with multiple inputs yet\"\n    );\n  }\n\n  const model = new ChatOpenAI({ temperature: 0 });\n  const embeddings = new OpenAIEmbeddings(\n    process.env.AZURE_OPENAI_API_KEY\n      ? { azureOpenAIApiDeploymentName: \"Embeddings2\" }\n      : {}\n  );\n\n  const browser = new WebBrowser({ model, embeddings });\n\n  const result = await browser.call(\n    `\"https://www.themarginalian.org/2015/04/09/find-your-bliss-joseph-campbell-power-of-myth\",\"who is joseph campbell\"`\n  );\n\n  console.log(result);\n  /*\n  Joseph Campbell was a mythologist and writer who discussed spirituality, psychological archetypes, cultural myths, and the mythology of self. He sat down with Bill Moyers for a lengthy conversation at George Lucas’s Skywalker Ranch in California, which continued the following year at the American Museum of Natural History in New York. The resulting 24 hours of raw footage were edited down to six one-hour episodes and broadcast on PBS in 1988, shortly after Campbell’s death, in what became one of the most popular in the history of public television.\n\n  Relevant Links:\n  - [The Holstee Manifesto](http://holstee.com/manifesto-bp)\n  - [The Silent Music of the Mind: Remembering Oliver Sacks](https://www.themarginalian.org/2015/08/31/remembering-oliver-sacks)\n  - [Joseph Campbell series](http://billmoyers.com/spotlight/download-joseph-campbell-and-the-power-of-myth-audio/)\n  - [Bill Moyers](https://www.themarginalian.org/tag/bill-moyers/)\n  - [books](https://www.themarginalian.org/tag/books/)\n  */\n}","metadata":{"source":"examples/src/tools/webbrowser.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":40}}}}],["60da5a61-673b-4f5e-bce3-dbab15713178",{"pageContent":"import { WikipediaQueryRun } from \"langchain/tools\";\n\nconst tool = new WikipediaQueryRun({\n  topKResults: 3,\n  maxDocContentLength: 4000,\n});\n\nconst res = await tool.call(\"Langchain\");\n\nconsole.log(res);","metadata":{"source":"examples/src/tools/wikipedia.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":10}}}}],["771de311-a960-477f-a670-9a5e77ee46a5",{"pageContent":"import { WolframAlphaTool } from \"langchain/tools\";\n\nconst tool = new WolframAlphaTool({\n  appid: \"YOUR_APP_ID\",\n});\n\nconst res = await tool.invoke(\"What is 2 * 2?\");\n\nconsole.log(res);","metadata":{"source":"examples/src/tools/wolframalpha.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":9}}}}],["46c05a07-9ca3-4e0b-bfb7-5faaf435eef3",{"pageContent":"import { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport { BufferMemory } from \"langchain/memory\";\nimport * as fs from \"fs\";\nimport { RunnableBranch, RunnableSequence } from \"langchain/schema/runnable\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport { StringOutputParser } from \"langchain/schema/output_parser\";\nimport { LLMChain } from \"langchain/chains\";\nimport { formatDocumentsAsString } from \"langchain/util/document\";\n\nexport const run = async () => {\n  /* Initialize the LLM to use to answer the question */\n  const model = new ChatOpenAI({});\n  /* Load in the file we want to do question answering over */\n  const text = fs.readFileSync(\"state_of_the_union.txt\", \"utf8\");\n  /* Split the text into chunks */\n  const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });\n  const docs = await textSplitter.createDocuments([text]);\n  /* Create the vectorstore */\n  const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());\n  const retriever = vectorStore.asRetriever();\n\n  const serializeChatHistory = (chatHistory: string | Array<string>) => {\n    if (Array.isArray(chatHistory)) {\n      return chatHistory.join(\"\\n\");\n    }\n    return chatHistory;\n  };\n\n  const memory = new BufferMemory({\n    memoryKey: \"chatHistory\",\n  });\n\n  /**\n   * Create a prompt template for generating an answer based on context and\n   * a question.\n   *\n   * Chat history will be an empty string if it's the first question.\n   *\n   * inputVariables: [\"chatHistory\", \"context\", \"question\"]\n   */\n  const questionPrompt = PromptTemplate.fromTemplate(\n    `Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\nCHAT HISTORY: {chatHistory}\n----------------\nCONTEXT: {context}\n----------------\nQUESTION: {question}\n----------------\nHelpful Answer:`\n  );\n\n  /**\n   * Creates a prompt template for __generating a question__ to then ask an LLM\n   * based on previous chat history, context and the question.\n   *\n   * inputVariables: [\"chatHistory\", \"question\"]\n   */\n  const questionGeneratorTemplate =\n    PromptTemplate.fromTemplate(`Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\n----------------\nCHAT HISTORY: {chatHistory}\n----------------\nFOLLOWUP QUESTION: {question}\n----------------\nStandalone question:`);\n\n  const handleProcessQuery = async (input: {\n    question: string;\n    context: string;\n    chatHistory?: string | Array<string>;\n  }) => {\n    const chain = new LLMChain({\n      llm: model,\n      prompt: questionPrompt,\n      outputParser: new StringOutputParser(),\n    });\n\n    const { text } = await chain.call({\n      ...input,\n      chatHistory: serializeChatHistory(input.chatHistory ?? \"\"),\n    });\n\n    await memory.saveContext(\n      {\n        human: input.question,\n      },\n      {\n        ai: text,\n      }\n    );\n\n    return text;\n  };","metadata":{"source":"examples/src/use_cases/advanced/conversational_qa.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":97}}}}],["bb5d89f5-d34c-4d94-97a5-30ab0d75a572",{"pageContent":"const answerQuestionChain = RunnableSequence.from([\n    {\n      question: (input: {\n        question: string;\n        chatHistory?: string | Array<string>;\n      }) => input.question,\n    },\n    {\n      question: (previousStepResult: {\n        question: string;\n        chatHistory?: string | Array<string>;\n      }) => previousStepResult.question,\n      chatHistory: (previousStepResult: {\n        question: string;\n        chatHistory?: string | Array<string>;\n      }) => serializeChatHistory(previousStepResult.chatHistory ?? \"\"),\n      context: async (previousStepResult: {\n        question: string;\n        chatHistory?: string | Array<string>;\n      }) => {\n        // Fetch relevant docs and serialize to a string.\n        const relevantDocs = await retriever.getRelevantDocuments(\n          previousStepResult.question\n        );\n        const serialized = formatDocumentsAsString(relevantDocs);\n        return serialized;\n      },\n    },\n    handleProcessQuery,\n  ]);\n\n  const generateQuestionChain = RunnableSequence.from([\n    {\n      question: (input: {\n        question: string;\n        chatHistory: string | Array<string>;\n      }) => input.question,\n      chatHistory: async () => {\n        const memoryResult = await memory.loadMemoryVariables({});\n        return serializeChatHistory(memoryResult.chatHistory ?? \"\");\n      },\n    },\n    questionGeneratorTemplate,\n    model,\n    // Take the result of the above model call, and pass it through to the\n    // next RunnableSequence chain which will answer the question\n    {\n      question: (previousStepResult: { text: string }) =>\n        previousStepResult.text,\n    },\n    answerQuestionChain,\n  ]);\n\n  const branch = RunnableBranch.from([\n    [\n      async () => {\n        const memoryResult = await memory.loadMemoryVariables({});\n        const isChatHistoryPresent = !memoryResult.chatHistory.length;\n\n        return isChatHistoryPresent;\n      },\n      answerQuestionChain,\n    ],\n    [\n      async () => {\n        const memoryResult = await memory.loadMemoryVariables({});\n        const isChatHistoryPresent =\n          !!memoryResult.chatHistory && memoryResult.chatHistory.length;\n\n        return isChatHistoryPresent;\n      },\n      generateQuestionChain,\n    ],\n    answerQuestionChain,\n  ]);\n\n  const fullChain = RunnableSequence.from([\n    {\n      question: (input: { question: string }) => input.question,\n    },\n    branch,\n  ]);\n\n  const resultOne = await fullChain.invoke({\n    question: \"What did the president say about Justice Breyer?\",\n  });\n\n  console.log({ resultOne });\n  /**\n   * {\n   *   resultOne: 'The president thanked Justice Breyer for his service and described him as an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court.'\n   * }\n   */\n\n  const resultTwo = await fullChain.invoke({\n    question: \"Was it nice?\",\n  });\n\n  console.log({ resultTwo });\n  /**\n   * {\n   *   resultTwo: \"Yes, the president's description of Justice Breyer was positive.\"\n   * }\n   */\n};","metadata":{"source":"examples/src/use_cases/advanced/conversational_qa.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":99,"to":203}}}}],["ec4cdc2e-99ef-48b2-b8ae-77fdd0012f30",{"pageContent":"import { ViolationOfExpectationsChain } from \"langchain/experimental/chains/violation_of_expectations\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { AIMessage, HumanMessage } from \"langchain/schema\";\nimport { HNSWLib } from \"langchain/vectorstores/hnswlib\";\n\n// Short GPT generated conversation between a human and an AI.\nconst dummyMessages = [\n  new HumanMessage(\n    \"I've been thinking about the importance of time with myself to discover my voice. I feel like 1-2 hours is never enough.\"\n  ),\n  new AIMessage(\n    \"The concept of 'adequate time' varies. Have you tried different formats of introspection, such as morning pages or long-form writing, to see if they make the process more efficient?\"\n  ),\n  new HumanMessage(\n    \"I have tried journaling but never consistently. Sometimes it feels like writing doesn't capture everything.\"\n  ),\n  new AIMessage(\n    \"Writing has its limits. What about other mediums like digital art, or interactive journal apps with dynamic prompts that dig deeper? Even coding a personal project can be a form of self-discovery.\"\n  ),\n  new HumanMessage(\n    \"That's an interesting idea. I've never thought about coding as a form of self-discovery.\"\n  ),\n  new AIMessage(\n    \"Since you're comfortable with code, consider building a tool to log and analyze your emotional state, thoughts, or personal growth metrics. It merges skill with introspection, makes the data quantifiable.\"\n  ),\n  new HumanMessage(\n    \"The idea of quantifying emotions and personal growth is fascinating. But I wonder how much it can really capture the 'dark zone' within us.\"\n  ),\n  new AIMessage(\n    \"Good point. The 'dark zone' isn't fully quantifiable. But a tool could serve as a scaffold to explore those areas. It gives a structured approach to an unstructured problem.\"\n  ),\n  new HumanMessage(\n    \"You might be onto something. A structured approach could help unearth patterns or triggers I hadn't noticed.\"\n  ),\n  new AIMessage(\n    \"Exactly. It's about creating a framework to understand what can't easily be understood. Then you can allocate those 5+ hours more effectively, targeting areas that your data flags.\"\n  ),\n];\n\n// Instantiate with an empty string to start, since we have no data yet.\nconst vectorStore = await HNSWLib.fromTexts(\n  [\" \"],\n  [{ id: 1 }],\n  new OpenAIEmbeddings()\n);\nconst retriever = vectorStore.asRetriever();\n\n// Instantiate the LLM,\nconst llm = new ChatOpenAI({\n  modelName: \"gpt-4\",\n});\n\n// And the chain.\nconst voeChain = ViolationOfExpectationsChain.fromLLM(llm, retriever);\n\n// Requires an input key of \"chat_history\" with an array of messages.\nconst result = await voeChain.call({\n  chat_history: dummyMessages,\n});\n\nconsole.log({\n  result,\n});\n\n/**\n * Output:\n{\n  result: [\n    'The user has experience with coding and has tried journaling before, but struggles with maintaining consistency and fully expressing their thoughts and feelings through writing.',\n    'The user shows a thoughtful approach towards new concepts and is willing to engage with and contemplate novel ideas before making a decision. They also consider time effectiveness as a crucial factor in their decision-making process.',\n    'The user is curious and open-minded about new concepts, but also values introspection and self-discovery in understanding emotions and personal growth.',\n    'The user is open to new ideas and strategies, specifically those that involve a structured approach to identifying patterns or triggers.',\n    'The user may not always respond or engage with prompts, indicating a need for varied and adaptable communication strategies.'\n  ]\n}\n */","metadata":{"source":"examples/src/use_cases/advanced/violation_of_expectations_chain.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":77}}}}],["a4eef127-2fb0-4603-bba7-55f68cceedab",{"pageContent":"import { CheerioWebBaseLoader } from \"langchain/document_loaders/web/cheerio\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { Ollama } from \"langchain/llms/ollama\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport {\n  RunnableSequence,\n  RunnablePassthrough,\n} from \"langchain/schema/runnable\";\nimport { StringOutputParser } from \"langchain/schema/output_parser\";\nimport { HuggingFaceTransformersEmbeddings } from \"langchain/embeddings/hf_transformers\";\nimport { formatDocumentsAsString } from \"langchain/util/document\";\n\nconst loader = new CheerioWebBaseLoader(\n  \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n);\nconst docs = await loader.load();\n\nconst splitter = new RecursiveCharacterTextSplitter({\n  chunkOverlap: 0,\n  chunkSize: 500,\n});\n\nconst splitDocuments = await splitter.splitDocuments(docs);\n\nconst vectorstore = await HNSWLib.fromDocuments(\n  splitDocuments,\n  new HuggingFaceTransformersEmbeddings()\n);\n\nconst retriever = vectorstore.asRetriever();\n\n// Prompt\nconst prompt =\n  PromptTemplate.fromTemplate(`Answer the question based only on the following context:\n{context}\n\nQuestion: {question}`);\n\n// Llama 2 7b wrapped by Ollama\nconst model = new Ollama({\n  baseUrl: \"http://localhost:11434\",\n  model: \"llama2\",\n});\n\nconst chain = RunnableSequence.from([\n  {\n    context: retriever.pipe(formatDocumentsAsString),\n    question: new RunnablePassthrough(),\n  },\n  prompt,\n  model,\n  new StringOutputParser(),\n]);\n\nconst result = await chain.invoke(\n  \"What are the approaches to Task Decomposition?\"\n);\n\nconsole.log(result);\n\n/*\n  Based on the provided context, there are three approaches to task decomposition:\n\n  1. Using simple prompts like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\" to elicit a list of tasks from a language model (LLM).\n  2. Providing task-specific instructions, such as \"Write a story outline\" for writing a novel, to guide the LLM in decomposing the task into smaller subtasks.\n  3. Incorporating human inputs to help the LLM learn and improve its decomposition abilities over time.\n*/","metadata":{"source":"examples/src/use_cases/local_retrieval_qa/chain.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":68}}}}],["2b85cb64-a9a4-473b-b756-317b2c4b67fb",{"pageContent":"import { CheerioWebBaseLoader } from \"langchain/document_loaders/web/cheerio\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { HuggingFaceTransformersEmbeddings } from \"langchain/embeddings/hf_transformers\";\n\nconst loader = new CheerioWebBaseLoader(\n  \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n);\nconst docs = await loader.load();\n\nconst splitter = new RecursiveCharacterTextSplitter({\n  chunkOverlap: 0,\n  chunkSize: 500,\n});\n\nconst splitDocuments = await splitter.splitDocuments(docs);\n\nconst vectorstore = await HNSWLib.fromDocuments(\n  splitDocuments,\n  new HuggingFaceTransformersEmbeddings()\n);\n\nconst retrievedDocs = await vectorstore.similaritySearch(\n  \"What are the approaches to Task Decomposition?\"\n);\n\nconsole.log(retrievedDocs[0]);\n\n/*\n  Document {\n    pageContent: 'Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.',\n    metadata: {\n      source: 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n      loc: { lines: [Object] }\n    }\n  }\n*/","metadata":{"source":"examples/src/use_cases/local_retrieval_qa/load_documents.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":37}}}}],["dc9ba1d2-7bae-4c4b-9eea-1480ffe4369b",{"pageContent":"import { RetrievalQAChain, loadQAStuffChain } from \"langchain/chains\";\nimport { CheerioWebBaseLoader } from \"langchain/document_loaders/web/cheerio\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { Ollama } from \"langchain/llms/ollama\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport { HuggingFaceTransformersEmbeddings } from \"langchain/embeddings/hf_transformers\";\n\nconst loader = new CheerioWebBaseLoader(\n  \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n);\nconst docs = await loader.load();\n\nconst splitter = new RecursiveCharacterTextSplitter({\n  chunkOverlap: 0,\n  chunkSize: 500,\n});\n\nconst splitDocuments = await splitter.splitDocuments(docs);\n\nconst vectorstore = await HNSWLib.fromDocuments(\n  splitDocuments,\n  new HuggingFaceTransformersEmbeddings()\n);\n\nconst retriever = vectorstore.asRetriever();\n\n// Llama 2 7b wrapped by Ollama\nconst model = new Ollama({\n  baseUrl: \"http://localhost:11434\",\n  model: \"llama2\",\n});\n\nconst template = `Use the following pieces of context to answer the question at the end.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\nUse three sentences maximum and keep the answer as concise as possible.\nAlways say \"thanks for asking!\" at the end of the answer.\n{context}\nQuestion: {question}\nHelpful Answer:`;\n\nconst QA_CHAIN_PROMPT = new PromptTemplate({\n  inputVariables: [\"context\", \"question\"],\n  template,\n});\n\n// Create a retrieval QA chain that uses a Llama 2-powered QA stuff chain with a custom prompt.\nconst chain = new RetrievalQAChain({\n  combineDocumentsChain: loadQAStuffChain(model, { prompt: QA_CHAIN_PROMPT }),\n  retriever,\n  returnSourceDocuments: true,\n  inputKey: \"question\",\n});","metadata":{"source":"examples/src/use_cases/local_retrieval_qa/qa_chain.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":53}}}}],["aa7a9a98-b78e-41a2-94f2-fa1796788d99",{"pageContent":"const response = await chain.call({\n  question: \"What are the approaches to Task Decomposition?\",\n});\n\nconsole.log(response);\n\n/*\n  {\n    text: 'Thanks for asking! There are several approaches to task decomposition, which can be categorized into three main types:\\n' +\n      '\\n' +\n      '1. Using language models with simple prompting (e.g., \"Steps for XYZ.\"), or asking for subgoals for achieving XYZ.\\n' +\n      '2. Providing task-specific instructions, such as writing a story outline for writing a novel.\\n' +\n      '3. Incorporating human inputs to decompose tasks.\\n' +\n      '\\n' +\n      'Each approach has its advantages and limitations, and the choice of which one to use depends on the specific task and the desired level of complexity and adaptability. Thanks for asking!',\n    sourceDocuments: [\n      Document {\n        pageContent: 'Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.',\n        metadata: [Object]\n      },\n      Document {\n        pageContent: 'Fig. 1. Overview of a LLM-powered autonomous agent system.\\n' +\n          'Component One: Planning#\\n' +\n          'A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\n' +\n          'Task Decomposition#',\n        metadata: [Object]\n      },\n      Document {\n        pageContent: 'Challenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.',\n        metadata: [Object]\n      },\n      Document {\n        pageContent: 'Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.',\n        metadata: [Object]\n      }\n    ]\n  }\n*/","metadata":{"source":"examples/src/use_cases/local_retrieval_qa/qa_chain.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":55,"to":92}}}}],["9415d3ba-755e-4717-8a23-c6edb5b55fb5",{"pageContent":"import { RetrievalQAChain } from \"langchain/chains\";\nimport { ChatOpenAI } from \"langchain/chat_models/openai\";\nimport { SearchApiLoader } from \"langchain/document_loaders/web/searchapi\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { TokenTextSplitter } from \"langchain/text_splitter\";\nimport { FaissStore } from \"langchain/vectorstores/faiss\";\n\nconst loader = new SearchApiLoader({\n  engine: \"youtube_transcripts\",\n  video_id: \"WTOm65IZneg\",\n});\n\nconst docs = await loader.load();\n\nconst textSplitterQA = new TokenTextSplitter({\n  chunkSize: 1000,\n  chunkOverlap: 200,\n});\n\nconst docsQA = await textSplitterQA.splitDocuments(docs);\n\nconst llm_question_answer = new ChatOpenAI({\n  modelName: \"gpt-3.5-turbo-16k\",\n  temperature: 0.2,\n});\n\nconst embeddings = new OpenAIEmbeddings();\n\nconst db = await FaissStore.fromDocuments(await docsQA, embeddings);\n\nconst qa = RetrievalQAChain.fromLLM(llm_question_answer, db.asRetriever(), {\n  verbose: true,\n});\n\nconst question = \"How many people he wanted to help?\";\n\nconst answer = await qa.run(question);\n\nconsole.log(answer);\n\n/**\n * He wanted to help 1,000 deaf people.\n */","metadata":{"source":"examples/src/use_cases/youtube/chat_with_podcast.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":43}}}}],["70ffe151-2275-43dd-8203-39dd4b0fd59b",{"pageContent":"import { loadSummarizationChain } from \"langchain/chains\";\nimport { ChatAnthropic } from \"langchain/chat_models/anthropic\";\nimport { SearchApiLoader } from \"langchain/document_loaders/web/searchapi\";\nimport { PromptTemplate } from \"langchain/prompts\";\nimport { TokenTextSplitter } from \"langchain/text_splitter\";\n\nconst loader = new SearchApiLoader({\n  engine: \"youtube_transcripts\",\n  video_id: \"WTOm65IZneg\",\n});\n\nconst docs = await loader.load();\n\nconst splitter = new TokenTextSplitter({\n  chunkSize: 10000,\n  chunkOverlap: 250,\n});\n\nconst docsSummary = await splitter.splitDocuments(docs);\n\nconst llmSummary = new ChatAnthropic({\n  modelName: \"claude-2\",\n  temperature: 0.3,\n});\n\nconst summaryTemplate = `\nYou are an expert in summarizing YouTube videos.\nYour goal is to create a summary of a podcast.\nBelow you find the transcript of a podcast:\n--------\n{text}\n--------\n\nThe transcript of the podcast will also be used as the basis for a question and answer bot.\nProvide some examples questions and answers that could be asked about the podcast. Make these questions very specific.\n\nTotal output will be a summary of the video and a list of example questions the user could ask of the video.\n\nSUMMARY AND QUESTIONS:\n`;\n\nconst SUMMARY_PROMPT = PromptTemplate.fromTemplate(summaryTemplate);\n\nconst summaryRefineTemplate = `\nYou are an expert in summarizing YouTube videos.\nYour goal is to create a summary of a podcast.\nWe have provided an existing summary up to a certain point: {existing_answer}\n\nBelow you find the transcript of a podcast:\n--------\n{text}\n--------\n\nGiven the new context, refine the summary and example questions.\nThe transcript of the podcast will also be used as the basis for a question and answer bot.\nProvide some examples questions and answers that could be asked about the podcast. Make\nthese questions very specific.\nIf the context isn't useful, return the original summary and questions.\nTotal output will be a summary of the video and a list of example questions the user could ask of the video.\n\nSUMMARY AND QUESTIONS:\n`;\n\nconst SUMMARY_REFINE_PROMPT = PromptTemplate.fromTemplate(\n  summaryRefineTemplate\n);\n\nconst summarizeChain = loadSummarizationChain(llmSummary, {\n  type: \"refine\",\n  verbose: true,\n  questionPrompt: SUMMARY_PROMPT,\n  refinePrompt: SUMMARY_REFINE_PROMPT,\n});","metadata":{"source":"examples/src/use_cases/youtube/podcast_summary.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":1,"to":73}}}}],["e7ea3e48-7a9a-475a-8a39-1c3bc5d7a1ca",{"pageContent":"const summary = await summarizeChain.run(docsSummary);\n\nconsole.log(summary);\n\n/*\n  Here is a summary of the key points from the podcast transcript:\n\n  - Jimmy helps provide hearing aids and cochlear implants to deaf and hard-of-hearing people who can't afford them. He helps over 1,000 people hear again.\n\n  - Jimmy surprises recipients with $10,000 cash gifts in addition to the hearing aids. He also gifts things like jet skis, basketball game tickets, and trips to concerts.\n\n  - Jimmy travels internationally to provide hearing aids, visiting places like Mexico, Guatemala, Brazil, South Africa, Malawi, and Indonesia. \n\n  - Jimmy donates $100,000 to organizations around the world that teach sign language.\n\n  - The recipients are very emotional and grateful to be able to hear their loved ones again.\n\n  Here are some example questions and answers about the podcast:\n\n  Q: How many people did Jimmy help regain their hearing?\n  A: Jimmy helped over 1,000 people regain their hearing.\n\n  Q: What types of hearing devices did Jimmy provide to the recipients?\n  A: Jimmy provided cutting-edge hearing aids and cochlear implants.\n\n  Q: In addition to the hearing devices, what surprise gifts did Jimmy give some recipients?\n  A: In addition to hearing devices, Jimmy surprised some recipients with $10,000 cash gifts, jet skis, basketball game tickets, and concert tickets.\n\n  Q: What countries did Jimmy travel to in order to help people?\n  A: Jimmy traveled to places like Mexico, Guatemala, Brazil, South Africa, Malawi, and Indonesia.\n\n  Q: How much money did Jimmy donate to organizations that teach sign language?\n  A: Jimmy donated $100,000 to sign language organizations around the world.\n\n  Q: How did the recipients react when they were able to hear again?\n  A: The recipients were very emotional and grateful, with many crying tears of joy at being able to hear their loved ones again.\n*/","metadata":{"source":"examples/src/use_cases/youtube/podcast_summary.ts","repository":"https://github.com/langchain-ai/langchainjs","branch":"main","loc":{"lines":{"from":75,"to":111}}}}]],{"0":"21c67406-2127-4691-b918-67a7a5a2d965","1":"ce3a846e-5592-4747-9e3b-1c4aede74555","2":"bac06d20-d2c8-45aa-85bb-478448e9a3eb","3":"9c62401c-a27a-4d9e-b7d0-f07887bed597","4":"b3e631f0-69d1-4403-9389-6d74767c1707","5":"8eb3551f-8daa-4d72-a07f-619b26fee1ea","6":"90a61672-fa16-48d7-bd7c-0c50861c223a","7":"bad4076c-d870-4799-aa9c-c24aed1a2bbd","8":"d06c1d6b-0cbd-4f18-a902-a4f39f1c00ac","9":"dcd5bea2-6370-41bf-9e73-26906724fb95","10":"d832a754-71b3-4a53-b4e3-4e30d2056ad3","11":"19830401-ec33-4a1b-bc0f-24ad923e70b8","12":"90e0df64-7489-442d-8d73-c2a388311a2f","13":"f8d1a06f-0843-4105-aa5f-f9d1bfbab815","14":"f8aab76a-aa4f-4bce-adb7-373c2034f779","15":"6dadcbbb-a747-4640-978f-48e8c5eae9ac","16":"d181a110-ef02-4ade-9e68-a46d4a6ee1a2","17":"d14862be-45bb-4bd7-8839-39f69835bcfc","18":"a391475f-803d-4772-92c4-09069ee11615","19":"dfe05912-8cfa-4fc9-8917-152b368e151e","20":"e1e9af40-5883-468d-8044-11a2da4fdbdc","21":"f72e46e1-8368-4451-8380-a8b491f5b105","22":"6d6161c4-80ac-4b94-8699-39175a9f0760","23":"8e760733-6417-4f31-8986-683a0bd49b01","24":"e36c562b-c69e-4a34-9692-e85170295763","25":"65ed9bf2-1b6e-43e1-a04a-ba15ba13b561","26":"ea69f6ad-b6a1-42cd-83d8-eaa7303da3c9","27":"c7633acc-362f-4c0d-8a89-e2a9a62b9bcf","28":"d6ec6b76-bec9-45d7-9acd-45036c4b9389","29":"f9f4ce2e-30cf-4231-9578-84fd3e6b2735","30":"0f106b30-dbf1-469e-a673-195fbe4d8871","31":"d8a8acaf-8e72-4b67-bef6-91a64e80f375","32":"f2d74435-0a5e-4606-a205-9c70bc28a21b","33":"271bc995-1a58-4a49-8059-fc05bbad8799","34":"22fc7253-538b-4665-8520-cc40e8ac4e48","35":"49065315-bf79-4904-af2d-f422d34e2c47","36":"af15dc56-d8ef-4310-8366-dce269f464ea","37":"5e895393-9189-4482-8d62-3b482969a262","38":"52409e6c-e915-4a50-84b2-0842ffa4d423","39":"2cd60b99-3b4b-45de-877c-c4d2506fb27b","40":"58f51c18-539f-4a88-bf30-38fa253dc6bc","41":"edee7678-ca92-4f88-81b9-907255a681ce","42":"7acc2880-3f0d-4b7b-a48d-b4cf2e815e2e","43":"f3834537-ecd7-4f37-8517-f7e753b418b6","44":"96b59be4-9e44-458d-989e-dc871d78f5b1","45":"56a254b6-2775-4264-9b88-2fe783f163c3","46":"54373c4d-eede-4514-a0fc-9755d65fb8c2","47":"94c66112-0c3c-49ad-8308-760e658c35ce","48":"97807ef3-91ce-4fc9-a8a4-b43ab808ec70","49":"a360160d-8b72-4971-8f4f-b3b7e554353c","50":"d50c64c1-d448-4eba-ad47-c906cd2d17cb","51":"4e2a2feb-bf88-4f92-8824-f34459c5dd38","52":"7169f8c0-91e3-4836-a1bb-14f7e27f0697","53":"8b9ac5ca-b25b-4a98-93f2-568379fe8d13","54":"e2c4620c-00ac-4952-9268-1dff065a9626","55":"566c0de2-890e-4cf5-9ecf-bf75a867a2af","56":"376ecbb5-3ae6-4ce4-84c3-89eaa18a1960","57":"97cf2a3d-ecae-4155-b340-c2b49d2b6063","58":"c1282c31-8049-4e68-bcd2-3f00d1c853c1","59":"1eedd31f-c2b3-454e-a0c7-baeb7181ad1a","60":"77fd2063-6256-4fe4-b411-1b42ef3220be","61":"9047d912-06a4-41f0-8bff-b6a9b2cfa178","62":"f46abc14-ef4f-4897-9a98-86f8e61f375a","63":"151b0d28-8d17-4dec-96b7-710d846174b4","64":"3c61d33e-c98c-4d62-8161-662f5c32c421","65":"bbf36a6f-693e-4822-8bee-43ab28cc6d93","66":"56449b7f-65cd-4e1f-b2cb-f0383a1a8ff6","67":"e0a9de39-af31-48fc-a0a5-491b8bc28b6f","68":"80bb7df4-0433-4e2c-a34c-4190628a7e8c","69":"d47c50a9-2df2-4b6c-bdf9-faac8e80e7c6","70":"8fe3aeec-e482-464e-8033-6653a0b3f4be","71":"0959b025-be80-4ac6-853f-6f3ccf5681f0","72":"890c51c5-18a0-4b7c-9c82-65fba87f0dbf","73":"0124a154-fb95-4405-8ada-9e05a24712bd","74":"d53e0226-a7fc-4794-9104-75a77a46f826","75":"68b4ca8c-222e-42ee-b8ca-480b424cb3ea","76":"cfe934d9-b157-4ca7-8107-ec716b58ea4f","77":"af5ac60c-561f-47b4-bba0-71d4a359376b","78":"d487f3bc-5529-4042-ba93-8e84669ce93c","79":"b8482a36-7fa5-4124-80b7-2c3fd23badc9","80":"6025a87e-0bc2-4dac-8cbe-5f10fc062514","81":"375e0c78-3e8d-4c63-a9e4-145c0ccf475b","82":"eb3eb29a-8275-4c36-8d7d-09b47952786e","83":"539489af-42ab-4617-8e64-ad5f246a5400","84":"5046e122-3f23-4924-a89a-e89d88a54e48","85":"d2e8aac7-99d1-4d63-979b-76cb1cf99d62","86":"6d0b4889-bb9d-4210-8533-066d4ca0b6e7","87":"cfc691ad-35db-4f47-a945-520c04d2cefb","88":"d8f4ca14-fedb-4c21-bef2-339dba596046","89":"cbe626fc-3330-4ce3-819e-a452e5099c0c","90":"384ac168-3907-4a3f-a2d0-fc0e4d812448","91":"42b19055-25a8-4d14-a1a3-30fa7bbb0800","92":"919d3cfe-026f-49aa-be39-167d39134216","93":"8524bd49-d794-4291-9bda-924de45b247e","94":"5c02f094-cba1-4f6c-a293-ebd230572c11","95":"d531ba18-4c4e-4ad0-80f1-811bd4c648e5","96":"315023ac-d305-4ba5-93c4-4b33a718fb69","97":"8c7f027c-4600-44c8-908f-1f2259392324","98":"dd297694-6ba7-457b-bb5a-c899f4790d83","99":"4144c095-d2d9-4ccf-8d43-5830069e8d79","100":"16d4ed1b-353e-4093-84e0-4e136c2217a7","101":"5ac67a9f-1fe0-4b29-a65f-828af91dc45d","102":"68d91498-2adb-411e-9bde-15c045183c53","103":"4ce9dbfb-47eb-44c7-9a19-da68b6894785","104":"2a61577f-5d2a-4013-ade9-36ca51a1ad09","105":"66d27fa7-0b91-4823-a048-b92f500f16d0","106":"2a21aa0b-3546-44d4-8599-e124954e73be","107":"e1850229-f7c5-43cd-b3ec-fc48976ad487","108":"17e99aca-6229-4f97-bd69-c9047c3f7bbf","109":"155d0b4f-313a-4832-a462-3ddacfea4783","110":"49db394d-c2d4-4f5e-9845-59138755ef50","111":"37d2f481-43ec-4ff5-bc2e-a760f4639db3","112":"b4edf8a0-a1b3-4272-8750-59d14eec6407","113":"99500a2e-c0a7-40b8-b85e-e37a2c2fe3c5","114":"4fccfd25-5147-4590-99a8-f5566b75858a","115":"0d278ea3-db6e-4974-b95c-5bb5b50bfb92","116":"0aea2574-6d72-4f41-a0eb-b483e6c9fc70","117":"e9059dae-951b-4603-8210-1e00d2b2f783","118":"683efd06-e1b1-4bd6-bd3c-6332ebe58b39","119":"d1f741dd-c5de-40ae-9c82-2649da9f1d7e","120":"6f4a61d4-fa16-4715-bc90-6891ce38dd1d","121":"d4f631d2-8ee1-46b9-9dce-61c19d8b871a","122":"adff9d87-6e9c-4b70-9109-82295a494da7","123":"cf3ced87-7abc-47bf-8c65-22d42d716c80","124":"3ed925e8-c63c-493f-9fcb-6758ec3d0537","125":"f23f9b32-a4c0-4e7b-9be1-5481de079257","126":"7d8d2e19-4bd9-41cc-ac84-46ec41055421","127":"1a8e844f-1d95-4dd1-8536-c6f930169766","128":"b5eafe17-07d5-4112-8cd3-b356102c0bcd","129":"0fd3f53a-f25c-4a38-ab76-d66b9f023035","130":"b6a0547e-9531-40d7-b883-78ff5e164e51","131":"f839db18-bcc0-4d3c-8638-2e0a2c255683","132":"e8465701-3783-4f48-926d-18fd3e5ca9f0","133":"315953ee-75e4-4d7d-b58a-051fdce280ab","134":"5fc29a39-778b-416b-a051-ea7aa0356155","135":"5faab9d5-4005-403f-8406-d6ccfc9a7520","136":"66b44511-c7e7-4987-9e50-a1cc6db6be25","137":"5b31aea4-5ad0-4aef-b322-c5fb3e64cf0d","138":"8f88a601-e419-4ad4-a9cd-b3fd1ddb0823","139":"6feaf83a-0872-467c-a5d7-d966f13ff1af","140":"a88a4347-160a-4f9f-8cfd-56e28bf0e72f","141":"8093987f-b5e1-4c4f-964f-235c7655339a","142":"a8209474-da1f-416b-b6e1-6b59358c3115","143":"a094f815-3ffc-4d3c-b7c1-ecf83370e3e6","144":"5e8d958f-4b18-4561-a6a9-9d23ad652665","145":"d792f0e4-04d3-4867-b03d-2dca8b311656","146":"c70000b5-8d32-4624-a5d5-c2022a7699ef","147":"d03638fb-4fb2-4c77-98b0-cb1766c6de7d","148":"894fc12b-7683-4a07-8b63-153361a42969","149":"1acc6868-84b6-47c4-b2d0-a90cb67fa35b","150":"3fab4c05-6a8f-40fb-b419-f8da73771c3c","151":"5605e0e5-6252-402b-a205-6fd3a7692fc2","152":"837b3ab5-9542-4613-a387-a4bec4922870","153":"4daf01f4-a755-4a31-ad3d-6e9154d66f0c","154":"3698837b-a6a2-46d8-a4b0-6f0438c3203e","155":"571757d4-1512-4111-8bdc-259bc5887b92","156":"96985552-2e31-4cf1-9fec-582e07131e9c","157":"faf58608-277b-4ef9-a2cd-59eccf5ae383","158":"2d33d309-6ffe-4401-8c05-60ccea21c19b","159":"36bd1093-de83-4138-acc2-46087e1584ff","160":"614b559c-04bc-47e9-9e98-23aa92297da6","161":"d7004199-fca3-4ebf-bcfc-74b5abf361f1","162":"1ba034f2-5947-4f6e-b19e-83da2cee1cb6","163":"7bf63212-5bb7-48d2-a541-214f8f49c4d9","164":"bb4a6725-c056-4898-a452-7dacd6d13c0f","165":"13f395f9-6ccc-49d5-a0cc-5913b9055276","166":"ee2967d2-370f-4180-8510-0924106f07ed","167":"ee8f0422-f235-49fb-b6c0-04b677f35702","168":"fb1ded65-87b0-4f86-aac6-12f9c32fc909","169":"3514c9a5-16ba-441e-88d1-f4bb03b3f054","170":"ad003dc8-f7ee-4da9-bcfe-1e8f848037b1","171":"44808fdf-7162-4c44-8dd5-aed71c2a7a55","172":"5727f01b-cf8d-4574-8edf-0bc0734fc3f3","173":"648354a8-739d-492a-ba51-9094eb20a0aa","174":"0243ed32-6725-4bbe-857a-d7a738836f74","175":"69ad7e03-f21e-4b3a-8601-4321cae4c0db","176":"c6e388c0-c176-4950-8a0f-ccb7b404d827","177":"33557be3-8c11-464b-8e7d-d103f16d3f55","178":"dd97575f-5dc2-4e29-a028-ab2b32719f44","179":"258f9030-45be-4be3-84f4-a90c0d95c712","180":"d1675ab4-ca2b-4ef4-a08a-9144af065f24","181":"6b2f0222-2b90-4bef-b742-26e6b5e77190","182":"a32327b3-39de-49af-80d1-8aadf96f98f7","183":"342824ea-914b-464b-b5ff-0e0e370bf60c","184":"78c50d57-ac11-47ac-8f81-9f79980f93fe","185":"4b332033-0334-458a-b0d3-6e2cdfeff2c6","186":"199a33ee-fd76-4019-a4a8-34cbfff1d55c","187":"cb123d6d-0f83-49a6-bf8d-0e3da9c6a162","188":"1a7a226d-bb6e-47a7-8da9-6ef966d42587","189":"c7c81413-e0a0-4b62-94b8-12ade2813081","190":"c7e6f574-2677-4c1a-b952-78f495bc20d9","191":"1f1ce2c4-12bc-40fa-8ba7-d8b85b7225c7","192":"2ece585d-7b3f-4542-b428-d8d0f53bc5c4","193":"3c5b6b91-35bd-4de1-b3c2-68598d0e7e22","194":"35e6db5a-98aa-4534-8ef3-accae548e61d","195":"9b3f67e3-5ed8-421a-a771-20aff2c7c7b9","196":"6ea6a908-cda1-4e7f-84dd-1b4f63edbc97","197":"b9b02419-13f3-46cf-8ac7-21fb9c9efb79","198":"c99193e8-2c01-424e-9416-e32d2f5d345e","199":"35bfb93e-3c33-4237-af2f-fc4b031c383d","200":"9a1f3931-4e48-49bf-abda-222d4d0a18e9","201":"b5dad3c2-e4fa-4bda-928c-7d7712649a19","202":"9b2224dd-f335-4680-af60-63f62e932deb","203":"faa6a8ee-8ce2-4bf0-9e3f-cddf0d109b24","204":"52062222-10ce-4148-a3e4-c2a1e05e098c","205":"00c9ec6b-1bb2-49c3-bf18-2e7716e75c1a","206":"fa1eb484-3b59-4ba5-8af2-db53aa63acfe","207":"cfc96975-3fe5-4cf4-a2a8-251dd65ea3d3","208":"b677b290-360c-4af1-9cab-1dac3049be65","209":"09680c93-d403-4799-952c-c14d9268fd25","210":"736d34f8-3270-4abc-b2c6-567e31a76c40","211":"039b944d-d500-4f70-87e0-48bf1c8d2d04","212":"d867ebf6-a4dd-4f28-a7d8-0ca219598d33","213":"59706847-96b3-42d5-9a86-4114bc69fc00","214":"ee570653-be7d-48ba-aa8f-a2746304f729","215":"8838b1a4-6c6a-49fb-b3a8-99aca2417930","216":"c15047ad-1c5c-4254-bde5-b5eae828fe5c","217":"46338d42-618b-4b02-a5a3-1170452746f8","218":"35c3fb37-c14a-4d65-842a-3b6998ffb235","219":"f9787c97-222c-469b-ae8b-ed77aa00434f","220":"bb1301a7-6309-4b68-82bd-ca945bfa32bd","221":"53a5481f-f22f-4609-a767-317bb3f4ed5f","222":"48249005-c5a1-4857-8968-e32952ba4054","223":"ae4169d7-2d2d-4d62-862f-20f8a34da0fe","224":"26dafd96-dfdc-40ac-a180-fac94a631a8d","225":"bc7701fa-f734-49c4-9295-3cb8494b87e8","226":"c519ef21-968a-4a2f-b509-7a95a795c122","227":"48cc5b1e-b19d-4044-bc53-a5b153c5c8b7","228":"0eda3a99-4c48-4d13-9569-55bdf7c5ee5a","229":"83cb1c71-b406-4aeb-b6d6-34e85e168e00","230":"8744c6fb-3f69-42b6-a7f1-601c1878b6fe","231":"981cacf1-b9a1-4f06-a0cb-593128ec2914","232":"16b51887-e4c4-4144-8e59-4d5e3a419d7f","233":"b89ac7df-f7f1-43a8-a711-d8d8dc4cde30","234":"f568c874-fc4d-4def-b7e2-56539e31341e","235":"5540fab5-6adc-4d00-97eb-8768cdc34853","236":"21fa8b71-c57d-4e8d-a995-e40a1fb28760","237":"1195e88f-a5e3-452e-86a3-493bce18d02e","238":"60637de3-c9de-4053-9d89-ee75ad182ed1","239":"b5bb432d-198a-4d4c-923b-aabd740de949","240":"e779b9de-a1ff-4962-828a-0347ffd1c638","241":"ba2f629d-8bc8-49cd-a109-e65a9b08d266","242":"4f6cb09e-e8ce-4d96-ae22-94df6642b8c8","243":"3e74c872-3fdd-4a70-b601-3e56974e72f2","244":"3eb0e6f9-9470-4603-9eb0-b4b943463ecd","245":"02e91e94-5e47-43b2-a5fe-6ef6ce03b4bd","246":"8e920683-17ad-4b2d-80fe-b96942167635","247":"2ba7169e-f0cf-4b40-bb56-e13c327154a4","248":"7cabe983-1903-4596-9f94-316aca4ac9f1","249":"9656114e-3399-4cae-b7fd-70619e43e790","250":"4e871b80-556d-43b1-8e20-74c7cd1f13b0","251":"3fa74d88-3732-4859-a0d7-2e5e485a51e3","252":"8a2f0b00-a756-4008-bc42-b2e8f5c8e891","253":"524bfbec-4128-4fbf-b9e2-5d91a3b8d03b","254":"3ee3ef98-428f-47bf-97e6-876cbb5b03bb","255":"03839d1b-3088-4c6c-8872-8f8890c655d2","256":"99c7edf8-e64b-4a6b-b2f7-1ea6bb5ec06c","257":"250f17bd-e95a-41dc-8697-22bdd385587b","258":"83671bbe-2407-4689-8a33-69316a0310ef","259":"b7d85caf-b79e-4e91-8458-117357eb4c27","260":"0e2babda-337d-43d6-b016-9dd1e2b08c08","261":"7a966220-2b57-4407-a0bf-f1a4b36e82f6","262":"886d87ec-e488-4049-a711-e77c75ad07e1","263":"c4a5d27a-c822-4cd3-b21b-208a254a6248","264":"ef589390-ff03-4388-a705-aecb7a358941","265":"f8b0f723-586c-4507-9ba6-ea74a558c010","266":"a9eec216-52f2-462e-a9e9-d2ee06bf9107","267":"43359ee9-b15a-4b99-a872-ba3548455c65","268":"01b5a59d-a03f-45f9-bbb4-4f10f6cc76ef","269":"87207d79-b99e-488c-a385-442aba02a905","270":"e1e16455-44e9-4004-8b57-afde28fc9478","271":"d58c3184-3860-44a6-a613-ca9587d747cc","272":"79cabd54-0dc0-4554-9d39-5e645a79b4f7","273":"8236ebb7-2f2a-4a0a-b7f2-05177b1b724d","274":"0df2a4c7-99d7-4d83-bb9c-ac0c26979c38","275":"320a0859-77fd-454d-962e-eb55e4b1adec","276":"2bfd2127-77b2-4db2-b498-313b1dea2f81","277":"6a51df89-13c3-4398-af63-e0c5e3ef21b3","278":"3106c4f1-7980-4751-b105-374a853c8870","279":"ef4a5bda-ca1e-4db5-84d5-dad2ba5710b6","280":"af87a856-1cef-4db8-ab48-8c2b66f17870","281":"73c91241-c3ff-448a-9d9d-859ecb8b2426","282":"f36c7a06-8622-4b33-87dc-e29494d72fb6","283":"9bd273d3-c587-4d58-830f-2024661a58a2","284":"e90d4f41-08f6-4146-9bbb-be7b73f532ef","285":"687b2154-1cf2-43b5-973d-c5d7690d1015","286":"4d9df60a-448d-40fc-a56a-922680e6f8e9","287":"1d2376df-8f0f-444e-9b53-211c3dbafdd1","288":"a2fbd8bb-b922-419d-8d5f-b588a1679de0","289":"3774b3d2-8e1e-458f-8119-4b4a43313e22","290":"f60fffb0-8257-496b-8638-0e5c37526b6b","291":"181bf83b-0bb2-4b18-9573-87fc9985d6f2","292":"30aa4096-0357-418f-a910-839e53788872","293":"07ec4dac-063c-4371-be93-ff13c9dbd1e9","294":"d31a054a-9e16-401b-9fc7-284a03fefc66","295":"5e091968-acaa-4059-9d0c-e7b0a8e9981c","296":"2edf5c6a-66f5-4e5d-bb2a-e0c6c695ad2b","297":"0bab9f00-b71e-418c-8025-4e2e1ffe2b79","298":"286202aa-54c9-44c9-b14a-f5a11e933046","299":"e2e0c063-9c00-4483-aeec-4816f81532f0","300":"f4324b37-ad81-42bc-af4a-f9194fd61ebe","301":"a17328a8-96a8-488f-9fe3-5a30a422c624","302":"ed81f4bf-c361-4605-b87b-71ad88cc9a82","303":"2f3af20f-e994-462d-bfe5-08a98ac51f86","304":"99c99cd0-70b8-4ddc-857a-219aed8765a1","305":"8e7b7a83-9db9-4367-8142-38b235088dab","306":"afede682-2d2b-4f8e-b72e-6b59d3e7397c","307":"3da30069-d4eb-4c3b-af98-47c99122c6d2","308":"c4edb35d-9416-47be-915a-3a2eb3da45b3","309":"3087025b-aa62-4178-af6c-30aec1beb34d","310":"f4e49b16-f894-4325-89c6-b5cda4e09f8c","311":"cb55b227-1473-42fa-a1f9-148797d40b20","312":"e7278112-8007-4124-875c-79903d32ac11","313":"6e4d3bc1-5407-4e56-abd9-cd9fb6fbe555","314":"8f3934d2-009e-478b-a793-e6d254df2b9a","315":"1641ccec-6aec-48b3-b29a-2395ad820617","316":"2badf650-ad14-40fa-82d5-a817041bf994","317":"d2bf5f0f-29e8-450e-a3d5-957ef77fa854","318":"3c428fa1-b620-40c2-bb3f-cdb31fcc6c86","319":"48bcbad5-cb53-44b8-9fa7-4e15ba6faf5d","320":"14e2543e-f5c8-448d-bfa6-8e5c3c646406","321":"6017b316-3f9e-4445-9b55-b4acf12d6f39","322":"9345dc08-7e8c-4264-890b-183df07d52c0","323":"f466bae7-23c6-4fd1-8691-38c7ebfa4f3b","324":"c969a305-2876-4331-ad28-98a8713b2198","325":"5f17ff55-1acd-41a4-922e-f8ec1728811a","326":"dc16c4c0-1ddf-4bd5-bbfa-ff463eefed5a","327":"5e48ba07-7f16-4cb9-b9a1-aeea67c662bf","328":"03c0349e-73c3-4ad4-b38a-9e9fe307e222","329":"5a93e026-63ac-4db2-b293-2123f030aefd","330":"3602a8d3-978e-4e42-ada7-ebd00c6bb873","331":"926f278e-9847-4121-9424-3ecc55899f59","332":"c129c5e6-959b-4027-a165-4c71c681a09f","333":"9778d4c4-a046-4b7b-830a-04ad8d045895","334":"7d9f599c-32a3-429f-806d-ef1eed0ac7ca","335":"54fdbb82-2811-4a32-ac87-4ac0d833c3bf","336":"d18538d5-aaec-4515-8f70-0b6146747b12","337":"d13f36d8-26d2-4f5f-8836-f54ee920940d","338":"3e9afeca-da81-49fd-8008-2f46bcb6a5cf","339":"d11b8577-df08-4c4f-bfc9-e598e5f14d68","340":"a7ba6d0c-9ee5-458d-9ed1-0f246f8330bd","341":"a604907b-9476-4f94-845f-2ff18e7a8018","342":"d54296c0-7c76-4eee-82d2-49b2fd101e86","343":"f4545a99-20a8-450d-8623-bcdafbc7c1a7","344":"d543a7b5-87cb-47c4-81c6-154e1350d314","345":"b9cdf408-41f9-4ab1-bada-a9ed32d38cc1","346":"1c9c2c37-a5c9-4599-95ec-bc4eb9770366","347":"913f6551-f236-4d84-91bf-1124f6d1ab8f","348":"6d00a45d-a6aa-4e12-ac1a-ded240412b03","349":"8452f9d0-2baa-4221-8222-6596e456a712","350":"55a877bd-41cc-4b04-8635-2e9b44df253e","351":"5a96ea5e-95d5-46c5-8cd7-3e485b50ab3e","352":"3b73843b-fc8c-495a-870e-37e9902415ea","353":"5bef3b86-803d-41fb-bbf1-a663e485b67e","354":"b3f27739-50fe-4217-b633-fd853f9bbc51","355":"ae8c2cbc-f18c-40ba-b220-2954c62a9c30","356":"23f4b44a-cd8e-474d-a99d-e179ad339b8e","357":"bc9698b4-3234-464e-986a-437742463b1c","358":"e949ae37-917d-4b7f-bd1b-04cc28b2d12a","359":"d47a769d-01a3-4828-99c0-b5d05ca24390","360":"a674cc62-8278-41e7-b51c-beb9cb0aa1c4","361":"06db8d6b-4167-4bce-97d8-fc3c3d814235","362":"b4c0a386-8461-45e4-b904-4160fc2b94bf","363":"e064f7a6-fbc3-45ce-84e2-c5f9037c349a","364":"1f4c0267-833e-4d37-b046-310680cd63dd","365":"66894fb4-081c-4323-994f-f91377248eda","366":"8809f8aa-d53a-4ad9-937c-717aff003d44","367":"d15a4a71-0e48-4b08-8e48-c9dd7b3056a5","368":"f40db729-b302-465b-96af-e8bf4827e7b6","369":"35d8d59a-ab67-4cd3-ba16-9f21030f9a0b","370":"20384620-a275-4078-a6e7-9347a00333cf","371":"95b34b3e-ec06-4415-ac7e-9d98a0f4099a","372":"7f6f4051-63b9-4d79-ae9c-35f75b271953","373":"0eb5010f-6522-4ae3-9667-34f84fb7d432","374":"f36ea1a1-02b5-4d30-acae-3778ac3bfb8a","375":"9c181171-b58b-4a3c-9cee-009f7977bab1","376":"cba300bc-8d70-40d2-80ab-e333235e9394","377":"0c3b2d83-f370-4895-85b7-59822844d598","378":"53b35dab-7c1e-4266-bdc4-62c29f1b4014","379":"99154967-9be7-42b8-851f-4afac2240ecf","380":"d03142b9-6cce-4be0-a05c-ca427e612df2","381":"d59b32ac-aeb3-4058-8e72-24d0e2194d62","382":"7a96f192-6b00-4ea1-85d3-a21164170be6","383":"d94ca7ec-5e3d-4418-9a90-a9d464d1b172","384":"dced0d88-10b5-43d0-a355-3f9562c312e6","385":"acb93910-9fec-4275-9b7a-f1cc40e9e655","386":"0a158352-1a46-4236-a552-af2e586ee5c2","387":"135b71d2-6136-49f4-a0ba-68eb601b0bf2","388":"fe94e96a-56cd-495d-a6dc-6b52def9f21b","389":"7d1b0fce-193d-4cb3-9cc5-53cc1a45220b","390":"9b9a225d-1655-4212-b277-6afe9317e0c3","391":"9a6474ac-9216-480c-97a2-419f3e114bdc","392":"3970cd37-b395-4022-ba1d-9f5875fd0761","393":"5817ab63-0a4a-4997-8b11-7381f2d9bc28","394":"db43cc0b-242a-48d8-905a-76477ed4ece5","395":"0ed827e8-62be-4e36-b5b6-bfbdf02113a5","396":"8dcf07ff-390e-4741-83e1-ab86779d253f","397":"78940d70-c7ce-43e6-8538-7a73fb3860e7","398":"dc6d8f8a-6229-4b7e-8f84-5322c0d11ea2","399":"60292f3d-fef3-4589-ba8e-b12bab676868","400":"9bb1b44b-d6b9-4232-9bec-008707744c7c","401":"f248fdf5-9435-4059-8ab5-f75596745aca","402":"df682c64-2b4c-404c-a7b7-4de8943506e5","403":"3f7ef881-b049-4c57-80f2-81d320795d72","404":"a1997a8a-65c0-47b2-beb6-9f7305ee6a7e","405":"faaad1e7-a9d5-4cbc-a010-6109c3b9afaa","406":"6fc03d1e-bfb9-456e-97e9-9616fb480c8c","407":"0b60c100-8b4b-4c67-8809-34abb3f6dff6","408":"47b7cd89-7f69-4c3d-a385-465d86798616","409":"7a394032-f855-42ba-9e89-bc0dd2348765","410":"901abc08-4286-4878-801e-2870447a4771","411":"e13c1e5e-815e-4a67-8649-baacbf593f9f","412":"0f679b97-e83a-4d6b-88d5-480566e2466b","413":"115586bf-897a-40a0-ba8a-d18e4241b4f5","414":"c211ef52-052b-4095-ba16-0d1ab751a494","415":"9a00a0c8-eb1a-4818-bb14-bd1856f5e410","416":"78ba7b6f-1724-4631-8747-0421ea1e1507","417":"cef147f7-5e64-4a96-a345-70454035e25b","418":"30807eef-63c3-44ab-b8b2-60b49251a2cc","419":"46a439f0-9d47-4e47-ac85-a94ab530cc4d","420":"d18343c5-26aa-44a4-a8d1-0e7f20e4cb13","421":"25677882-bd19-41c5-9afd-98406cd4598e","422":"d2bd86e9-9770-4370-a5ae-da0f9a9a3fbf","423":"d2a41afb-cd88-4fb3-9dba-9a08bbe080f8","424":"b2f51566-f752-4e54-8754-21fa32fde8f2","425":"a4e08933-e940-48e9-bb50-0005d19b2788","426":"2cc4e9b1-b892-4f69-b523-77163dd0be0c","427":"55559229-02e3-4ac6-9a0d-0673869f0729","428":"b6c7cb5c-d53c-4578-a05d-676e0637a4ff","429":"059bb9ae-f3f0-4bb7-8b38-5981dd778192","430":"278202f4-d17b-4791-8985-6f6044228db3","431":"c09c627c-f1f3-41ae-a2cb-a7b3aff4ffed","432":"967a457e-0878-4afd-ad26-95e5fb3e4052","433":"d892d12e-24dd-41e6-a280-8f1d925610e4","434":"1321f7ec-4f17-4f86-a99e-6a448f492595","435":"4ffdd619-af59-4a62-8618-d75185b0b43e","436":"7589322e-44f2-4ffc-b0e2-d646b4d96da2","437":"393b460f-e139-4229-8ef4-3621b310d2b8","438":"293910ef-a6dc-42de-accb-a6bdff90406b","439":"b8c1a065-30a6-4bae-98a3-e1e678e10db5","440":"43a5008d-128a-4b0c-a260-76fd283541f5","441":"d7035aa1-f090-46cb-b635-f5202d8e5ef6","442":"a1a6ff9d-e8ef-49ea-8adb-4c7c36f4c8b9","443":"a6731dad-b1d0-43e9-8840-6c62d5982375","444":"a10fc623-cc39-4120-9296-4126edeb39e2","445":"860fd9b5-2f0a-41d3-b045-9af7cde13308","446":"786d84ee-a81e-460a-89f9-71820a94084c","447":"f70841e7-0009-4ac7-a861-ab238761227c","448":"6805561d-8c51-45f2-b3ae-27ad232a8895","449":"bf5b1ffc-c161-45ec-a77f-addfc435de3f","450":"656ddab8-4a84-4a98-8172-a3ada5bbc923","451":"8c068934-55ae-4177-af99-5eb4444c8707","452":"28567242-dff5-447b-86c5-62b5146daccd","453":"d3cad356-45b8-4364-bed4-50f7ba098af8","454":"98312199-230d-4cca-8e7f-d084973e2f7c","455":"4d2b34b6-ac63-4b8d-9139-621f30bcb259","456":"bc475072-da3e-4908-a111-f121f7d2e711","457":"dac4d1fe-7d59-46b5-a651-171959a25d2f","458":"5cf26777-11a1-44a4-9d13-b48ba377d513","459":"b9aaa4c2-58c5-4e77-ac06-7eed5787dc43","460":"7f571ce1-911f-4f96-9bb4-5fd77336fd25","461":"433d25ff-4b2e-4b49-9ccc-9e2a4a8088a2","462":"1672a6ef-5ded-4c50-b2fd-adb3d783d092","463":"bd6dbfec-8c60-4403-9170-4031449f37b9","464":"29c3b80b-cfa7-461d-a068-425b92a4355a","465":"e413999e-a840-4570-902a-9ea27bf7bbb4","466":"921fdc69-8638-4076-92b6-8bab33b79f10","467":"2470b16e-ae1d-4183-ab5d-a3e664c622ff","468":"6f071e3b-2eed-4d68-b84b-40183c5bd8b9","469":"4f148aed-554c-4aaf-8484-5fc658197e76","470":"e6b4e6c4-2b9f-4316-88dd-3f0437eea5af","471":"d147cbbb-ded4-4f9e-afd8-fcbef437640c","472":"994adffb-297d-4250-8b05-0881777d1a6e","473":"9f3117c3-fffd-43c2-b063-485647e7d100","474":"492eb06e-ee6e-4b58-939f-a3e81220ec01","475":"d69d0cb8-a80c-4f5e-9e60-934d2a4a274a","476":"d90cd747-be24-486d-8c41-0c3b44bbecb0","477":"0b1032de-d452-4969-b078-525fb9cb9b3a","478":"9ac55c9d-3c83-45ca-b5e0-458a784d3a16","479":"5cf3295a-101e-4de4-b2d4-b10e2ba8668d","480":"666d772a-a68e-4435-a9ae-c2e8527a0a8c","481":"0fb4d744-4254-4d8f-9553-2750815c6e1f","482":"7b69b7d8-6dd2-4c36-9016-172cbd47ab1f","483":"4693478d-7764-4e4e-9419-ec67eb415d63","484":"75481c50-b8b9-45b7-a981-cf8b6573856f","485":"fc8b2ab8-02a2-4329-b4c3-06e40494d55a","486":"8ef71ade-c574-49e0-98d1-46de6183d0e9","487":"01373a58-0b54-4ce1-a24a-535377472a10","488":"4f84a6c0-b76a-4155-9e66-5d31cb30a5be","489":"224c7ef4-d1b3-48dd-8743-e7e07f089a38","490":"dcd8b730-c324-4619-bb5d-e9f378a67e2e","491":"2e94253a-fc12-44ae-9192-c3b960550d3b","492":"4e1b8949-7143-42e6-8322-5ab800b1fd14","493":"b5a00b30-5ac7-4646-86e3-fd995e9eeab4","494":"ddeed930-cac3-410d-a042-bd425fbca9f9","495":"7864064e-29d2-4052-ba19-9f2bb5a1b376","496":"a6cdae0a-07f6-4c36-8e4c-cb04f3cd4ef6","497":"5d82ebef-9b05-4cb8-9f9e-c9d98b27edf6","498":"33af14e5-1db8-4141-bbaf-22b07d19ae20","499":"28df93eb-b2ac-41fe-8e8b-61278ac69cef","500":"5febb36f-dcd4-49a6-ae9e-32202c91b14f","501":"9772c60c-6a71-4458-bda5-ae3aab6c7a01","502":"19800d40-0eff-4990-8ef9-86aa4a9e52e5","503":"d6f84e51-14e2-45f7-86f8-c22c8aed13f8","504":"54481986-2be9-4ea8-9bb4-dbc9a05c1770","505":"a1b1fcdb-1481-4a38-947d-66423691ffd2","506":"ded83938-2113-4565-b502-a9cf7aed932e","507":"c87d0ebc-fefc-4754-87c8-ff748deca767","508":"822d9f30-78b1-4f16-96ca-7abbed5df3c7","509":"2eeb9bff-73fe-4af9-9079-33f59e130d9a","510":"24568417-45c6-4748-a5ed-17acb83dd3b3","511":"169bde47-f301-47e9-9fbf-d677c21a9f1c","512":"069e39a9-ebd9-4637-875c-6ddfe10eb6ce","513":"074b4914-3c95-4edd-b35c-e72ce3999e67","514":"4f5b7cf6-4f46-4d8c-b291-9ecbb1c2229f","515":"6b297e1f-3618-427e-9c61-9764824bff33","516":"a4a7d4f4-1d3c-4ca5-bf8e-b66c5f3340f5","517":"05f1c411-ccdc-4927-8551-c78fe2570805","518":"b461db72-6c56-4258-97e4-bf6ab0f2f897","519":"5075c9b0-305c-4def-9c62-ed2b1349355c","520":"7ea45d4e-b599-41d4-85d5-57c1941ebb14","521":"ce1c1092-9206-4384-a739-e3118613399b","522":"31965ef3-4d37-4f44-b8fa-b9cecec81e2b","523":"6b7f27cd-a515-4e95-b32c-0be1d8e7d5b9","524":"28e3b8f8-3c30-463f-baa6-c08ffd572ea3","525":"c66be175-2eb3-4ffc-a482-2f358775054a","526":"0800f1cf-eac2-4a5c-a6e2-5923a1991825","527":"f3be4f4a-2b61-4e9b-94ad-c212a3d39b08","528":"a56a767e-2803-446c-a9b1-62f8cfd472fe","529":"2e491f64-d4fa-40cd-b1ad-c0ee56e49b27","530":"43d5844c-03ca-4735-a59f-7539404f2721","531":"cfe260c2-e5d6-45bb-b229-21a6f7fcc08b","532":"c468f849-e2c0-467f-bbbf-3cb7051b9d44","533":"87c1c1e3-1ca6-4e0f-b0fd-ab0a3d70dbdf","534":"6705ac36-bea3-4faa-a201-5e67cfaa29dd","535":"27790a50-e56f-4fcb-971f-1598e36f7d2b","536":"45b0af90-1152-4bcc-9364-9759fcf9531d","537":"1f93f6d7-0853-4de4-b575-2d49c7736238","538":"f793822c-423e-4a61-b521-8e83903e1953","539":"c0f39c8e-7bb8-4948-be68-f4a874189eb0","540":"22224a2b-9b87-4f26-a389-bb80a19b0d31","541":"7b174de6-10bd-4967-a1a6-91af7c31ee2d","542":"330ea531-709d-4ebb-842a-7e7490503952","543":"4437fa29-4054-4763-9001-0b52fd1ef67e","544":"029b838f-b6ab-4b74-9b7e-6af947dc4954","545":"345b71d9-b22e-4c04-bf6d-5bb477f7223b","546":"ebe2849a-1e4e-4a62-9e28-f525879aec97","547":"d24516d2-9468-4b86-82bd-fc5ef9817d12","548":"3f88bc12-2851-4f98-bb1e-7d333353ea6c","549":"5d046db7-2eae-46ee-88dd-5f5ec1c4c48b","550":"68ff5d39-934b-4375-9e2c-dee4876af756","551":"aa9a4fc3-07e6-4d37-9f70-096aa14f0fbd","552":"3beba20c-5fc9-4268-8b2e-26fdf7671e56","553":"5a7eb2fb-8845-4861-b47f-5f6c7b1be43c","554":"54311d2f-8510-4b12-bc44-47ce65c8cfd2","555":"e6000e60-93e1-4f26-9369-51d3c97029fe","556":"f68ab22d-dd46-4ed1-bd64-0a26567cbc82","557":"82ca539f-020b-4d2f-8553-5c0e1f0bdc79","558":"5a007e2f-112b-4861-90ca-e54cd4ee4f4e","559":"c8dee76a-a619-4614-aa5f-2c4a475f329a","560":"7195f605-69d7-48b3-a334-d2016c269ac6","561":"d4899df8-93ff-4a48-aa9b-dbd71c7c49e6","562":"21ccb9da-08e1-4e34-99d4-ba0667ff7366","563":"3a0e1662-c8b8-4e1c-b898-41e0bc192f07","564":"519e2b08-832c-4c48-8358-5fdf187f75ee","565":"9d5f38e4-6f8c-4f23-a2a8-721809bcf637","566":"70581cc9-8923-4867-9c7b-6625c9695b72","567":"35dddebf-234f-4432-829f-ca0793de39f9","568":"de8d4cfa-05a3-45f8-9a49-1a3aa51a57a5","569":"964a30ad-3e6e-4810-98a1-d6d7284d33ae","570":"60da5a61-673b-4f5e-bce3-dbab15713178","571":"771de311-a960-477f-a670-9a5e77ee46a5","572":"46c05a07-9ca3-4e0b-bfb7-5faaf435eef3","573":"bb5d89f5-d34c-4d94-97a5-30ab0d75a572","574":"ec4cdc2e-99ef-48b2-b8ae-77fdd0012f30","575":"a4eef127-2fb0-4603-bba7-55f68cceedab","576":"2b85cb64-a9a4-473b-b756-317b2c4b67fb","577":"dc9ba1d2-7bae-4c4b-9eea-1480ffe4369b","578":"aa7a9a98-b78e-41a2-94f2-fa1796788d99","579":"9415d3ba-755e-4717-8a23-c6edb5b55fb5","580":"70ffe151-2275-43dd-8203-39dd4b0fd59b","581":"e7ea3e48-7a9a-475a-8a39-1c3bc5d7a1ca"}]